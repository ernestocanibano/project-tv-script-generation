<!DOCTYPE html>
<!-- saved from url=(0074)https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/branch-latest.min.js.descarga"></script><script async="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/analytics.js.descarga"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><script defer="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/16180790160.js.descarga"></script><title>Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2017-04-26T13:25:16.686Z"><meta data-rh="true" name="title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta data-rh="true" property="og:title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta data-rh="true" property="twitter:title" content="Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs"><meta data-rh="true" name="twitter:site" content="@mckapur"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/10300100899b"><meta data-rh="true" property="al:android:url" content="medium://p/10300100899b"><meta data-rh="true" property="al:ios:url" content="medium://p/10300100899b"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="It seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way…"><meta data-rh="true" property="og:description" content="The ultimate guide to machine learning’s favorite child."><meta data-rh="true" property="twitter:description" content="The ultimate guide to machine learning’s favorite child."><meta data-rh="true" property="og:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta data-rh="true" property="al:web:url" content="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*khIKl9t4XmZGSsKhW_Yg2w.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://ayearofai.com/@mckapur"><meta data-rh="true" name="twitter:creator" content="@mckapur"><meta data-rh="true" name="author" content="Rohan Kapur"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="85 min read"><meta data-rh="true" name="parsely-post-id" content="10300100899b"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://ayearofai.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/m2.css"><link data-rh="true" rel="author" href="https://ayearofai.com/@mckapur"><link data-rh="true" rel="canonical" href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/10300100899b"><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/16180790160.js.descarga" as="script"><style type="text/css" data-fela-rehydration="839" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:absolute}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ag{max-width:1192px}.ah{min-width:0}.ai{width:100%}.aj{height:65px}.am{flex:1 0 auto}.an{flex:0 0 auto}.ao{visibility:hidden}.ap{margin-left:16px}.aq{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ar{font-style:normal}.as{line-height:20px}.at{font-size:15.8px}.au{letter-spacing:0px}.av{color:rgba(0, 0, 0, 0.54)}.aw{fill:rgba(0, 0, 0, 0.54)}.ax{color:rgba(115, 113, 113, 1)}.ay{fill:rgba(134, 132, 132, 1)}.az{font-size:inherit}.ba{border:inherit}.bb{font-family:inherit}.bc{letter-spacing:inherit}.bd{font-weight:inherit}.be{padding:0}.bf{margin:0}.bg:hover{cursor:pointer}.bh:hover{color:rgba(105, 104, 103, 1)}.bi:hover{fill:rgba(115, 113, 113, 1)}.bj:focus{outline:none}.bk:disabled{cursor:default}.bl:disabled{color:rgba(52, 148, 166, 0.5)}.bm:disabled{fill:rgba(52, 148, 166, 0.5)}.bn{border-top:none}.bo{background-color:rgba(0, 0, 0, 1)}.bq{height:54px}.br{overflow:hidden}.bs{margin-right:40px}.bt{height:36px}.bu{width:54px}.bv{overflow:auto}.bw{flex:0 1 auto}.bx{list-style-type:none}.by{line-height:40px}.bz{white-space:nowrap}.ca{overflow-x:auto}.cb{align-items:flex-start}.cc{margin-top:20px}.cd{padding-top:20px}.ce{height:80px}.cf{height:20px}.cg{margin-right:15px}.ch{margin-left:15px}.ci:first-child{margin-left:0}.cj{font-weight:300}.ck{font-size:15px}.cl{color:rgba(177, 174, 174, 1)}.cm{text-transform:uppercase}.cn{letter-spacing:1px}.co{color:inherit}.cp{fill:inherit}.cq:hover{color:rgba(236, 233, 233, 1)}.cr:hover{fill:rgba(217, 214, 214, 1)}.cs:disabled{color:rgba(123, 121, 121, 1)}.ct:disabled{fill:rgba(123, 121, 121, 1)}.cu{margin-bottom:0px}.cv{height:119px}.cy{padding-left:24px}.cz{padding-right:24px}.da{margin-left:auto}.db{margin-right:auto}.dc{max-width:728px}.dd{box-sizing:border-box}.de{flex-direction:column}.df{top:calc(100vh + 100px)}.dg{bottom:calc(100vh + 100px)}.dh{width:10px}.di{pointer-events:none}.dj{word-break:break-word}.dk{word-wrap:break-word}.dl:after{display:block}.dm:after{content:""}.dn:after{clear:both}.do{clear:both}.dp{margin-top:0px}.dq{opacity:0}.dr{transition:opacity 100ms 400ms}.ds{height:100%}.dt{will-change:transform}.du{transform:translateZ(0)}.dv{margin:auto}.dw{position:relative}.dx{background-color:rgba(0, 0, 0, 0.05)}.dy{padding-bottom:45.349999999999994%}.dz{height:0}.ea{filter:blur(20px)}.eb{transform:scale(1.1)}.ec{visibility:visible}.ed{background:rgba(255, 255, 255, 1)}.ee{margin-top:10px}.ef{text-align:center}.ei{font-size:16px}.ej{max-width:680px}.ek{line-height:1.23}.el{letter-spacing:0}.em{color:rgba(0, 0, 0, 0.84)}.en{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.ey{margin-bottom:-0.27em}.fe{line-height:1.394}.fp{margin-bottom:-0.42em}.fv{margin-top:32px}.fw{justify-content:space-between}.ga{border-radius:50%}.gb{height:48px}.gc{width:48px}.gd{margin-left:12px}.ge{margin-bottom:2px}.gg{max-height:20px}.gh{text-overflow:ellipsis}.gi{display:-webkit-box}.gj{-webkit-line-clamp:1}.gk{-webkit-box-orient:vertical}.gl:hover{text-decoration:underline}.gm:disabled{color:rgba(0, 0, 0, 0.54)}.gn:disabled{fill:rgba(0, 0, 0, 0.54)}.go{margin-left:8px}.gp{padding:0px 8px}.gq{background:0}.gr{border-color:rgba(0, 0, 0, 0.54)}.gs:hover{color:rgba(0, 0, 0, 0.97)}.gt:hover{fill:rgba(0, 0, 0, 0.97)}.gu:hover{border-color:rgba(0, 0, 0, 0.84)}.gv:disabled{fill:rgba(0, 0, 0, 0.76)}.gw:disabled{border-color:rgba(0, 0, 0, 0.2)}.gx:disabled{cursor:inherit}.gy:disabled:hover{color:rgba(0, 0, 0, 0.54)}.gz:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.ha:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.hb{border-radius:4px}.hc{line-height:18px}.hd{border-width:1px}.he{border-style:solid}.hf{display:inline-block}.hg{text-decoration:none}.hh{align-items:flex-end}.hp{padding-right:6px}.hq:hover{color:rgba(0, 0, 0, 0.9)}.hr:hover{fill:rgba(0, 0, 0, 0.9)}.hs{margin-right:8px}.ht{margin-right:-6px}.hu{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.hv{font-size:28px}.hw{color:rgba(0, 0, 0, 0.97)}.hx{border:none}.hy{margin-top:30px}.hz:before{content:"..."}.ia:before{letter-spacing:0.6em}.ib:before{text-indent:0.6em}.ic:before{font-style:italic}.id:before{line-height:1.4}.ie{padding-left:30px}.if{line-height:1.48}.ig{letter-spacing:-0.014em}.ih{color:rgba(0, 0, 0, 0.76)}.ii{font-size:24px}.io{margin-bottom:-0.46em}.ip{line-height:44px}.iq{font-size:30px}.ir{background:none}.is{background-repeat:repeat-x}.it{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.iu{background-size:1px 1px}.iv{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.iw{line-height:1.58}.ix{letter-spacing:-0.004em}.iy{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.jh{clear:left}.ji{letter-spacing:-0.003em}.jl{float:left}.jm{font-size:66px}.jn{line-height:.83}.jo{margin-right:12px}.ju{max-width:570px}.ka{border-width:2px}.kb{border-color:rgba(255, 255, 255, 1)}.kc{margin-left:-150px}.kd{margin-right:30px}.ke{width:75%}.kf{padding-bottom:10px}.kk{margin-bottom:16px}.kl{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.km{cursor:zoom-in}.kn{z-index:auto}.ko{padding-bottom:68.24561403508771%}.kp{font-weight:700}.kq{font-style:italic}.kr{line-height:1.12}.ks{letter-spacing:-0.022em}.kt{font-weight:600}.lc{margin-bottom:-0.28em}.li{list-style-type:decimal}.lj{margin-left:30px}.lk{padding-left:0px}.lq{max-width:713px}.lr{padding-bottom:56.100981767180926%}.ls{box-shadow:inset 3px 0 0 0 rgba(0, 0, 0, 0.84)}.lt{padding-left:23px}.lu{margin-left:-20px}.lv{list-style-type:disc}.lw{max-width:500px}.lx{padding-bottom:61.8%}.ly{max-width:305px}.lz{padding-bottom:59.67213114754098%}.ma{max-width:625px}.mb{padding-bottom:22.240000000000002%}.mc{font-style:inherit}.md{max-width:543px}.me{padding-bottom:16.574585635359117%}.mf{max-width:250px}.mg{padding-bottom:100%}.mh{max-width:552px}.mi{padding-bottom:48.188405797101446%}.mj{max-width:557px}.mk{padding-bottom:18.31238779174147%}.ml{max-width:310px}.mm{padding-bottom:70.96774193548387%}.mn{max-width:300px}.mo{padding-bottom:21.666666666666668%}.mp{max-width:50px}.mq{padding-bottom:462%}.mr{max-width:190px}.ms{padding-bottom:121.05263157894737%}.mt{max-width:323px}.mu{padding-bottom:88.85448916408669%}.mv{max-width:1298px}.mw{padding-bottom:60.24653312788906%}.nh{padding-bottom:120%}.ni{padding-bottom:69.03225806451613%}.nj{max-width:313px}.nk{padding-bottom:97.44408945686902%}.nl{padding-bottom:21.323529411764707%}.nm{max-width:276px}.nn{padding-bottom:88.76811594202898%}.no{max-width:551px}.np{padding-bottom:19.963702359346644%}.nq{padding-bottom:121.57894736842105%}.nr{padding-bottom:131.57894736842104%}.ns{max-width:141px}.nt{padding-bottom:54.60992907801418%}.nu{max-width:157px}.nv{padding-bottom:26.114649681528665%}.nw{padding-bottom:82.4%}.nx{max-width:400px}.ny{padding-bottom:26.75%}.nz{max-width:390px}.oa{padding-bottom:59.743589743589745%}.ob{max-width:404px}.oc{padding-bottom:55.940594059405946%}.od{max-width:324px}.oe{padding-bottom:51.23456790123456%}.of{max-width:241px}.og{padding-bottom:34.85477178423236%}.oh{max-width:177px}.oi{padding-bottom:24.858757062146893%}.oj{max-width:440px}.ok{padding-bottom:43.63636363636364%}.ol{padding-bottom:90.80000000000001%}.om{padding-bottom:90.4%}.on{max-width:280px}.oo{padding-bottom:132.5%}.op{padding-bottom:191.33333333333331%}.oq{padding:2px 4px}.or{font-size:75%}.os> strong{font-family:inherit}.ot{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.ou{max-width:651px}.ov{padding-bottom:12.135176651305684%}.pb{box-shadow:0 1px 4px rgba(0, 0, 0, 0.1), inset 0 0 0 1px rgba(0, 0, 0, 0.1)}.pc{padding:0px}.pd{padding:16px 20px}.pe{flex:1 1 auto}.pg{font-size:18px}.ph{max-height:40px}.pi{-webkit-line-clamp:2}.pj{margin-top:8px}.pk{margin-top:12px}.pl{width:160px}.pm{background-image:url(https://miro.medium.com/max/320/1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg)}.pn{background-origin:border-box}.po{background-size:cover}.pp{height:167px}.pq{background-position:50% 50%}.pr{max-width:100%}.ps{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.1)}.pt{max-width:346px}.pu{padding-bottom:22.832369942196532%}.pv{max-width:773px}.pw{padding-bottom:10.34928848641656%}.px{max-width:298px}.py{padding-bottom:57.04697986577181%}.pz{max-width:350px}.qa{padding-bottom:102%}.qb{padding-bottom:78.5%}.qc{max-width:195px}.qd{padding-bottom:117.43589743589743%}.qe{padding-bottom:49%}.qf{max-width:490px}.qg{padding-bottom:122.44897959183673%}.qh{max-width:459px}.qi{padding-bottom:20.697167755991284%}.qj{max-width:330px}.qk{padding-bottom:73.03030303030303%}.ql{max-width:450px}.qm{padding-bottom:61.333333333333336%}.qn{max-width:208px}.qo{padding-bottom:18.75%}.qp{max-width:444px}.qq{padding-bottom:8.783783783783784%}.qr{max-width:2161px}.qs{padding-bottom:31.929662193428967%}.qt{max-width:610px}.qu{padding-bottom:31.63934426229508%}.qv{max-width:740px}.qw{padding-bottom:35.4054054054054%}.qx{max-width:287px}.qy{padding-bottom:100.34843205574913%}.qz{max-width:466px}.ra{padding-bottom:51.502145922746784%}.rb{max-width:167px}.rc{padding-bottom:49.10179640718563%}.rd{max-width:419px}.re{padding-bottom:37.70883054892602%}.rf{max-width:533px}.rg{padding-bottom:47.09193245778612%}.rh{max-width:222px}.ri{padding-bottom:37.83783783783784%}.rj{max-width:290px}.rk{padding-bottom:14.137931034482758%}.rl{max-width:384px}.rm{padding-bottom:46.875%}.rn{padding-bottom:58.25771324863884%}.ro{max-width:770px}.rp{padding-bottom:45.324675324675326%}.rq{padding-bottom:28.497041420118343%}.rr{max-width:87px}.rs{padding-bottom:90.80459770114942%}.rt{max-width:675px}.ru{padding-bottom:11.703703703703704%}.rv{max-width:445px}.rw{padding-bottom:17.97752808988764%}.rx{max-width:667px}.ry{padding-bottom:11.994002998500749%}.rz{max-width:515px}.sa{padding-bottom:15.533980582524272%}.sb{max-width:674px}.sc{padding-bottom:11.869436201780415%}.sd{max-width:413px}.se{padding-bottom:27.360774818401936%}.sf{max-width:236px}.sg{padding-bottom:16.52542372881356%}.sh{max-width:2436px}.si{padding-bottom:40.476190476190474%}.sj{max-width:113px}.sk{padding-bottom:69.91150442477876%}.sl{max-width:380px}.sm{padding-bottom:42.63157894736842%}.sn{max-width:271px}.so{padding-bottom:45.75645756457565%}.sp{max-width:381px}.sq{padding-bottom:43.044619422572175%}.sr{max-width:150px}.ss{padding-bottom:51.333333333333336%}.st{max-width:320px}.su{padding-bottom:38.75%}.sv{max-width:361px}.sw{padding-bottom:45.42936288088643%}.sx{max-width:461px}.sy{padding-bottom:17.136659436008678%}.sz{max-width:642px}.ta{padding-bottom:12.305295950155763%}.tb{max-width:458px}.tc{padding-bottom:15.502183406113536%}.td{max-width:541px}.te{padding-bottom:14.602587800369685%}.tf{max-width:1095px}.tg{padding-bottom:41.36986301369863%}.th{padding-bottom:40.38461538461538%}.ti{max-width:372px}.tj{padding-bottom:44.08602150537635%}.tk{max-width:603px}.tl{padding-bottom:13.764510779436153%}.tm{max-width:692px}.tn{padding-bottom:14.595375722543352%}.to{max-width:106px}.tp{padding-bottom:74.52830188679245%}.tq{max-width:609px}.tr{padding-bottom:26.929392446633827%}.ts{max-width:501px}.tt{padding-bottom:15.768463073852296%}.tu{max-width:474px}.tv{padding-bottom:16.666666666666668%}.tw{max-width:2190px}.tx{padding-bottom:31.689497716894977%}.ty{max-width:669px}.tz{padding-bottom:24.81315396113602%}.ua{max-width:205px}.ub{padding-bottom:38.53658536585366%}.uk{padding-bottom:56.206088992974244%}.ul{background-image:url(https://miro.medium.com/max/320/0*7sIxt7RqO7deGldw.)}.um{max-width:574px}.un{padding-bottom:107.8397212543554%}.uo{padding:20px}.up{background:rgba(0, 0, 0, 0.05)}.uq{line-height:1.18}.ur{margin-top:-0.09em}.us{margin-bottom:-0.09em}.ut{white-space:pre-wrap}.uu{padding-bottom:130%}.vf{margin-bottom:-0.31em}.vg{padding-bottom:124.75247524752474%}.vh{max-width:606px}.vi{padding-bottom:83.16831683168317%}.vj{max-width:677px}.vk{padding-bottom:74.74150664697193%}.vl{max-width:201px}.vm{padding-bottom:16.915422885572138%}.vn{max-width:200px}.vo{padding-bottom:17%}.vp{max-width:182px}.vq{padding-bottom:20.32967032967033%}.vr{max-width:656px}.vs{padding-bottom:55.33536585365854%}.vt{max-width:704px}.vu{padding-bottom:34.65909090909091%}.vv{max-width:355px}.vw{padding-bottom:89.85915492957747%}.vx{padding-bottom:NaN%}.vy{max-width:426px}.vz{padding-bottom:20.187793427230048%}.wa{max-width:321px}.wb{padding-bottom:26.79127725856698%}.wc{max-width:265px}.wd{padding-bottom:27.92452830188679%}.we{max-width:286px}.wf{padding-bottom:26.923076923076923%}.wg{padding-bottom:26.82926829268293%}.wh{max-width:80px}.wi{will-change:opacity}.wj{position:fixed}.wk{width:188px}.wl{left:50%}.wm{transform:translateX(406px)}.wn{top:calc(65px + 54px + 14px)}.wq{top:calc(65px + 54px + 40px)}.ws{width:131px}.wt{padding-bottom:28px}.wu{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.wv{padding-bottom:20px}.ww{padding-top:2px}.wx{max-height:120px}.wy{-webkit-line-clamp:6}.wz{padding:4px 12px}.xa{border-color:rgba(134, 132, 132, 1)}.xb:hover{border-color:rgba(115, 113, 113, 1)}.xc{padding-top:28px}.xd{margin-bottom:19px}.xe{margin-left:-3px}.xf{margin-right:5px}.xg{outline:0}.xh{border:0}.xi{user-select:none}.xj{cursor:pointer}.xk> svg{pointer-events:none}.xl:active{border-style:none}.xm{-webkit-user-select:none}.xn{fill:rgba(0, 0, 0, 0.76)}.xo:focus{fill:rgba(0, 0, 0, 0.54)}.xp:hover{fill:rgba(0, 0, 0, 0.54)}.xq{margin-top:5px}.xr button{text-align:left}.xs{margin-top:40px}.xt{flex-wrap:wrap}.xu{margin-top:25px}.xv{margin-bottom:8px}.xw{border-radius:3px}.xx{padding:5px 10px}.xy{line-height:22px}.xz{margin-top:15px}.ya{flex-direction:row}.yb{max-width:155px}.yc{margin-right:16px}.yd{border:1px solid rgba(0, 0, 0, 0.1)}.ye{height:60px}.yf{width:60px}.ys:hover{border-color:rgba(0, 0, 0, 0.54)}.yt:active{border-style:solid}.yu{z-index:2}.zb{padding-right:8px}.zc{padding-top:32px}.zd{border-top:1px solid rgba(0, 0, 0, 0.1)}.ze{margin-bottom:25px}.zf{margin-bottom:32px}.zg{min-height:80px}.zl{width:80px}.zm{padding-left:102px}.zo{letter-spacing:0.05em}.zp{margin-bottom:6px}.zq{line-height:36px}.zr{max-width:555px}.zs{line-height:24px}.zt{display:none}.zv{max-width:550px}.zw{padding-top:25px}.zx{opacity:1}.zy{border:1px solid rgba(134, 132, 132, 1)}.zz{margin-top:64px}.aba{background-color:rgba(0, 0, 0, 0.02)}.abb{margin:32px 0}.abd{padding-bottom:8px}.abe{font-size:22px}.abf{line-height:28px}.acm{flex-grow:0}.aeg{margin-bottom:12px}.aej{margin-bottom:4px}.ael{font-weight:500}.aem{line-height:32px}.aen{letter-spacing:0em}.aeo{margin-right:10px}.aep{border-right:1px solid rgba(0, 0, 0, 0.1)}.aeq{width:1px}.aer{padding:60px 0}.aes{background-color:rgba(0, 0, 0, 0.9)}.afj{padding-bottom:48px}.afk{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.afl{margin:0 -12px}.afm{margin:0 12px}.afn{flex:1 1 0}.afo{padding-bottom:12px}.afp:hover{color:rgba(255, 255, 255, 0.99)}.afq:hover{fill:rgba(255, 255, 255, 0.99)}.afr:disabled{color:rgba(255, 255, 255, 0.7)}.afs:disabled{fill:rgba(255, 255, 255, 0.7)}.aft{color:rgba(255, 255, 255, 0.98)}.afu{fill:rgba(255, 255, 255, 0.98)}.afv{text-align:inherit}.afw{font-size:21.6px}.afx{letter-spacing:-0.32px}.afy{color:rgba(255, 255, 255, 0.7)}.afz{fill:rgba(255, 255, 255, 0.7)}.aga{text-decoration:underline}.agb{padding-top:8px}.agc{width:200px}.agi:disabled{color:rgba(3, 168, 124, 0.5)}.agj:disabled{fill:rgba(3, 168, 124, 0.5)}.agk{background-image:url(https://miro.medium.com/max/160/1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg)}.agl{background-image:url(https://miro.medium.com/max/160/0*7sIxt7RqO7deGldw.)}.agm{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.af{margin:0 64px}.ew{font-size:40px}.ex{margin-top:0.78em}.fd{line-height:48px}.fn{font-size:24px}.fo{margin-top:0.79em}.fu{line-height:32px}.ho{margin-left:30px}.in{margin-top:1.75em}.jf{font-size:21px}.jg{margin-top:2em}.jt{padding-top:7px}.jz{margin-top:56px}.la{font-size:34px}.lb{margin-top:1.95em}.lh{margin-top:0.86em}.lp{margin-top:1.05em}.nb{margin-top:2.75em}.ng{margin-top:3.14em}.pa{margin-top:32px}.uj{max-width:1192px}.vd{font-size:26px}.ve{margin-top:1.72em}.za{width:25px}.abs{width:calc(100% + 32px)}.abt{margin-left:-16px}.abu{margin-right:-16px}.aci{padding-left:16px}.acj{padding-right:16px}.ack{flex-basis:33.333333%}.acl{max-width:33.333333%}.acw{align-items:flex-start}.acx{flex-direction:row}.aee{flex-basis:100%}.aef{max-width:100%}.afg{padding-left:64px}.afh{padding-right:64px}.afi{max-width:1320px}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.eg{margin-left:auto}.eh{text-align:center}.hn{margin-left:30px}.kg{float:none}.kh{margin-left:0}.ki{margin-right:0}.kj{width:100%}.yz{width:25px}.afd{padding-left:64px}.afe{padding-right:64px}.aff{max-width:1080px}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.hm{margin-left:30px}.yy{width:15px}.abc{margin-bottom:0px}.acn{margin-bottom:48px}.aek{display:block}.afa{padding-left:48px}.afb{padding-right:48px}.afc{max-width:904px}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ak{height:56px}.al{display:flex}.bp{display:block}.cw{margin-bottom:0px}.cx{height:110px}.fy{margin-top:32px}.fz{flex-direction:column-reverse}.hk{margin-bottom:30px}.hl{margin-left:0px}.pf{padding:10px 12px 10px}.yx{width:15px}.zh{margin-bottom:24px}.zi{align-items:center}.zj{width:102px}.zk{position:relative}.zn{padding-left:0}.zu{margin-top:24px}.aet{padding:32px 0}.aex{padding-left:24px}.aey{padding-right:24px}.aez{max-width:728px}.agd{width:140px}.age{margin-bottom:16px}.agf{margin-top:30px}.agg{width:100%}.agh{flex-direction:row}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.z{margin:0 24px}.eo{font-size:30px}.ep{margin-top:0.72em}.ez{line-height:40px}.ff{font-size:18px}.fg{margin-top:0.79em}.fq{line-height:24px}.fx{margin-top:32px}.gf{margin-bottom:0px}.hi{margin-bottom:30px}.hj{margin-left:0px}.ij{margin-top:1.08em}.iz{margin-top:1.56em}.jj{line-height:28px}.jp{padding-top:0}.jv{margin-top:40px}.ku{margin-top:1.2em}.ld{margin-top:0.67em}.ll{margin-top:1.34em}.mx{margin-top:1.42em}.nc{margin-top:2em}.ow{margin-top:24px}.uc{margin:0}.ud{max-width:100%}.uv{font-size:24px}.uw{margin-top:1.23em}.yw{width:15px}.abg{width:calc(100% + 24px)}.abh{margin-left:-12px}.abi{margin-right:-12px}.abv{padding-left:12px}.abw{padding-right:12px}.abx{flex-basis:100%}.aco{align-items:flex-start}.acp{flex-direction:row}.aeh{display:block}.aei{margin-bottom:16px}.aeu{padding-left:24px}.aev{padding-right:24px}.aew{max-width:552px}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ae{margin:0 64px}.eu{font-size:40px}.ev{margin-top:0.78em}.fc{line-height:48px}.fl{font-size:24px}.fm{margin-top:0.79em}.ft{line-height:32px}.im{margin-top:1.75em}.jd{font-size:21px}.je{margin-top:2em}.js{padding-top:7px}.jy{margin-top:56px}.ky{font-size:34px}.kz{margin-top:1.95em}.lg{margin-top:0.86em}.lo{margin-top:1.05em}.na{margin-top:2.75em}.nf{margin-top:3.14em}.oz{margin-top:32px}.ui{max-width:1192px}.vb{font-size:26px}.vc{margin-top:1.72em}.abp{width:calc(100% + 32px)}.abq{margin-left:-16px}.abr{margin-right:-16px}.ace{padding-left:16px}.acf{padding-right:16px}.acg{flex-basis:33.333333%}.ach{max-width:33.333333%}.acu{align-items:flex-start}.acv{flex-direction:row}.aec{flex-basis:100%}.aed{max-width:100%}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ac{margin:0 48px}.es{font-size:40px}.et{margin-top:0.78em}.fb{line-height:48px}.fj{font-size:24px}.fk{margin-top:0.79em}.fs{line-height:32px}.il{margin-top:1.75em}.jb{font-size:21px}.jc{margin-top:2em}.jr{padding-top:7px}.jx{margin-top:56px}.kw{font-size:34px}.kx{margin-top:1.95em}.lf{margin-top:0.86em}.ln{margin-top:1.05em}.mz{margin-top:2.75em}.ne{margin-top:3.14em}.oy{margin-top:32px}.ug{margin:0}.uh{max-width:100%}.uz{font-size:26px}.va{margin-top:1.72em}.abm{width:calc(100% + 28px)}.abn{margin-left:-14px}.abo{margin-right:-14px}.acb{padding-left:14px}.acc{padding-right:14px}.acd{flex-basis:100%}.acs{align-items:center}.act{flex-direction:row-reverse}.aea{flex-basis:50%}.aeb{max-width:50%}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ab{margin:0 24px}.eq{font-size:30px}.er{margin-top:0.72em}.fa{line-height:40px}.fh{font-size:18px}.fi{margin-top:0.79em}.fr{line-height:24px}.ik{margin-top:1.08em}.ja{margin-top:1.56em}.jk{line-height:28px}.jq{padding-top:0}.jw{margin-top:40px}.kv{margin-top:1.2em}.le{margin-top:0.67em}.lm{margin-top:1.34em}.my{margin-top:1.42em}.nd{margin-top:2em}.ox{margin-top:24px}.ue{margin:0}.uf{max-width:100%}.ux{font-size:24px}.uy{margin-top:1.23em}.abj{width:calc(100% + 24px)}.abk{margin-left:-12px}.abl{margin-right:-12px}.aby{padding-left:12px}.abz{padding-right:12px}.aca{flex-basis:100%}.acq{align-items:center}.acr{flex-direction:row-reverse}.acy{flex-basis:50%}.acz{max-width:50%}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="print">.y{display:none}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.wo{transition:opacity 200ms}.yg{transition:border-color 150ms ease}.yh::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.yi::before{border-radius:50%}.yj::before{content:""}.yk::before{display:block}.yl::before{z-index:0}.ym::before{left:0}.yn::before{height:100%}.yo::before{position:absolute}.yp::before{top:0}.yq::before{width:100%}.yr:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.yv{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 1230px)">.wp{display:none}</style><style type="text/css" data-fela-rehydration="839" data-fela-type="RULE" media="all and (max-width: 1198px)">.wr{display:none}</style><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*khIKl9t4XmZGSsKhW_Yg2w.png"],"url":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-3-recurrent-neural-networks-10300100899b","dateCreated":"2017-04-13T08:27:31.153Z","datePublished":"2017-04-13T08:27:31.153Z","dateModified":"2018-06-19T18:26:24.937Z","headline":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","name":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","description":"It seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way…","identifier":"10300100899b","keywords":["Lite:true","Tag:Machine Learning","Tag:Artificial Intelligence","Tag:Data Science","Tag:Deep Learning","Tag:Algorithms","Publication:a-year-of-artificial-intelligence","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Rohan Kapur","url":"https:\u002F\u002Fayearofai.com\u002F@mckapur"},"creator":["Rohan Kapur"],"publisher":{"@type":"Organization","name":"A Year of Artificial Intelligence","url":"ayearofai.com","logo":{"@type":"ImageObject","width":90,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F90\u002F1*NZsNSuNxe_O2YW1ybboOvA.jpeg"}},"mainEntityOfPage":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-3-recurrent-neural-networks-10300100899b"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y"><div><div class="r c"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="aj n o ak al"><div class="n o am w"><a href="https://medium.com/?source=post_page-----10300100899b----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div><div class="r an w"><div class="n o"><div class="n g"><div class="ec" id="lo-post-page-navbar-sign-in-link"><div class="ap r"><span class="aq b ar as at au r av aw"><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_page-----10300100899b---------------------nav_reg-" class="ax ay az ba bb bc bd be bf bg bh bi bj bk bl bm" rel="noopener">Sign in</a></span></div></div></div><div class="zt al zi"><div class="ec" id="lo-general-navbar-open-in-app-button"><div class="ap zt bp"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F10300100899b&amp;~feature=LoMobileNavBar&amp;~channel=ShowPostUnderCollection&amp;source=post_page-----10300100899b----------------------" class="ax ay az ba bb bc bd be bf bg bh bi bj bk agi agj" rel="noopener">Open in app</a></div></div></div><div class="ec" id="lo-post-page-navbar-sign-in-button"><div class="ap r"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_page-----10300100899b---------------------nav_reg-" class="ahu gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Get started</a></div></div></div></div></div></div></div></div><div class="bn r bo bp"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="bq br n o"><div class="bs r an"><a href="https://ayearofai.com/?source=post_page-----10300100899b----------------------" rel="noopener"><div class="bt bu r"><img alt="A Year of Artificial Intelligence" class="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_NZsNSuNxe_O2YW1ybboOvA.jpeg" width="54" height="36"></div></a></div><div class="bv r bw"><ul class="bx bf by bz ca n cb g cc cd ce"><li class="n o cf cg ch ci"><span class="aq cj ck as cl cm cn"><a href="https://ayearofai.com/tagged/algorithms?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg cq cr bj bk cs ct" rel="noopener">Algorithms</a></span></li><li class="n o cf cg ch ci"><span class="aq cj ck as cl cm cn"><a href="https://ayearofai.com/tagged/today-i-learned?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg cq cr bj bk cs ct" rel="noopener">Today I Learned</a></span></li><li class="n o cf cg ch ci"><span class="aq cj ck as cl cm cn"><a href="https://ayearofai.com/tagged/case-studies?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg cq cr bj bk cs ct" rel="noopener">Case Studies</a></span></li><li class="n o cf cg ch ci"><span class="aq cj ck as cl cm cn"><a href="https://ayearofai.com/tagged/philosophical?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg cq cr bj bk cs ct" rel="noopener">Philosophical</a></span></li><li class="n o cf cg ch ci"><span class="aq cj ck as cl cm cn"><a href="https://ayearofai.com/tagged/meta?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg cq cr bj bk cs ct" rel="noopener">Meta</a></span></li></ul></div></div></div></div></div></div></nav><div class="cu cv r cw cx"></div><article><section class="cy cz da db ai dc dd n de"></section><span class="r"></span><div><div class="s u df dg dh di"></div><div class="da db dc dw"><div class="r h g f e"><aside class="ahw s t" style="width: 572.5px;"><div class="ahz pr s aic bz ai"><h4 class="aq cj ck as av"><span class="hf pr bz br gh">Top highlight</span></h4></div></aside></div></div><section class="dj dk dl dm dn"><div class="do ai"><figure class="dp do ai paragraph-image"><div class="dv r dw dx"><div class="dy dz r"><div class="dq dr s t u ds ai br dt du"><img class="s t u ds ai ea eb ao aib" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_khIKl9t4XmZGSsKhW_Yg2w.png" width="2000" height="907" role="presentation"></div><img class="zx ahv s t u ds ai ed" width="2000" height="907" srcset="https://miro.medium.com/max/552/1*khIKl9t4XmZGSsKhW_Yg2w.png 276w, https://miro.medium.com/max/1104/1*khIKl9t4XmZGSsKhW_Yg2w.png 552w, https://miro.medium.com/max/1280/1*khIKl9t4XmZGSsKhW_Yg2w.png 640w, https://miro.medium.com/max/1456/1*khIKl9t4XmZGSsKhW_Yg2w.png 728w, https://miro.medium.com/max/1632/1*khIKl9t4XmZGSsKhW_Yg2w.png 816w, https://miro.medium.com/max/1808/1*khIKl9t4XmZGSsKhW_Yg2w.png 904w, https://miro.medium.com/max/1984/1*khIKl9t4XmZGSsKhW_Yg2w.png 992w, https://miro.medium.com/max/2160/1*khIKl9t4XmZGSsKhW_Yg2w.png 1080w, https://miro.medium.com/max/2700/1*khIKl9t4XmZGSsKhW_Yg2w.png 1350w, https://miro.medium.com/max/3240/1*khIKl9t4XmZGSsKhW_Yg2w.png 1620w, https://miro.medium.com/max/3780/1*khIKl9t4XmZGSsKhW_Yg2w.png 1890w, https://miro.medium.com/max/4000/1*khIKl9t4XmZGSsKhW_Yg2w.png 2000w" sizes="2000px" role="presentation" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_khIKl9t4XmZGSsKhW_Yg2w(1).png"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/4000/1*khIKl9t4XmZGSsKhW_Yg2w.png" width="2000" height="907" srcSet="https://miro.medium.com/max/552/1*khIKl9t4XmZGSsKhW_Yg2w.png 276w, https://miro.medium.com/max/1104/1*khIKl9t4XmZGSsKhW_Yg2w.png 552w, https://miro.medium.com/max/1280/1*khIKl9t4XmZGSsKhW_Yg2w.png 640w, https://miro.medium.com/max/1456/1*khIKl9t4XmZGSsKhW_Yg2w.png 728w, https://miro.medium.com/max/1632/1*khIKl9t4XmZGSsKhW_Yg2w.png 816w, https://miro.medium.com/max/1808/1*khIKl9t4XmZGSsKhW_Yg2w.png 904w, https://miro.medium.com/max/1984/1*khIKl9t4XmZGSsKhW_Yg2w.png 992w, https://miro.medium.com/max/2160/1*khIKl9t4XmZGSsKhW_Yg2w.png 1080w, https://miro.medium.com/max/2700/1*khIKl9t4XmZGSsKhW_Yg2w.png 1350w, https://miro.medium.com/max/3240/1*khIKl9t4XmZGSsKhW_Yg2w.png 1620w, https://miro.medium.com/max/3780/1*khIKl9t4XmZGSsKhW_Yg2w.png 1890w, https://miro.medium.com/max/4000/1*khIKl9t4XmZGSsKhW_Yg2w.png 2000w" sizes="2000px" role="presentation"/></noscript></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Sequences upon sequences upon sequences. Sequen-ception.</figcaption></figure></div><div class="n p"><div class="z ab ac ae af ej ah ai"><div><div id="84b9" class="ek el em ar en b eo ep eq er es et eu ev ew ex ey"><h1 class="en b eo ez eq fa es fb eu fc ew fd em">Rohan &amp; Lenny #3: Recurrent Neural Networks &amp; LSTMs</h1></div></div><h2 id="9485" class="fe el av ar aq cj ff fg fq fh fi fr fj fk fs fl fm ft fn fo fu fp">The ultimate guide to machine learning’s favorite child.</h2><div class="fv"><div class="n fw fx fy fz"><div class="o n"><div><a href="https://ayearofai.com/@mckapur?source=post_page-----10300100899b----------------------" rel="noopener"><img alt="Rohan Kapur" class="r ga gb gc" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/2_sT3MBbbeiRa900wKofa4_w.png" width="48" height="48"></a></div><div class="gd ai r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r em q"><div class="ge n o gf"><span class="aq cj ei as br gg gh gi gj gk em"><a href="https://ayearofai.com/@mckapur?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Rohan Kapur</a></span><div class="go r an h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=-cb55958ea3bb-------------------------follow_byline-" class="gp em q gq gr gs gt gu bg gm gv gw gx gy gz ha hb aq b ar hc ck au hd he dd hf hg bj" rel="noopener">Follow</a></div></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq cj ei as br gg gh gi gj gk av"><div><a class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener" href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b?source=post_page-----10300100899b----------------------">Apr 13, 2017</a> <!-- -->·<!-- --> <!-- -->85<!-- --> min read</div></span></span></div></div><div class="n hh hi hj hk hl hm hn ho y"><div class="n o"><div class="hp r an"><a href="https://medium.com/p/10300100899b/share/twitter?source=post_actions_header---------------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hp r an"><button class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hp r an"><a href="https://medium.com/p/10300100899b/share/facebook?source=post_actions_header---------------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="hs r"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div><div class="ht r am"></div></div></div></div></div></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><blockquote class="ie"><p id="4569" class="ip ig av ar en b iq ij ik il im in io" data-selectable-paragraph="">This is the third group (<a href="https://medium.com/u/de8e2540b759?source=post_page-----10300100899b----------------------" class="ir ax hg" target="_blank" rel="noopener">Lenny</a> and <a href="https://medium.com/u/cb55958ea3bb?source=post_page-----10300100899b----------------------" class="ir ax hg" target="_blank" rel="noopener">Rohan</a>) entry in our <a href="https://medium.com/a-year-of-artificial-intelligence" class="co hg is it iu iv" target="_blank" rel="noopener">journey</a> to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this <a href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" class="co hg is it iu iv" target="_blank" rel="noopener">introduction</a> post.</p></blockquote></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="a16a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io jh dj" data-selectable-paragraph=""><span class="r jl jm jn jo jp jq jr js jt dw">It</span> seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way. Whether our articles are more spaced out than we’d like them to be, well, we haven’t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we’ve been grinding on school (basically, getting it over and done with), banging out <a href="http://getcontra.com/" class="co hg is it iu iv" target="_blank" rel="noopener">Contra</a> v2, and lazing around more than we should. End of senior year is a fun time.</p><figure class="jv jw jx jy jz do ed ka he kb jl kc kd ke be kf kg kh ki kj kk paragraph-image"><div class="kl km dw kn ai"><div class="da db ju"><div class="dv r dw dx"><div class="ko dz r"><div class="dq dr s t u ds ai br dt du"><img class="s t u ds ai ea eb ao aib" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_N5l-Uj5yUVPIDwIuGUvnfg.jpeg" width="570" height="389" role="presentation"></div><img class="zx ahv s t u ds ai ed" width="570" height="389" srcset="https://miro.medium.com/max/552/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg 276w, https://miro.medium.com/max/1000/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg 500w" sizes="500px" role="presentation" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_N5l-Uj5yUVPIDwIuGUvnfg(1).jpeg"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1140/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg" width="570" height="389" srcSet="https://miro.medium.com/max/552/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg 276w, https://miro.medium.com/max/1000/1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg 500w" sizes="500px" role="presentation"/></noscript></div></div></div></div></figure><p id="93ae" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It’s 2017. We started A <strong class="iy kp"><em class="kq">Year</em></strong><em class="kq"> </em>Of AI in 2016. Last year. Don’t panic, though. If you’ve read our <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a">letter</a>, you’ll know that, despite our name and inception date, we’re not going anywhere anytime soon. There’s a good chance we’ll move off Medium, but we’re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.</p><p id="f6d1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I’ll probably be studying <a href="https://symsys.stanford.edu/" class="co hg is it iu iv" target="_blank" rel="noopener">Symbolic Systems</a>, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn 😀.</p><p id="b842" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="co hg is it iu iv" target="_blank" rel="noopener">favorite one</a>, personally, is from Andrej Karpathy’s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there’s space to simplify the topic even more, though. As usual, that’s our aim for the article — to teach you RNNs in a fun, simple manner. We’re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don’t worry, you’ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.</p><p id="60e2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Before we get started, you should try to familiarize yourself with “vanilla” neural networks. If you need a refresher, check out our <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d">neural networks and backpropogation mega-post</a> from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b">convolutional neural networks</a> may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b">this</a> article I wrote on vanishing gradients will help later on, as well.</p><p id="81ec" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Rule of thumb: the more you know, the better!</p><h1 id="3288" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Table of Contents</h1><p id="003f" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">I can’t link to each section, but here’s what we cover in this article (save the intro and conclusion):</p><ol class=""><li id="8cfa" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">What can RNNs do? </strong>Where we look at… what RNNs can do!</li><li id="7f98" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Why? </strong>Where we talk about the gap that RNNs fill in machine learning’s suite of algorithms.</li><li id="de84" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Show me. </strong>Where we visualize RNNs for the first time.</li><li id="f2f4" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Formalism. </strong>Where we walk through how an RNN mathematically works with proper notation.</li><li id="c163" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">An example? Okay! </strong>Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.</li><li id="394e" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Training (or, why vanilla RNNs suck.) </strong>Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.</li><li id="5756" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Fixing the problem with LSTMs (Part I). </strong>Where we introduce the solution to vanilla RNNs’ inability to learn: LSTMs.</li><li id="654d" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Fixing the problem with LSTMs (Part II). </strong>Where we analyze on a close, technical level, the reasons LSTMs don’t suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.</li><li id="2dfc" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Yay RNNs! </strong>Where <em class="kq">you </em>get to see neat little things RNNs have done!</li><li id="e56c" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">In Practice. </strong>Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot/recent research papers.</li><li id="7bc0" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph=""><strong class="iy kp">Building a Vanilla Recurrent Neural Network. </strong>Where you get to code your very first RNN! Woohoo!</li></ol><h1 id="02a8" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">What can RNNs do?</h1><p id="1c1d" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a <em class="kq">lot </em>more interesting things that have been presented in recent research papers (for example… <a href="https://arxiv.org/pdf/1606.04474.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">learning to learn by gradient descent by gradient descent</a>!).</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db lq"><div class="dv r dw dx"><div class="lr dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_X5dk-xGw2yNYsEB3QvHWIA.png" width="713" height="400" role="presentation"></div><img class="dq dr s t u ds ai ed" width="713" height="400" srcset="https://miro.medium.com/max/552/1*X5dk-xGw2yNYsEB3QvHWIA.png 276w, https://miro.medium.com/max/1104/1*X5dk-xGw2yNYsEB3QvHWIA.png 552w, https://miro.medium.com/max/1280/1*X5dk-xGw2yNYsEB3QvHWIA.png 640w, https://miro.medium.com/max/1400/1*X5dk-xGw2yNYsEB3QvHWIA.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1426/1*X5dk-xGw2yNYsEB3QvHWIA.png" width="713" height="400" srcSet="https://miro.medium.com/max/552/1*X5dk-xGw2yNYsEB3QvHWIA.png 276w, https://miro.medium.com/max/1104/1*X5dk-xGw2yNYsEB3QvHWIA.png 552w, https://miro.medium.com/max/1280/1*X5dk-xGw2yNYsEB3QvHWIA.png 640w, https://miro.medium.com/max/1400/1*X5dk-xGw2yNYsEB3QvHWIA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Image captioning, taken from CS231n slides: <a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf</a></figcaption></figure><p id="02ef" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">RNNs are very powerful. Y’know how regular neural networks have been proved to be “universal function approximators” ? If you didn’t:</p><blockquote class="ls lt lu"><p id="8206" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.</p></blockquote><p id="314e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it’s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but <a href="http://neuralnetworksanddeeplearning.com/chap4.html" class="co hg is it iu iv" target="_blank" rel="noopener">this</a> is a brilliant article offering a visual approach as to why<em class="kq"> </em>it’s true.</p><p id="a351" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, that’s great. ANNs are universal function approximators. RNNs take it a step further, though; <a href="http://stats.stackexchange.com/a/221142/98975" class="co hg is it iu iv" target="_blank" rel="noopener">they can compute/describe <em class="kq">programs</em></a>. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:</p><blockquote class="ls lt lu"><p id="8f10" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).</p><p id="c6a2" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, if somebody says “my new thing is Turing Complete” that means in principle (although <strong class="iy kp">often not in practice</strong>) it could be used to solve any computation problem.</p><p id="1429" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">— <a href="http://stackoverflow.com/a/7320/1260708" class="co hg is it iu iv" target="_blank" rel="noopener">http://stackoverflow.com/a/7320/1260708</a></p></blockquote><p id="2ba5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s cool, isn’t it? Now, this is all theoretical, and in practice means less than you think, so don’t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning — and why you should read on.</p><p id="662c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">At this point, if you weren’t previously hooked on learning what the heck these things are, you should be now. (If you still aren’t, just bare with me. Things will get spicy soon.) So, let’s dive in.</p><h1 id="72f6" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Why?</h1><p id="54dc" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">We took a bit of a detour to talk about how great RNNs are, but haven’t focused on <em class="kq">why</em> ANNs can’t perform well in the tasks that RNNs can.</p><blockquote class="ls lt lu"><p id="b88b" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?</p></blockquote><p id="95c9" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It boils down to a few things:</p><ul class=""><li id="3d40" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">ANNs can’t deal with sequential or “temporal” data</li><li id="2a22" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">ANNs lack memory</li><li id="0d57" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">ANNs have a fixed architecture</li><li id="fbb9" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">RNNs are more “biologically realistic” because of the recurrent connectivity found in the visual cortex of the brain</li></ul><p id="341a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s address the first three points individually. The first issue refers to the fact that ANNs have a <em class="kq">fixed input size </em>and a <em class="kq">fixed output size</em>. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of <em class="kq">variable</em> size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db lw"><div class="dv r dw dx"><div class="lx dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_BQ0SxdqC9Pl_3ZQtd3e45A.png" width="500" height="309" role="presentation"></div><img class="dq dr s t u ds ai ed" width="500" height="309" srcset="https://miro.medium.com/max/552/1*BQ0SxdqC9Pl_3ZQtd3e45A.png 276w, https://miro.medium.com/max/1000/1*BQ0SxdqC9Pl_3ZQtd3e45A.png 500w" sizes="500px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1000/1*BQ0SxdqC9Pl_3ZQtd3e45A.png" width="500" height="309" srcSet="https://miro.medium.com/max/552/1*BQ0SxdqC9Pl_3ZQtd3e45A.png 276w, https://miro.medium.com/max/1000/1*BQ0SxdqC9Pl_3ZQtd3e45A.png 500w" sizes="500px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">We might choose this architecture for our ANN, with 4 inputs and 1 output. But that’s it — we can’t input a vector with 5 values, for example. <a href="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" class="co hg is it iu iv" target="_blank" rel="noopener">https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4</a>.</figcaption></figure><p id="1c8e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I’ll give you a couple examples of why this matters.</p><p id="e228" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It’s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence — a list of words in a specific order — which is a<em class="kq"> sequence</em>.<em class="kq"> </em>It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a <em class="kq">single </em>word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn’t sound like a good idea.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ly"><div class="dv r dw dx"><div class="lz dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_GFVoFpD6cdCY_PGqnjhOlQ.png" width="305" height="182" role="presentation"></div><img class="dq dr s t u ds ai ed" width="305" height="182" srcset="https://miro.medium.com/max/552/1*GFVoFpD6cdCY_PGqnjhOlQ.png 276w, https://miro.medium.com/max/610/1*GFVoFpD6cdCY_PGqnjhOlQ.png 305w" sizes="305px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/610/1*GFVoFpD6cdCY_PGqnjhOlQ.png" width="305" height="182" srcSet="https://miro.medium.com/max/552/1*GFVoFpD6cdCY_PGqnjhOlQ.png 276w, https://miro.medium.com/max/610/1*GFVoFpD6cdCY_PGqnjhOlQ.png 305w" sizes="305px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">A reminder of what the output of an ANN looks like — a probability distribution over classes — and how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest 0.</figcaption></figure><p id="f8ec" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Wow, that was a lot of words. Nevertheless, I hope it’s clear that, with ANNs, there’s no feasible way to output a sequence.</p><p id="f308" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, what about <em class="kq">inputting </em>a sequence into an ANN? In other words, “temporal” data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it’s just one neuron that’s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn’t we input each “set of values” separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.</p><p id="8029" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s take the case of this utterly false, and most certainly negative sentence, to evaluate:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ma"><div class="dv r dw dx"><div class="mb dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_yq_zmka1ssikrmD9GkWmnw.png" width="625" height="139" role="presentation"></div><img class="dq dr s t u ds ai ed" width="625" height="139" srcset="https://miro.medium.com/max/552/1*yq_zmka1ssikrmD9GkWmnw.png 276w, https://miro.medium.com/max/1104/1*yq_zmka1ssikrmD9GkWmnw.png 552w, https://miro.medium.com/max/1250/1*yq_zmka1ssikrmD9GkWmnw.png 625w" sizes="625px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1250/1*yq_zmka1ssikrmD9GkWmnw.png" width="625" height="139" srcSet="https://miro.medium.com/max/552/1*yq_zmka1ssikrmD9GkWmnw.png 276w, https://miro.medium.com/max/1104/1*yq_zmka1ssikrmD9GkWmnw.png 552w, https://miro.medium.com/max/1250/1*yq_zmka1ssikrmD9GkWmnw.png 625w" sizes="625px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">This is just an alternative fact, believe me! Lenny is actually a <em class="mc">great</em><strong class="aq kt"><em class="mc"> </em></strong><em class="mc">coder. The best I know of. The best.</em></figcaption></figure><p id="322d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We’d input “Lenny” first, then “Khazan”, then “is”, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on <em class="kq">only </em>that word. We’d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.</p><p id="13ff" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Think of it this way — this means you’re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren’t linked in any way; they’re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it’s a collection of words put together in a specific order to form <em class="kq">meaning</em>. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having <em class="kq">memory</em>. ANNs have no memory.</p><p id="e71f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I like this quote from another article on RNNs:</p><blockquote class="ls lt lu"><p id="9cd3" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p><p id="b760" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">— <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="co hg is it iu iv" target="_blank" rel="noopener">http://colah.github.io/posts/2015–08-Understanding-LSTMs/</a></p></blockquote><p id="e0e4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">(Furthermore, take the case where we had sequential data in <em class="kq">both </em>the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren’t the answer.)</p><p id="fa2f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">RNNs don’t just need memory; they need <em class="kq">long term</em> memory. Let’s take the example of predictive typing. Let’s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db md"><div class="dv r dw dx"><div class="me dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_TugitPvwm_IZqAdAPR-7UA.png" width="543" height="90" role="presentation"></div><img class="dq dr s t u ds ai ed" width="543" height="90" srcset="https://miro.medium.com/max/552/1*TugitPvwm_IZqAdAPR-7UA.png 276w, https://miro.medium.com/max/1086/1*TugitPvwm_IZqAdAPR-7UA.png 543w" sizes="543px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1086/1*TugitPvwm_IZqAdAPR-7UA.png" width="543" height="90" srcSet="https://miro.medium.com/max/552/1*TugitPvwm_IZqAdAPR-7UA.png 276w, https://miro.medium.com/max/1086/1*TugitPvwm_IZqAdAPR-7UA.png 543w" sizes="543px" role="presentation"/></noscript></div></div></div></figure><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mf"><div class="dv r dw dx"><div class="mg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_7l0aNgpXDXZnY-P9C8K4IA.jpeg" width="250" height="250" role="presentation"></div><img class="dq dr s t u ds ai ed" width="250" height="250" srcset="" sizes="250px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/500/1*7l0aNgpXDXZnY-P9C8K4IA.jpeg" width="250" height="250" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">The face of a criminal?</figcaption></figure><p id="325c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here, if the RNN wasn’t able to look back much (ie. before “should”), then many different options could arise:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mh"><div class="dv r dw dx"><div class="mi dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_aMJu60wscb9m4A-Zn_xyqQ.png" width="552" height="266" role="presentation"></div><img class="dq dr s t u ds ai ed" width="552" height="266" srcset="https://miro.medium.com/max/552/1*aMJu60wscb9m4A-Zn_xyqQ.png 276w, https://miro.medium.com/max/1104/1*aMJu60wscb9m4A-Zn_xyqQ.png 552w" sizes="552px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1104/1*aMJu60wscb9m4A-Zn_xyqQ.png" width="552" height="266" srcSet="https://miro.medium.com/max/552/1*aMJu60wscb9m4A-Zn_xyqQ.png 276w, https://miro.medium.com/max/1104/1*aMJu60wscb9m4A-Zn_xyqQ.png 552w" sizes="552px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Lenny in the military? Make it into a TV show! I’d watch it.</figcaption></figure><p id="930e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The word “sent” would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word “criminal”, then it would be much more confident that:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mj"><div class="dv r dw dx"><div class="mk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_4CZskdiGqIx29BQylqnYuA.png" width="557" height="102" role="presentation"></div><img class="dq dr s t u ds ai ed" width="557" height="102" srcset="https://miro.medium.com/max/552/1*4CZskdiGqIx29BQylqnYuA.png 276w, https://miro.medium.com/max/1104/1*4CZskdiGqIx29BQylqnYuA.png 552w, https://miro.medium.com/max/1114/1*4CZskdiGqIx29BQylqnYuA.png 557w" sizes="557px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1114/1*4CZskdiGqIx29BQylqnYuA.png" width="557" height="102" srcSet="https://miro.medium.com/max/552/1*4CZskdiGqIx29BQylqnYuA.png 276w, https://miro.medium.com/max/1104/1*4CZskdiGqIx29BQylqnYuA.png 552w, https://miro.medium.com/max/1114/1*4CZskdiGqIx29BQylqnYuA.png 557w" sizes="557px" role="presentation"/></noscript></div></div></div></figure><p id="c9f1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The probability of outputting “jail” drastically increases when it sees the word “criminal” is present. That’s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to “forget” or “retain” (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.</p><p id="eb76" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As it turns out, RNNs — especially deep ones — are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That’s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.</p><p id="c00c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">To address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. <strong class="iy kp">Exciting stuff.</strong></p><h1 id="4abf" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Show me.</h1><p id="0160" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">OK, that’s enough teasing. Three sections into the article, and you’re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!</p><p id="d391" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The first thing I’m going to do is show you what a normal ANN diagram looks like:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ml"><div class="dv r dw dx"><div class="mm dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_GapzcZDrwnVbflhlRoWZ9g.png" width="310" height="220" role="presentation"></div><img class="dq dr s t u ds ai ed" width="310" height="220" srcset="https://miro.medium.com/max/552/1*GapzcZDrwnVbflhlRoWZ9g.png 276w, https://miro.medium.com/max/620/1*GapzcZDrwnVbflhlRoWZ9g.png 310w" sizes="310px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/620/1*GapzcZDrwnVbflhlRoWZ9g.png" width="310" height="220" srcSet="https://miro.medium.com/max/552/1*GapzcZDrwnVbflhlRoWZ9g.png 276w, https://miro.medium.com/max/620/1*GapzcZDrwnVbflhlRoWZ9g.png 310w" sizes="310px" role="presentation"/></noscript></div></div></div></figure><p id="deda" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Each neuron stores a single scalar value. Thus, each layer can be considered a vector.</p><p id="f077" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now I’m going to show you what this ANN looks like in our RNN visual notation:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mn"><div class="dv r dw dx"><div class="mo dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_ntKLnv52DCUnkseNcm91iQ.png" width="300" height="65" role="presentation"></div><img class="dq dr s t u ds ai ed" width="300" height="65" srcset="https://miro.medium.com/max/552/1*ntKLnv52DCUnkseNcm91iQ.png 276w, https://miro.medium.com/max/600/1*ntKLnv52DCUnkseNcm91iQ.png 300w" sizes="300px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/600/1*ntKLnv52DCUnkseNcm91iQ.png" width="300" height="65" srcSet="https://miro.medium.com/max/552/1*ntKLnv52DCUnkseNcm91iQ.png 276w, https://miro.medium.com/max/600/1*ntKLnv52DCUnkseNcm91iQ.png 300w" sizes="300px" role="presentation"/></noscript></div></div></div></figure><p id="f1fc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That’s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a <strong class="iy kp">vector </strong>of information. The term “cell” is also used, and is interchangeable with neuron. (I’ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron’s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.</p><p id="abee" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s flip it the other way:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mp"><div class="dv r dw dx"><div class="mq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_HewvhrMCcdy-oQcpeeNJ9w.png" width="50" height="231" role="presentation"></div><img class="dq dr s t u ds ai ed" width="50" height="231" srcset="" sizes="50px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/100/1*HewvhrMCcdy-oQcpeeNJ9w.png" width="50" height="231" role="presentation"/></noscript></div></div></div></figure><p id="8a15" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is in fact a type of recurrent neural network — a <strong class="iy kp">one to one</strong> recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.</p><p id="b960" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We can have a one to <em class="kq">many</em> recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning — the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mr"><div class="dv r dw dx"><div class="ms dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_-Jv3TxauJBwBgWwjoe_UkA.png" width="190" height="230" role="presentation"></div><img class="dq dr s t u ds ai ed" width="190" height="230" srcset="" sizes="190px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/380/1*-Jv3TxauJBwBgWwjoe_UkA.png" width="190" height="230" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Changed the shades of the green nodes… hope that’s OK!</figcaption></figure><p id="ea3c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This may be confusing at first, so I’m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mt"><div class="dv r dw dx"><div class="mu dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_OEyIsiEi5SJ9l3GrB5DpuA.png" width="323" height="287" role="presentation"></div><img class="dq dr s t u ds ai ed" width="323" height="287" srcset="https://miro.medium.com/max/552/1*OEyIsiEi5SJ9l3GrB5DpuA.png 276w, https://miro.medium.com/max/646/1*OEyIsiEi5SJ9l3GrB5DpuA.png 323w" sizes="323px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/646/1*OEyIsiEi5SJ9l3GrB5DpuA.png" width="323" height="287" srcSet="https://miro.medium.com/max/552/1*OEyIsiEi5SJ9l3GrB5DpuA.png 276w, https://miro.medium.com/max/646/1*OEyIsiEi5SJ9l3GrB5DpuA.png 323w" sizes="323px" role="presentation"/></noscript></div></div></div></figure><p id="22b2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When I refer to “time” on the x-axis, I’m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say “depth” on the y-axis, I’m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.</p><p id="f94b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple “timesteps” where they take on different values, which are, again, vectors. The input neuron in our example above doesn’t, because it’s not representing sequential data (one to many), but for other architectures it could.</p><p id="e200" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The hidden neuron will take on the vector value <strong class="iy kp">h_1</strong> first, then <strong class="iy kp">h_2</strong>, and finally <strong class="iy kp">h_3</strong>. At each timestep, the hidden neuron’s vector <strong class="iy kp">h_t </strong>is a function of the vector at the previous timestep <strong class="iy kp">h_t-1</strong>, except for <strong class="iy kp">h_1 </strong>which is dependent <em class="kq">only</em> on the input <strong class="iy kp">x_1</strong>. In the diagram above, each hidden vector then gives rise to an output <strong class="iy kp">y_t</strong>, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.</p><p id="64eb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron — be it input, hidden, or output — at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.</p><p id="5402" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The RNN would execute like so:</p><ol class=""><li id="3623" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io li lj lk" data-selectable-paragraph="">Input <strong class="iy kp">x_1</strong></li><li id="1923" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">h_1</strong> based on <strong class="iy kp">x_1</strong> (the arrow implies functional dependency)</li><li id="00ff" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">h_2</strong> based on <strong class="iy kp">h_1</strong></li><li id="ca4c" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">h_3</strong> based on <strong class="iy kp">h_2</strong></li><li id="9fac" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">y_1</strong> based on <strong class="iy kp">h_1</strong></li><li id="649b" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">y_2</strong> based on <strong class="iy kp">h_2</strong></li><li id="ba63" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io li lj lk" data-selectable-paragraph="">Compute <strong class="iy kp">y_3</strong> based on <strong class="iy kp">h_3</strong></li></ol><p id="bba5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You could compute <strong class="iy kp">y_t </strong>either immediately after <strong class="iy kp">h_t </strong>has been computed, or, like above, compute all outputs once all hidden states have been computed. I’m not entirely sure which is more common in practice.</p><p id="6c68" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.</p><p id="e78f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It’s actually 2, because the RNN would need to output a period or &lt;END&gt; marker at the final timestep, but we’ll get into that later.)</p><p id="a009" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In case you don’t understand yet exactly <em class="kq">why </em>RNNs work, I’ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db mv"><div class="dv r dw dx"><div class="mw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_iBtLegQFwfsqWZpTVAjrEw.jpeg" width="1298" height="782" role="presentation"></div><img class="dq dr s t u ds ai ed" width="1298" height="782" srcset="https://miro.medium.com/max/552/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 276w, https://miro.medium.com/max/1104/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 552w, https://miro.medium.com/max/1280/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 640w, https://miro.medium.com/max/1400/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/2596/1*iBtLegQFwfsqWZpTVAjrEw.jpeg" width="1298" height="782" srcSet="https://miro.medium.com/max/552/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 276w, https://miro.medium.com/max/1104/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 552w, https://miro.medium.com/max/1280/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 640w, https://miro.medium.com/max/1400/1*iBtLegQFwfsqWZpTVAjrEw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Lenny and I on student scholarship at WWDC 2013. Good times!</figcaption></figure><p id="6b74" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When you combine an RNN and CNN, you — in practice — get an “LCRN”. The architecture for LCRNs are more complex than what I’m going to present in the next paragraph; rather, I’m going to simplify it to convey my point. We’ll actually get fully into how they work later.</p><p id="e029" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Imagine an RNN tries to caption this image. An accurate result might be:</p><blockquote class="ie"><p id="f14e" class="ip ig av ar en b iq mx my mz na nb io" data-selectable-paragraph="">Two people happily posing for a photo inside a building.</p></blockquote><p id="482a" class="iw ji em ar iy b ff nc jj fh nd jk jb ne fs jd nf ft jf ng fu io dj" data-selectable-paragraph="">The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer — that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state<strong class="iy kp">¹</strong> of the recurrent neural network to be one where the most likely candidate word is “two”.</p><p id="f5ec" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Pro-tip<strong class="iy kp">¹</strong>: The term “hidden state” refers to the vector of a hidden neuron at a given timestep. “First hidden state” refers to the hidden state at timestep 1.</p><p id="10db" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The first output, which represents the word “two”, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, “two” was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, “people”, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the <em class="kq">first </em>hidden state. This means that the word “people” was the most likely candidate given the hidden state where “two” was likely. In other words, the RNN recognized that, given the word “two”, the word “people” should be next, based on the RNN’s experience from training and the initial image [analysis] we inputted.</p><p id="b004" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.</p><p id="d51d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">To put it bluntly, you can boil down what the RNN is “thinking” to this:</p><blockquote class="ls lt lu"><p id="4a94" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><mark class="ahx ahy xj">Based on what I’ve seen from the input, based on the current timestep I’m at, and based on what I know from all my training, I need to output: </mark><mark class="ahx ahy xj"><strong class="iy kp">“x”.</strong></mark></p></blockquote><p id="a918" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It’s indirect because the outputs are only dependent on the hidden states, <em class="kq">not </em>on each other (ie. the RNN doesn’t deduce “people” from “two”, it deduces “people”, partly, from the information — the hidden state — that <em class="kq">gave rise</em> to “two”). In LCRNs, though, this is explicit instead of implicit; we “sample” the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.</p><p id="cdc3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The exact quantitative relationships depend on the RNN’s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.</p><blockquote class="ls lt lu"><p id="0d89" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Yep, I went to France for a holiday. And I actually learned to speak some &lt;wait, shit, what was the language again? oh yea, “France”…&gt; French!</p></blockquote><p id="5857" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren’t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.</p><p id="34be" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.</p><p id="05f6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So far we’ve looked at one to one and one to many recurrent networks. We can also have <em class="kq">many to one</em>:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mr"><div class="dv r dw dx"><div class="nh dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_KjYQyc-JD_zs5ERQypM9EA.png" width="190" height="228" role="presentation"></div><img class="dq dr s t u ds ai ed" width="190" height="228" srcset="" sizes="190px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/380/1*KjYQyc-JD_zs5ERQypM9EA.png" width="190" height="228" role="presentation"/></noscript></div></div></div></figure><p id="da0a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on <em class="kq">both </em>the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after <strong class="iy kp">h_1 </strong>is only dependent on the previous hidden state. That’s why, in the image above, the second hidden state has two arrows directed at it.</p><p id="4715" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.</p><p id="a55f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The final type of recurrent net is many to many, where both the input and output are sequential:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ml"><div class="dv r dw dx"><div class="ni dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_MPpLCBI1J6r6VmsDm7G4_g.png" width="310" height="214" role="presentation"></div><img class="dq dr s t u ds ai ed" width="310" height="214" srcset="https://miro.medium.com/max/552/1*MPpLCBI1J6r6VmsDm7G4_g.png 276w, https://miro.medium.com/max/620/1*MPpLCBI1J6r6VmsDm7G4_g.png 310w" sizes="310px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/620/1*MPpLCBI1J6r6VmsDm7G4_g.png" width="310" height="214" srcSet="https://miro.medium.com/max/552/1*MPpLCBI1J6r6VmsDm7G4_g.png 276w, https://miro.medium.com/max/620/1*MPpLCBI1J6r6VmsDm7G4_g.png 310w" sizes="310px" role="presentation"/></noscript></div></div></div></figure><p id="7038" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.</p><p id="2538" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db nj"><div class="dv r dw dx"><div class="nk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_vfUWsAgW-c5hUntaPhb1aQ.png" width="313" height="305" role="presentation"></div><img class="dq dr s t u ds ai ed" width="313" height="305" srcset="https://miro.medium.com/max/552/1*vfUWsAgW-c5hUntaPhb1aQ.png 276w, https://miro.medium.com/max/626/1*vfUWsAgW-c5hUntaPhb1aQ.png 313w" sizes="313px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/626/1*vfUWsAgW-c5hUntaPhb1aQ.png" width="313" height="305" srcSet="https://miro.medium.com/max/552/1*vfUWsAgW-c5hUntaPhb1aQ.png 276w, https://miro.medium.com/max/626/1*vfUWsAgW-c5hUntaPhb1aQ.png 313w" sizes="313px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">We’re getting deeper and deeper!</figcaption></figure><p id="2703" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Really, this could be considered as <em class="kq">multiple RNNs</em>. Technically, you can consider each “hidden layer” as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I’ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.</p><p id="a0dc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced <em class="kq">after</em> the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn’t just dependent on the first word of the inputted English; it’s dependent on the <em class="kq">entire </em>sentence.</p><p id="98d9" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">One way to demonstrate why this matters is to use Google Translate:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ej"><div class="dv r dw dx"><div class="nl dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_mptNrzbgaDT3YuQL-tpEOw.png" width="680" height="145" role="presentation"></div><img class="dq dr s t u ds ai ed" width="680" height="145" srcset="https://miro.medium.com/max/552/1*mptNrzbgaDT3YuQL-tpEOw.png 276w, https://miro.medium.com/max/1104/1*mptNrzbgaDT3YuQL-tpEOw.png 552w, https://miro.medium.com/max/1280/1*mptNrzbgaDT3YuQL-tpEOw.png 640w, https://miro.medium.com/max/1360/1*mptNrzbgaDT3YuQL-tpEOw.png 680w" sizes="680px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1360/1*mptNrzbgaDT3YuQL-tpEOw.png" width="680" height="145" srcSet="https://miro.medium.com/max/552/1*mptNrzbgaDT3YuQL-tpEOw.png 276w, https://miro.medium.com/max/1104/1*mptNrzbgaDT3YuQL-tpEOw.png 552w, https://miro.medium.com/max/1280/1*mptNrzbgaDT3YuQL-tpEOw.png 640w, https://miro.medium.com/max/1360/1*mptNrzbgaDT3YuQL-tpEOw.png 680w" sizes="680px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">One of my favorite <strong class="aq kt">Green Day</strong> lyrics, from the song “Fashion Victim” on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is <em class="mc">probably </em>off.</figcaption></figure><p id="3229" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now I’ll input “He’s a victim” and “of his own time” separately. You’ll notice that when you join the two translated outputs, this won’t be equal to the corresponding phrase in the first translation:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db nm"><div class="dv r dw dx"><div class="nn dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_lO5oCsSXTy4Loic3SASMlw.png" width="276" height="245" role="presentation"></div><img class="dq dr s t u ds ai ed" width="276" height="245" srcset="" sizes="276px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/552/1*lO5oCsSXTy4Loic3SASMlw.png" width="276" height="245" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">What happens if we break up the English into different parts, translate, and join together the translated Chinese parts?</figcaption></figure><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db no"><div class="dv r dw dx"><div class="np dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_oJRFlstyAI-MkBguW6otfA.png" width="551" height="110" role="presentation"></div><img class="dq dr s t u ds ai ed" width="551" height="110" srcset="https://miro.medium.com/max/552/1*oJRFlstyAI-MkBguW6otfA.png 276w, https://miro.medium.com/max/1102/1*oJRFlstyAI-MkBguW6otfA.png 551w" sizes="551px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1102/1*oJRFlstyAI-MkBguW6otfA.png" width="551" height="110" srcSet="https://miro.medium.com/max/552/1*oJRFlstyAI-MkBguW6otfA.png 276w, https://miro.medium.com/max/1102/1*oJRFlstyAI-MkBguW6otfA.png 551w" sizes="551px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><strong class="aq kt">They’re not equal.</strong></figcaption></figure><p id="a661" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it’s used. It all depends on the <strong class="iy kp">context </strong>and the entire sentence as a whole — the meaning you’re trying to convey. This is the exact approach a human translator would take.</p><p id="fad3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Another type of many to many architecture exists where each neuron has a state at every timestep, in a “synchronized” fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn’t be suitable for translation.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mr"><div class="dv r dw dx"><div class="nq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_84IkP_dqLUfImZ5SyZLwjA.png" width="190" height="231" role="presentation"></div><img class="dq dr s t u ds ai ed" width="190" height="231" srcset="" sizes="190px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/380/1*84IkP_dqLUfImZ5SyZLwjA.png" width="190" height="231" role="presentation"/></noscript></div></div></div></figure><p id="cef1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note — an RNN is better at this task than CNNs are because what’s going on in a scene is much easier to understand if you’ve watched the video up to that point and thus can contextualize it. <strong class="iy kp">That’s what humans do!</strong></p><p id="f0a6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Quick note: we can “wrap” the RNN into a much more succinct form, where we collapse the depth and time properties, like so:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db mr"><div class="dv r dw dx"><div class="nr dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_7POP9GXAsRlbRRrsrhr-jA.png" width="190" height="250" role="presentation"></div><img class="dq dr s t u ds ai ed" width="190" height="250" srcset="" sizes="190px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/380/1*7POP9GXAsRlbRRrsrhr-jA.png" width="190" height="250" role="presentation"/></noscript></div></div></div></div></figure><p id="fc78" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it’s sort of like a loop that feeds itself.</p><p id="3ba7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When you ever read about “unrolling” an RNN into a feedforward network that looks like it’s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.</p><p id="0d76" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Another quick note: when somebody or a research paper mentions that they are using “512 RNN units”, this translates to: “1 RNN neuron that outputs a 512-wide vector”; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it’s luckily much simpler than that… albeit strangely worded.</p><p id="2ae1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Furthermore, one “RNN unit” usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: “stacking RNNs on top of each other”. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps <strong class="iy kp">t </strong>and fixed layers <strong class="iy kp">ℓ</strong>, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.</p><h1 id="e5b5" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Formalism</h1><p id="a93d" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">So, now, let’s walk through the formal mathematical notation involved in RNNs.</p><p id="ad6a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If an input or output neuron has a value at timestep <strong class="iy kp">t</strong>, we denote the vector as:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ns"><div class="dv r dw dx"><div class="nt dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1__D9bOLepOSSbC2zgK7wreQ.png" width="141" height="77" role="presentation"></div><img class="dq dr s t u ds ai ed" width="141" height="77" srcset="" sizes="141px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/282/1*_D9bOLepOSSbC2zgK7wreQ.png" width="141" height="77" role="presentation"/></noscript></div></div></div></figure><p id="a6fd" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">For the hidden neurons it’s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep <strong class="iy kp">t </strong>and hidden layer <strong class="iy kp">ℓ </strong>as:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db nu"><div class="dv r dw dx"><div class="nv dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_QJFWCdOVxGAge0ZT17hw1g.png" width="157" height="41" role="presentation"></div><img class="dq dr s t u ds ai ed" width="157" height="41" srcset="" sizes="157px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/314/1*QJFWCdOVxGAge0ZT17hw1g.png" width="157" height="41" role="presentation"/></noscript></div></div></div></figure><p id="7ba4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The input is obviously some preset values that we know. The outputs and hidden states are<em class="kq"> not</em>; they are calculated.</p><p id="982f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s start with hidden states. First, we’ll revisit the most complex recurrent net we came across earlier — the many to many architecture:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mf"><div class="dv r dw dx"><div class="nw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_TJxcM6GI8dMHEq6sK3Ky8Q.png" width="250" height="206" role="presentation"></div><img class="dq dr s t u ds ai ed" width="250" height="206" srcset="" sizes="250px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/500/1*TJxcM6GI8dMHEq6sK3Ky8Q.png" width="250" height="206" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Many to many, non-synchronized.</figcaption></figure><p id="5f3c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.</p><p id="af44" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">First, let’s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:</p><ul class=""><li id="5021" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">An input</li><li id="8970" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">Hidden state at the previous timestep, same layer</li><li id="429d" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">Hidden state at the current timestep, previous layer</li></ul><p id="dc21" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.</p><p id="b721" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.</p><p id="5a0a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db nx"><div class="dv r dw dx"><div class="ny dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_n5QR9Q9ZGnWFRf7pROtliA.png" width="400" height="107" role="presentation"></div><img class="dq dr s t u ds ai ed" width="400" height="107" srcset="https://miro.medium.com/max/552/1*n5QR9Q9ZGnWFRf7pROtliA.png 276w, https://miro.medium.com/max/800/1*n5QR9Q9ZGnWFRf7pROtliA.png 400w" sizes="400px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/800/1*n5QR9Q9ZGnWFRf7pROtliA.png" width="400" height="107" srcSet="https://miro.medium.com/max/552/1*n5QR9Q9ZGnWFRf7pROtliA.png 276w, https://miro.medium.com/max/800/1*n5QR9Q9ZGnWFRf7pROtliA.png 400w" sizes="400px" role="presentation"/></noscript></div></div></div></figure><p id="7d12" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This probably looks a bit confusing; let me break it down for you. The function <strong class="iy kp">ƒw </strong>computes the numeric hidden state vector for timestep <strong class="iy kp">t </strong>and layer <strong class="iy kp">ℓ</strong>; it contains the “activation function” you’re used to hearing about with ANNs. <strong class="iy kp">W </strong>are the weights of the recurrent net, and thus <strong class="iy kp">ƒ </strong>is conditioned on <strong class="iy kp">W</strong>. We haven’t exactly defined <strong class="iy kp">ƒ</strong> just yet, but what’s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:</p><blockquote class="ls lt lu"><p id="a42e" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Where <strong class="iy kp">ℓ </strong>= 1, the hidden state at time <strong class="iy kp">t </strong>and layer <strong class="iy kp">ℓ </strong>is a function of the hidden state vector at time <strong class="iy kp">t-1 </strong>and layer <strong class="iy kp">ℓ </strong>as well as the input vector at time <strong class="iy kp">t</strong>. Where <strong class="iy kp">ℓ &gt; 1</strong>, this hidden state is a function of the hidden state vector at time <strong class="iy kp">t-1 </strong>and layer <strong class="iy kp">ℓ </strong>as well as the hidden state vector at time <strong class="iy kp">t</strong>, layer <strong class="iy kp">ℓ-1</strong>.</p></blockquote><p id="7713" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You might notice that we have a couple issues:</p><ul class=""><li id="69f7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">When <strong class="iy kp">t = 1</strong> — that is, when each neuron is at the initial timestep — then no previous timestep exists. However, we still attempt to pass <strong class="iy kp">h_0 </strong>as a parameter to <strong class="iy kp">ƒw</strong>.</li><li id="7b09" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">If no input exists at time <strong class="iy kp">t</strong> — thus, <strong class="iy kp">x_t </strong>does not exist — then we still attempt to pass <strong class="iy kp">x_t </strong>as a parameter.</li></ul><p id="2062" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Our respective solutions follow:</p><ul class=""><li id="c5c1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">Define <strong class="iy kp">h_0 </strong>for any layer as 0</li><li id="68f6" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">Consider <strong class="iy kp">x_t </strong>where no input exists at timestep <strong class="iy kp">t</strong> as 0</li></ul><p id="0ed3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.</p><p id="cf77" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We actually have five different types of weight matrices:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db nz"><div class="dv r dw dx"><div class="oa dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_r09EQtFlEA1kIiOJD2aZ6g.png" width="390" height="233" role="presentation"></div><img class="dq dr s t u ds ai ed" width="390" height="233" srcset="https://miro.medium.com/max/552/1*r09EQtFlEA1kIiOJD2aZ6g.png 276w, https://miro.medium.com/max/780/1*r09EQtFlEA1kIiOJD2aZ6g.png 390w" sizes="390px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/780/1*r09EQtFlEA1kIiOJD2aZ6g.png" width="390" height="233" srcSet="https://miro.medium.com/max/552/1*r09EQtFlEA1kIiOJD2aZ6g.png 276w, https://miro.medium.com/max/780/1*r09EQtFlEA1kIiOJD2aZ6g.png 390w" sizes="390px" role="presentation"/></noscript></div></div></div></figure><p id="b66b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. <strong class="iy kp">W_xh </strong>maps an input vector <strong class="iy kp">x </strong>to a hidden state vector <strong class="iy kp">h</strong>. <strong class="iy kp">W_hht </strong>maps a hidden state vector <strong class="iy kp">h</strong> to another hidden state vector <strong class="iy kp">h </strong>along the time axis, ie. from <strong class="iy kp">h_t-1 </strong>to <strong class="iy kp">h_t</strong>. On the other hand, <strong class="iy kp">W_hhd </strong>maps a hidden state vector <strong class="iy kp">h </strong>to another hidden state vector <strong class="iy kp">h </strong>along the depth axis, ie. from <strong class="iy kp">h^(ℓ-1)_t </strong>to <strong class="iy kp">h^ℓ_t</strong>. <strong class="iy kp">W_hy </strong>maps a hidden state vector <strong class="iy kp">h </strong>to an output vector <strong class="iy kp">y</strong>.</p><p id="283a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Like with ANNs, we also learn and add a constant bias vector, denoted <strong class="iy kp">b_h</strong>, that can vertically shift what we pass to the activation function. We can also shift our outputs with <strong class="iy kp">b_y</strong>. More about bias units <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52">here</a>.</p><p id="246f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">For both <strong class="iy kp">b_h </strong>and <strong class="iy kp">W_hht/W_hhd</strong>, we actually have multiple weight matrices depending on the value of <strong class="iy kp">ℓ</strong>, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn’t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values — 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn’t have anything to use. Since this would also mean that the number of parameters in the neural network would grow linearly relative to the input, we would have way too many parameters very potentially causing overfitting.</p><p id="6da9" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">W_hy </strong>is just one matrix because only the final layer gives rise to the outputs denoted <strong class="iy kp">y</strong>. At the final hidden layer <strong class="iy kp">ℓ</strong>, we could suggest that <strong class="iy kp">W_hhd </strong>will not exist because <strong class="iy kp">W_hy </strong>will be in its place.</p><p id="e3ed" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now we’ll define the function <strong class="iy kp">ƒw</strong>:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db ob"><div class="dv r dw dx"><div class="oc dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_9YGuqXdNiknmZR2HScMDKw.png" width="404" height="226" role="presentation"></div><img class="dq dr s t u ds ai ed" width="404" height="226" srcset="https://miro.medium.com/max/552/1*9YGuqXdNiknmZR2HScMDKw.png 276w, https://miro.medium.com/max/808/1*9YGuqXdNiknmZR2HScMDKw.png 404w" sizes="404px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/808/1*9YGuqXdNiknmZR2HScMDKw.png" width="404" height="226" srcSet="https://miro.medium.com/max/552/1*9YGuqXdNiknmZR2HScMDKw.png 276w, https://miro.medium.com/max/808/1*9YGuqXdNiknmZR2HScMDKw.png 404w" sizes="404px" role="presentation"/></noscript></div></div></div></div></figure><p id="e06a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The function is very similar to the ANN hidden function you’ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or “squashing” function to introduce non-linearities. The key difference, though, is that this is not a weighted sum<em class="kq"> </em>but rather a weighted sum <em class="kq">vector</em>; any <strong class="iy kp">W ⋅ h</strong>, along with the bias,<strong class="iy kp"> </strong>will have the dimensions of a vector. The <strong class="iy kp">tanh </strong>function will thus simply output a vector where each value is the tanh<strong class="iy kp"> </strong>of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.</p><p id="d904" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If you’ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs — more on that later), the fact that they produce gradients with a greater range, and that their second derivative don’t die off as quickly.</p><p id="25ba" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db od"><div class="dv r dw dx"><div class="oe dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_NPI9iLLVlYLQ2gu9A9xp0A.png" width="324" height="166" role="presentation"></div><img class="dq dr s t u ds ai ed" width="324" height="166" srcset="https://miro.medium.com/max/552/1*NPI9iLLVlYLQ2gu9A9xp0A.png 276w, https://miro.medium.com/max/648/1*NPI9iLLVlYLQ2gu9A9xp0A.png 324w" sizes="324px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/648/1*NPI9iLLVlYLQ2gu9A9xp0A.png" width="324" height="166" srcSet="https://miro.medium.com/max/552/1*NPI9iLLVlYLQ2gu9A9xp0A.png 276w, https://miro.medium.com/max/648/1*NPI9iLLVlYLQ2gu9A9xp0A.png 324w" sizes="324px" role="presentation"/></noscript></div></div></div></div></figure><p id="9d21" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If interested, the tanh equation follows (though I won’t walk you through it):</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db of"><div class="dv r dw dx"><div class="og dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_w7LV9vY1hCAXcLk2K_peEg.png" width="241" height="84" role="presentation"></div><img class="dq dr s t u ds ai ed" width="241" height="84" srcset="" sizes="241px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/482/1*w7LV9vY1hCAXcLk2K_peEg.png" width="241" height="84" role="presentation"/></noscript></div></div></div></figure><p id="85eb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The final equation is mapping a hidden state to an output.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db oh"><div class="dv r dw dx"><div class="oi dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_n7simJp73WxCRx_Bz4dXwg.png" width="177" height="44" role="presentation"></div><img class="dq dr s t u ds ai ed" width="177" height="44" srcset="" sizes="177px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/354/1*n7simJp73WxCRx_Bz4dXwg.png" width="177" height="44" role="presentation"/></noscript></div></div></div></figure><p id="93b1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is <em class="kq">one</em> such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.</p><p id="3584" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And that’s how we express recurrent nets, mathematically!</p><p id="2650" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example — some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is <em class="kq">much </em>more general than mine to promote simplicity, ie. doesn’t cover edge cases like I did or obfuscates certain indices like <strong class="iy kp">ℓ </strong>with hidden to hidden weight matrices. So, just keep note that specifics don’t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.</p><h1 id="ed09" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">An example? Okay!</h1><p id="e53b" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Let’s take a look at a quick example of an RNN in action. I’m going to adapt a super dumbed down one from Andrej Karpathy’s Stanford CS231n <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="co hg is it iu iv" target="_blank" rel="noopener">RNN lecture</a>, where a one to many “character level language model” single layer recurrent neural network needs to output “hello”. We’ll kick it of by giving the RNN the letter “h” , such that it needs to complete the word by outputting the other four letters.</p><p id="1901" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Sidenote: this model nicknamed “char-rnn” — remember it for later, where we get to code our own!</p><p id="fc9c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word “hello”. We will input the first character, “h”, and from there expect the output at the following timesteps to be: “e”, “l”, “l”, and “o” respectively, to form:</p><blockquote class="ie"><p id="56af" class="ip ig av ar en b iq mx my mz na nb io" data-selectable-paragraph="">hello</p></blockquote><p id="9f5f" class="iw ji em ar iy b ff nc jj fh nd jk jb ne fs jd nf ft jf ng fu io dj" data-selectable-paragraph="">We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent “h”, “e”, “l”, and “o” respectively.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db oj"><div class="dv r dw dx"><div class="ok dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_pgWSPyximAFHqZtUkiLeKg.png" width="440" height="192" role="presentation"></div><img class="dq dr s t u ds ai ed" width="440" height="192" srcset="https://miro.medium.com/max/552/1*pgWSPyximAFHqZtUkiLeKg.png 276w, https://miro.medium.com/max/880/1*pgWSPyximAFHqZtUkiLeKg.png 440w" sizes="440px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/880/1*pgWSPyximAFHqZtUkiLeKg.png" width="440" height="192" srcSet="https://miro.medium.com/max/552/1*pgWSPyximAFHqZtUkiLeKg.png 276w, https://miro.medium.com/max/880/1*pgWSPyximAFHqZtUkiLeKg.png 440w" sizes="440px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">This is called “one-hot encoding”, because only one of the values in the vector is equal to 1 and thus on (or “hot”).</figcaption></figure><p id="c682" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is what we’d expect with a trained RNN:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mf"><div class="dv r dw dx"><div class="ol dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_mmuQb8msqqQLtz580_lpvw.png" width="250" height="227" role="presentation"></div><img class="dq dr s t u ds ai ed" width="250" height="227" srcset="" sizes="250px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/500/1*mmuQb8msqqQLtz580_lpvw.png" width="250" height="227" role="presentation"/></noscript></div></div></div></figure><p id="17a3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As you can see, we input the first letter and the word is completed. We don’t know exactly what the hidden states will be — that’s why they’re hidden!</p><p id="04ad" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">One interesting technique would be to sample the output at each timestep and feed it into the next as input:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mf"><div class="dv r dw dx"><div class="om dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_KyVSttLGcexQWDLvSWD0Lg.png" width="250" height="226" role="presentation"></div><img class="dq dr s t u ds ai ed" width="250" height="226" srcset="" sizes="250px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/500/1*KyVSttLGcexQWDLvSWD0Lg.png" width="250" height="226" role="presentation"/></noscript></div></div></div></figure><p id="08da" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When we “sample” from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is “e” at the first timestep’s output. Let’s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep’s input, there’s a 90% chance we select “e”; <em class="kq">most</em> of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don’t end up in a loop where you keep sampling the same letter or sequence of letters over and over again.</p><p id="0880" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As mentioned earlier, this is used pretty heavily with LCRNs. It’s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)</p><p id="9cac" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">However, to be clear, this does not mean that the RNN can <em class="kq">only </em>rely on these sampled inputs. For example, at timestep 3 the input is “l” and the expected output is also “l”. However, at timestep 4, the input is again “l” but the output is now “o”, to complete the word. Memory is still needed to make a distinction like this.</p><p id="db16" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In numerical form, it would look something like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db on"><div class="dv r dw dx"><div class="oo dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_AguGRuRZg6e6RZ7Ctvn2Ww.png" width="280" height="371" role="presentation"></div><img class="dq dr s t u ds ai ed" width="280" height="371" srcset="https://miro.medium.com/max/552/1*AguGRuRZg6e6RZ7Ctvn2Ww.png 276w, https://miro.medium.com/max/560/1*AguGRuRZg6e6RZ7Ctvn2Ww.png 280w" sizes="280px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/560/1*AguGRuRZg6e6RZ7Ctvn2Ww.png" width="280" height="371" srcSet="https://miro.medium.com/max/552/1*AguGRuRZg6e6RZ7Ctvn2Ww.png 276w, https://miro.medium.com/max/560/1*AguGRuRZg6e6RZ7Ctvn2Ww.png 280w" sizes="280px" role="presentation"/></noscript></div></div></div></figure><p id="c815" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Of course, we won’t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we’d apply softmax to the output), and will sample from this distribution to get a single character output.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db mn"><div class="dv r dw dx"><div class="op dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_6067M6Oqz2zNoyyyQC1Suw.png" width="300" height="574" role="presentation"></div><img class="dq dr s t u ds ai ed" width="300" height="574" srcset="https://miro.medium.com/max/552/1*6067M6Oqz2zNoyyyQC1Suw.png 276w, https://miro.medium.com/max/600/1*6067M6Oqz2zNoyyyQC1Suw.png 300w" sizes="300px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/600/1*6067M6Oqz2zNoyyyQC1Suw.png" width="300" height="574" srcSet="https://miro.medium.com/max/552/1*6067M6Oqz2zNoyyyQC1Suw.png 276w, https://miro.medium.com/max/600/1*6067M6Oqz2zNoyyyQC1Suw.png 300w" sizes="300px" role="presentation"/></noscript></div></div></div></figure><p id="9419" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.</p><p id="58bf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The RNN is saying: given “h”, “e” is most likely to be the next character. Given “he”, “l” is the next likely character. With “hel”, “l” should be next, and with “hell”, the final character should be “o”.</p><p id="b7ec" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.</p><p id="399a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">One more important thing to note: <strong class="iy kp">start and end tokens</strong>. They signify when input begins and when output ends. For example, when the final character is outputted (“o”), we can sample this back as input and expect that the “&lt;END&gt;” token (however we choose to represent it — could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn’t as obvious in this fabricated example, because we know when “hello” has been completed, but consider a real-life scenario where we don’t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using <code class="dx oq or os ot b">while</code> or stop after the upper limit/max possible preset constant value of n is reached).</p><p id="a127" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we’ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a “&lt;START&gt;” token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.</p><p id="85be" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I’ve also noticed that another potential use case of start tokens is when we have some other sort of <em class="kq">initial </em>input, like CNN produced image data with image captioning, that doesn’t “fit” what we’ll normally use for input at timesteps after <strong class="iy kp">t=1 </strong>(the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as “&lt;START&gt;” instead.</p><p id="2aca" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, just to be clear, the RNN doesn’t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.</p><p id="1c00" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is how we can get RNNs to “write”! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.</p><h1 id="6b00" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Training (or, why vanilla RNNs suck.)</h1><p id="58b5" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">For a recurrent net to be useful, it needs to learn proper weights via training. That’s no surprise.</p><p id="8a33" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Recall this snippet from earlier:</p><blockquote class="ls lt lu"><p id="f72f" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.</p></blockquote><p id="f9d1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is, of course, because we initialize the <strong class="iy kp">W </strong>weights randomly at first, so random stuff will come out.</p><p id="c7a6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be “hello” in one-hot encoding form, and we’d compute the discrepancy between this output and what the recurrent net predicts (we’d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.</p><p id="2ded" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like <strong class="iy kp">Y</strong> outputs, we’d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we’re differentiating a sum:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ou"><div class="dv r dw dx"><div class="ov dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_d5mzuu-EmcZz0IFukW6XsQ.png" width="651" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="651" height="79" srcset="https://miro.medium.com/max/552/1*d5mzuu-EmcZz0IFukW6XsQ.png 276w, https://miro.medium.com/max/1104/1*d5mzuu-EmcZz0IFukW6XsQ.png 552w, https://miro.medium.com/max/1280/1*d5mzuu-EmcZz0IFukW6XsQ.png 640w, https://miro.medium.com/max/1302/1*d5mzuu-EmcZz0IFukW6XsQ.png 651w" sizes="651px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1302/1*d5mzuu-EmcZz0IFukW6XsQ.png" width="651" height="79" srcSet="https://miro.medium.com/max/552/1*d5mzuu-EmcZz0IFukW6XsQ.png 276w, https://miro.medium.com/max/1104/1*d5mzuu-EmcZz0IFukW6XsQ.png 552w, https://miro.medium.com/max/1280/1*d5mzuu-EmcZz0IFukW6XsQ.png 640w, https://miro.medium.com/max/1302/1*d5mzuu-EmcZz0IFukW6XsQ.png 651w" sizes="651px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">For any arbitrary weight <strong class="aq kt">W</strong>.</figcaption></figure><p id="bb6e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, you should know that, with artificial neural networks, calculating these gradients isn’t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).</p><p id="c4f4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading <a href="http://sebastianruder.com/optimizing-gradient-descent/" class="co hg is it iu iv" target="_blank" rel="noopener">this article</a>, if you want. (I think we’re long overdue for our own mega-post on optimization!)</p><p id="0be4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Backpropagation with RNNs is called “Backpropagation Through Time” (short for BPTT), since it operates on sequences in time. But don’t be fooled — there’s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you “unroll” an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it’s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth <em class="kq">as well as </em>time. There’s more work to do to compute the gradients, but it’s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I’m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.</p><p id="224b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the <strong class="iy kp">W_hh </strong>for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.</p><p id="1fee" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, if you haven’t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:</p><div class="ow ox oy oz pa pb"><a target="_blank" rel="noopener" href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b"><div class="pc n an"><div class="pd n de p pe pf"><h2 class="aq kt pg as br ph gh gi pi gk em">Rohan #4: The vanishing gradient problem</h2><div class="pj r"><h3 class="aq cj ei as br ph gh gi pi gk av">Oh no — an obstacle to deep learning!</h3></div><div class="pk r"><h4 class="aq cj ck as br ph gh gi pi gk av">ayearofai.com</h4></div></div><div class="pl r"><div class="pm r pn po pp pl pq pr ps"></div></div></div></a></div><p id="be11" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have <em class="kq">hundreds</em> of timesteps. That’s like an ANN with hundreds of entire hidden layers! That’s <em class="kq">deep</em>.<em class="kq"> </em>(Well, it’s more <em class="kq">long </em>because we’re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.</p><p id="4878" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Imagine trying to propagate the error to the 1st timestep in an RNN with <strong class="iy kp">k</strong> timesteps. The derivative would look something like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db pt"><div class="dv r dw dx"><div class="pu dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_gKbRtQfPwGK2d7jnKZdv5w.png" width="346" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="346" height="79" srcset="https://miro.medium.com/max/552/1*gKbRtQfPwGK2d7jnKZdv5w.png 276w, https://miro.medium.com/max/692/1*gKbRtQfPwGK2d7jnKZdv5w.png 346w" sizes="346px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/692/1*gKbRtQfPwGK2d7jnKZdv5w.png" width="346" height="79" srcSet="https://miro.medium.com/max/552/1*gKbRtQfPwGK2d7jnKZdv5w.png 276w, https://miro.medium.com/max/692/1*gKbRtQfPwGK2d7jnKZdv5w.png 346w" sizes="346px" role="presentation"/></noscript></div></div></div></figure><p id="0e72" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With a tanh activation function, that’s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix <strong class="iy kp">W_hh</strong>, we’d add — or, as mentioned before, we could average as well — each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db pv"><div class="dv r dw dx"><div class="pw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_jf52uXcsAX6Nn8ghLYoJWQ.png" width="773" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="773" height="80" srcset="https://miro.medium.com/max/552/1*jf52uXcsAX6Nn8ghLYoJWQ.png 276w, https://miro.medium.com/max/1104/1*jf52uXcsAX6Nn8ghLYoJWQ.png 552w, https://miro.medium.com/max/1280/1*jf52uXcsAX6Nn8ghLYoJWQ.png 640w, https://miro.medium.com/max/1400/1*jf52uXcsAX6Nn8ghLYoJWQ.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1546/1*jf52uXcsAX6Nn8ghLYoJWQ.png" width="773" height="80" srcSet="https://miro.medium.com/max/552/1*jf52uXcsAX6Nn8ghLYoJWQ.png 276w, https://miro.medium.com/max/1104/1*jf52uXcsAX6Nn8ghLYoJWQ.png 552w, https://miro.medium.com/max/1280/1*jf52uXcsAX6Nn8ghLYoJWQ.png 640w, https://miro.medium.com/max/1400/1*jf52uXcsAX6Nn8ghLYoJWQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Assuming our sequence is of length <strong class="aq kt">k</strong>.</figcaption></figure><p id="9baa" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So we’d be effectively adding together a bunch of terms that have vanished — the exception being very late gradients with a small number of terms — and so <strong class="iy kp">dJ/dWhh </strong>would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).</p><p id="fd7e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, you might be asking, instead of tanh — which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 — why don’t we just use ReLUs? Don’t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?</p><p id="6a76" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Well, not entirely; it’s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish — or vice-versa, explode — we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.</p><p id="4faf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is more my suspicion though — I’m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="mg dz r"></div></div></figure><p id="e537" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">From this, something interesting I learned is that: since ReLUs are <em class="kq">un</em>bounded (it’s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn’t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.</p><p id="b4fc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what <em class="kq">then</em> causes the gradients to explode: large activations → large gradients → large change in weights → even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:</p><blockquote class="ls lt lu"><p id="2c04" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that’s why you see most people using sigmoid/tanh units, despite the vanishing gradient descent problem.</p></blockquote><p id="4f2a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Also well said:</p><blockquote class="ls lt lu"><p id="e86a" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With RNN’s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.</p></blockquote><p id="4cb6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don’t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.</p><p id="4a29" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And that’s why <strong class="iy kp">vanilla RNNs suck</strong>. Seriously. In practice, nobody uses them. Even if you didn’t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn’t matter anyways. Because, everything you’ve read up to this point so far… throw it all away. Forget about it.</p><p id="044a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Just kidding. Don’t do that.</p><h1 id="1d2b" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Fixing the problem with LSTMs (Part I)</h1><p id="daad" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">You shouldn’t do that because RNNs actually <em class="kq">aren’t </em>a lost cause. They’re far from it. We just need to make a few… modifications.</p><p id="9491" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Enter the LSTM.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db px"><div class="dv r dw dx"><div class="py dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_JuC5afKk7QIntsvyEn-IFA.png" width="298" height="170" role="presentation"></div><img class="dq dr s t u ds ai ed" width="298" height="170" srcset="https://miro.medium.com/max/552/1*JuC5afKk7QIntsvyEn-IFA.png 276w, https://miro.medium.com/max/596/1*JuC5afKk7QIntsvyEn-IFA.png 298w" sizes="298px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/596/1*JuC5afKk7QIntsvyEn-IFA.png" width="298" height="170" srcSet="https://miro.medium.com/max/552/1*JuC5afKk7QIntsvyEn-IFA.png 276w, https://miro.medium.com/max/596/1*JuC5afKk7QIntsvyEn-IFA.png 298w" sizes="298px" role="presentation"/></noscript></div></div></div></figure><p id="731a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Makes sense, no?</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db pz"><div class="dv r dw dx"><div class="qa dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_tB6QdzunJV08wyep0ZhVMA.png" width="350" height="357" role="presentation"></div><img class="dq dr s t u ds ai ed" width="350" height="357" srcset="https://miro.medium.com/max/552/1*tB6QdzunJV08wyep0ZhVMA.png 276w, https://miro.medium.com/max/700/1*tB6QdzunJV08wyep0ZhVMA.png 350w" sizes="350px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/700/1*tB6QdzunJV08wyep0ZhVMA.png" width="350" height="357" srcSet="https://miro.medium.com/max/552/1*tB6QdzunJV08wyep0ZhVMA.png 276w, https://miro.medium.com/max/700/1*tB6QdzunJV08wyep0ZhVMA.png 350w" sizes="350px" role="presentation"/></noscript></div></div></div></figure><p id="03ff" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">How about this?</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db nx"><div class="dv r dw dx"><div class="qb dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_Oin8uuuQzp_wqtHAX1yyjQ.png" width="400" height="314" role="presentation"></div><img class="dq dr s t u ds ai ed" width="400" height="314" srcset="https://miro.medium.com/max/552/1*Oin8uuuQzp_wqtHAX1yyjQ.png 276w, https://miro.medium.com/max/800/1*Oin8uuuQzp_wqtHAX1yyjQ.png 400w" sizes="400px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/800/1*Oin8uuuQzp_wqtHAX1yyjQ.png" width="400" height="314" srcSet="https://miro.medium.com/max/552/1*Oin8uuuQzp_wqtHAX1yyjQ.png 276w, https://miro.medium.com/max/800/1*Oin8uuuQzp_wqtHAX1yyjQ.png 400w" sizes="400px" role="presentation"/></noscript></div></div></div></figure><p id="0d8c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">OK. Clearly something’s not registering here. But that’s fine; LSTM diagrams are frikin’ difficult for beginners to grasp. I too remember when I first searched up “LSTM” on Google to encounter something similar to the works of art above. I reacted like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qc"><div class="dv r dw dx"><div class="qd dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_S7ABwK33X7no_MP3epry6A.gif" width="195" height="229" role="presentation"></div><img class="dq dr s t u ds ai ed" width="195" height="229" srcset="" sizes="195px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/390/1*S7ABwK33X7no_MP3epry6A.gif" width="195" height="229" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">MRW first Google Image-ing LSTMs.</figcaption></figure><p id="7cd4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In this section, I’m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I’ll probably fail.</p><p id="b288" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With that being said, let’s dive into <strong class="iy kp">Long Short-Term Memory networks</strong>. (Yes, that’s what LSTM stands for.)</p></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="bb27" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With RNNs, the real “substance” of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and/or previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden “layer”.</p><p id="66c2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If you need a refresher on this, look through the “Formalism” section once again.</p><p id="cd18" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With LSTMs, we still have hidden states, but they’re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell’s value (the “cell state”) at that timestep. Each cell state is in turn functionally dependent on the previous<em class="kq"> </em>cell state and any available input or previous hidden states. That’s right — hidden states are computed from cell states, and cell states are (in part) computed from older and/or shallower hidden states.</p><p id="095f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The cell state at a specific timestep <strong class="iy kp">t </strong>is denoted <strong class="iy kp">c_t</strong>. Like a hidden state, a cell state is just a vector.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db lw"><div class="dv r dw dx"><div class="qe dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_sr8XQzvr-WTNSbgWwI9qbQ.png" width="500" height="245" role="presentation"></div><img class="dq dr s t u ds ai ed" width="500" height="245" srcset="https://miro.medium.com/max/552/1*sr8XQzvr-WTNSbgWwI9qbQ.png 276w, https://miro.medium.com/max/1000/1*sr8XQzvr-WTNSbgWwI9qbQ.png 500w" sizes="500px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1000/1*sr8XQzvr-WTNSbgWwI9qbQ.png" width="500" height="245" srcSet="https://miro.medium.com/max/552/1*sr8XQzvr-WTNSbgWwI9qbQ.png 276w, https://miro.medium.com/max/1000/1*sr8XQzvr-WTNSbgWwI9qbQ.png 500w" sizes="500px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">For simplicity’s sake, I’ve obfuscated layer index <strong class="aq kt">ℓ</strong>.</figcaption></figure><p id="68f5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If the diagram above seems a bit trippy, let me break it down for you.</p><p id="8bd2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">c_t</strong>, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:</p><ul class=""><li id="4f8f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">The previous hidden state in time: <strong class="iy kp">h_t-1</strong>. Again, if <strong class="iy kp">t = 1</strong>, then this won’t exist. If it does, this would be the first arrow pointing into the left side of <strong class="iy kp">c_t</strong>.</li><li id="f283" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">The previous cell state: <strong class="iy kp">c_t-1</strong>. If <strong class="iy kp">t = 1</strong>, the dependency obviously won’t exist. This refers to the second arrow pointing into the left side of <strong class="iy kp">c_t</strong>.</li><li id="7237" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">Input at the current timestep: <strong class="iy kp">x_t</strong>. There may very well be no input available, for example if we are at a hidden layer <strong class="iy kp">ℓ &gt; 1</strong>. So this dependency doesn’t always exist. When it does, it’s the arrow pointing into the bottom of <strong class="iy kp">c_t</strong>.</li><li id="83f3" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">The previous hidden state in depth: <strong class="iy kp">h^(ℓ-1)_t</strong>. This applies for any hidden layer <strong class="iy kp">ℓ &gt; 1</strong>. In such case, it would — like the input <strong class="iy kp">x_t</strong> — be the arrow pointing into the bottom.</li></ul><p id="2a71" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Only three can exist at once because the last two are mutually exclusive.</p><p id="5160" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">From there, we pass information to the next cell state <strong class="iy kp">c_t+1</strong> and compute <strong class="iy kp">h_t</strong>. As you can hopefully see, <strong class="iy kp">h_t</strong> then goes on to also influence <strong class="iy kp">c_t+1 </strong>(as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).</p><p id="0f0f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Right now the cells are a black box… literally; we know what is inputted to them and what they output, but we don’t know their internal process. So… what’s inside these cells? What do they do? What are the exact computations involved? How have the equations changed?</p><p id="ee47" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a <strong class="iy kp">conveyer belt</strong>. Think of, hell, I don’t know — chicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qf"><div class="dv r dw dx"><div class="qg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_4avrG18SFOMJGI4CpDIsoA.png" width="490" height="600" role="presentation"></div><img class="dq dr s t u ds ai ed" width="490" height="600" srcset="https://miro.medium.com/max/552/1*4avrG18SFOMJGI4CpDIsoA.png 276w, https://miro.medium.com/max/980/1*4avrG18SFOMJGI4CpDIsoA.png 490w" sizes="490px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/980/1*4avrG18SFOMJGI4CpDIsoA.png" width="490" height="600" srcSet="https://miro.medium.com/max/552/1*4avrG18SFOMJGI4CpDIsoA.png 276w, https://miro.medium.com/max/980/1*4avrG18SFOMJGI4CpDIsoA.png 490w" sizes="490px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">I’m not sure what product this conveyer belt carries, but it certainly doesn’t look appetizing (or like chicken nuggets).</figcaption></figure><p id="02ea" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You’re thinking: “OK Rohan, but how does this relate to LSTMs?”. Good question.</p><p id="9004" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget — or, the cell state value.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qh"><div class="dv r dw dx"><div class="qi dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_qNUGFMhlnl0-mNLIVvyGAg.png" width="459" height="95" role="presentation"></div><img class="dq dr s t u ds ai ed" width="459" height="95" srcset="https://miro.medium.com/max/552/1*qNUGFMhlnl0-mNLIVvyGAg.png 276w, https://miro.medium.com/max/918/1*qNUGFMhlnl0-mNLIVvyGAg.png 459w" sizes="459px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/918/1*qNUGFMhlnl0-mNLIVvyGAg.png" width="459" height="95" srcSet="https://miro.medium.com/max/552/1*qNUGFMhlnl0-mNLIVvyGAg.png 276w, https://miro.medium.com/max/918/1*qNUGFMhlnl0-mNLIVvyGAg.png 459w" sizes="459px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Chicken. Nugget.</figcaption></figure><p id="bca7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It’s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‘modified’ is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully <strong class="iy kp"><em class="kq">transforms</em></strong> it by applying a function over it. LSTM cells instead take information and make minor <strong class="iy kp"><em class="kq">modifications</em></strong> (like additions or multiplications) to it while it flows through.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qj"><div class="dv r dw dx"><div class="qk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_I_nQdhxdoDa7KrZBTFeHSQ.png" width="330" height="241" role="presentation"></div><img class="dq dr s t u ds ai ed" width="330" height="241" srcset="https://miro.medium.com/max/552/1*I_nQdhxdoDa7KrZBTFeHSQ.png 276w, https://miro.medium.com/max/660/1*I_nQdhxdoDa7KrZBTFeHSQ.png 330w" sizes="330px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/660/1*I_nQdhxdoDa7KrZBTFeHSQ.png" width="330" height="241" srcSet="https://miro.medium.com/max/552/1*I_nQdhxdoDa7KrZBTFeHSQ.png 276w, https://miro.medium.com/max/660/1*I_nQdhxdoDa7KrZBTFeHSQ.png 330w" sizes="330px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Ew. Vanilla RNNs.</figcaption></figure><p id="72ea" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Vanilla RNNs look something like this. And it’s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ql"><div class="dv r dw dx"><div class="qm dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_360GYNV8kyF5ATWefrSasA.png" width="450" height="276" role="presentation"></div><img class="dq dr s t u ds ai ed" width="450" height="276" srcset="https://miro.medium.com/max/552/1*360GYNV8kyF5ATWefrSasA.png 276w, https://miro.medium.com/max/900/1*360GYNV8kyF5ATWefrSasA.png 450w" sizes="450px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/900/1*360GYNV8kyF5ATWefrSasA.png" width="450" height="276" srcSet="https://miro.medium.com/max/552/1*360GYNV8kyF5ATWefrSasA.png 276w, https://miro.medium.com/max/900/1*360GYNV8kyF5ATWefrSasA.png 450w" sizes="450px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">LSTMs 💦 💦 💦</figcaption></figure><p id="0a23" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is an extreme a simplification — and I’ll go on to fill in the blanks later — but it’s sort of what an LSTM looks like. The previous timestep’s cell state value flows through and instead of transforming the information, we tweak it by <em class="kq">adding </em>(another vector) to it. The added term is some function <strong class="iy kp">ƒw </strong>of previous information, but this is <strong class="iy kp"><em class="kq">not</em></strong> the same function as with vanilla RNNs — it’s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.</p><p id="ba34" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through<strong class="iy kp"> ƒw</strong>, it’s added to the information flowing towards <strong class="iy kp">c_t</strong>. Thus, in equation form it could look something like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qn"><div class="dv r dw dx"><div class="qo dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_qGqSrpJmO5h6ZGeIT7RK3w.png" width="208" height="39" role="presentation"></div><img class="dq dr s t u ds ai ed" width="208" height="39" srcset="" sizes="208px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/416/1*qGqSrpJmO5h6ZGeIT7RK3w.png" width="208" height="39" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Again… sort of. We’ll get into the <strong class="aq kt">actual</strong><em class="mc"> equations soon. This is a good proxy to convey my point.</em></figcaption></figure><p id="c99b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With a bit of substitution, we can expand this to:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qp"><div class="dv r dw dx"><div class="qq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_OZs7rDSty0VhDhzTLH4Dgg.png" width="444" height="39" role="presentation"></div><img class="dq dr s t u ds ai ed" width="444" height="39" srcset="https://miro.medium.com/max/552/1*OZs7rDSty0VhDhzTLH4Dgg.png 276w, https://miro.medium.com/max/888/1*OZs7rDSty0VhDhzTLH4Dgg.png 444w" sizes="444px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/888/1*OZs7rDSty0VhDhzTLH4Dgg.png" width="444" height="39" srcSet="https://miro.medium.com/max/552/1*OZs7rDSty0VhDhzTLH4Dgg.png 276w, https://miro.medium.com/max/888/1*OZs7rDSty0VhDhzTLH4Dgg.png 444w" sizes="444px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express <strong class="aq kt">c_t </strong>for some large value of <strong class="aq kt">t </strong>as a really really really really long function of, ultimately, <strong class="aq kt">c_1</strong>.</figcaption></figure><p id="48be" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Why is this better? Well, if you have basic differentiation knowledge, you’ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it’ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates “gradient super-highways”, where gradients can flow back super easily.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db qr"><div class="dv r dw dx"><div class="qs dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_n26drGfEkc-Xnmqc2Lw7cw.png" width="2161" height="690" role="presentation"></div><img class="dq dr s t u ds ai ed" width="2161" height="690" srcset="https://miro.medium.com/max/552/1*n26drGfEkc-Xnmqc2Lw7cw.png 276w, https://miro.medium.com/max/1104/1*n26drGfEkc-Xnmqc2Lw7cw.png 552w, https://miro.medium.com/max/1280/1*n26drGfEkc-Xnmqc2Lw7cw.png 640w, https://miro.medium.com/max/1400/1*n26drGfEkc-Xnmqc2Lw7cw.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/4322/1*n26drGfEkc-Xnmqc2Lw7cw.png" width="2161" height="690" srcSet="https://miro.medium.com/max/552/1*n26drGfEkc-Xnmqc2Lw7cw.png 276w, https://miro.medium.com/max/1104/1*n26drGfEkc-Xnmqc2Lw7cw.png 552w, https://miro.medium.com/max/1280/1*n26drGfEkc-Xnmqc2Lw7cw.png 640w, https://miro.medium.com/max/1400/1*n26drGfEkc-Xnmqc2Lw7cw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Look — it’s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the <strong class="aq kt"><em class="mc">whole unrolled LSTM</em></strong> as well. Each cell state is a subsection of the conveyer belt.)</figcaption></figure><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qt"><div class="dv r dw dx"><div class="qu dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_szBIWNdr0O0doBI8rfGjzw.png" width="610" height="193" role="presentation"></div><img class="dq dr s t u ds ai ed" width="610" height="193" srcset="https://miro.medium.com/max/552/1*szBIWNdr0O0doBI8rfGjzw.png 276w, https://miro.medium.com/max/1104/1*szBIWNdr0O0doBI8rfGjzw.png 552w, https://miro.medium.com/max/1220/1*szBIWNdr0O0doBI8rfGjzw.png 610w" sizes="610px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1220/1*szBIWNdr0O0doBI8rfGjzw.png" width="610" height="193" srcSet="https://miro.medium.com/max/552/1*szBIWNdr0O0doBI8rfGjzw.png 276w, https://miro.medium.com/max/1104/1*szBIWNdr0O0doBI8rfGjzw.png 552w, https://miro.medium.com/max/1220/1*szBIWNdr0O0doBI8rfGjzw.png 610w" sizes="610px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Look — it’s an outdated machine learning algorithm!</figcaption></figure><p id="68da" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it’ll easily flow back all the way to the beginning. Contributions by the <strong class="iy kp">ƒw </strong>function will be made to this gradient flowing on the bottom conveyer belt as well.</p><p id="779d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is what gradient flow would look like:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db qv"><div class="dv r dw dx"><div class="qw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_dqOCXyepO590ORWV3VgBKw.png" width="740" height="262" role="presentation"></div><img class="dq dr s t u ds ai ed" width="740" height="262" srcset="https://miro.medium.com/max/552/1*dqOCXyepO590ORWV3VgBKw.png 276w, https://miro.medium.com/max/1104/1*dqOCXyepO590ORWV3VgBKw.png 552w, https://miro.medium.com/max/1280/1*dqOCXyepO590ORWV3VgBKw.png 640w, https://miro.medium.com/max/1400/1*dqOCXyepO590ORWV3VgBKw.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1480/1*dqOCXyepO590ORWV3VgBKw.png" width="740" height="262" srcSet="https://miro.medium.com/max/552/1*dqOCXyepO590ORWV3VgBKw.png 276w, https://miro.medium.com/max/1104/1*dqOCXyepO590ORWV3VgBKw.png 552w, https://miro.medium.com/max/1280/1*dqOCXyepO590ORWV3VgBKw.png 640w, https://miro.medium.com/max/1400/1*dqOCXyepO590ORWV3VgBKw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7ea0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly &lt; 1, as is usually the case for us) or explode (if they are mostly &gt; 1). Here’s some real calculus to demonstrate this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qx"><div class="dv r dw dx"><div class="qy dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_09oGK1btsVezIoMyBAwqbw.png" width="287" height="288" role="presentation"></div><img class="dq dr s t u ds ai ed" width="287" height="288" srcset="https://miro.medium.com/max/552/1*09oGK1btsVezIoMyBAwqbw.png 276w, https://miro.medium.com/max/574/1*09oGK1btsVezIoMyBAwqbw.png 287w" sizes="287px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/574/1*09oGK1btsVezIoMyBAwqbw.png" width="287" height="288" srcSet="https://miro.medium.com/max/552/1*09oGK1btsVezIoMyBAwqbw.png 276w, https://miro.medium.com/max/574/1*09oGK1btsVezIoMyBAwqbw.png 287w" sizes="287px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Former is akin to RNNs. Latter is akin to LSTMs.</figcaption></figure><p id="0175" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Imagine <strong class="iy kp">f </strong>being any sort of function, like our <strong class="iy kp">ƒw</strong>. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won’t vanish or explode quickly, so our LSTMs won’t vanish or explode quickly. Yay!</p><p id="4e50" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Furthermore, if some of our gradients vanish — for whatever reason — then it should still be OK. It won’t be optimal, but since our gradient terms add together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 × 0 = 0.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qz"><div class="dv r dw dx"><div class="ra dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_K7rYONPTfcpCb0xTvO3ydw.jpeg" width="466" height="240" role="presentation"></div><img class="dq dr s t u ds ai ed" width="466" height="240" srcset="https://miro.medium.com/max/552/1*K7rYONPTfcpCb0xTvO3ydw.jpeg 276w, https://miro.medium.com/max/932/1*K7rYONPTfcpCb0xTvO3ydw.jpeg 466w" sizes="466px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/932/1*K7rYONPTfcpCb0xTvO3ydw.jpeg" width="466" height="240" srcSet="https://miro.medium.com/max/552/1*K7rYONPTfcpCb0xTvO3ydw.jpeg 276w, https://miro.medium.com/max/932/1*K7rYONPTfcpCb0xTvO3ydw.jpeg 466w" sizes="466px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">A gradient super highway? Sounds good to me! <a href="http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg" class="co hg is it iu iv" target="_blank" rel="noopener">http://www.dyoung.com/assets/images/Articles%20images/article4_PPH.jpg</a></figcaption></figure></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="a58a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So far, we haven’t <em class="kq">really </em>explored LSTMs. We’ve more setup a foundation for them. And there’s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.</p><p id="7ba6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it “forgets”, a.k.a. resets, information it doesn’t find useful from the previous cell state, “writes” in new information it <em class="kq">does</em> find useful from the current input and/or previous hidden state, and similarly only “reads” out part of its information — the good stuff — in the computation of <strong class="iy kp">h_t</strong>. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a “memory cell”.</p><p id="6706" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The “writing to memory” part is additive — it’s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The “resetting memory” part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The “reading from memory” part is also multiplicative with a similar 0–1 range vector, but it doesn’t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.</p><p id="6813" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Both of these multiplications are <em class="kq">element wise</em>, like so:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rb"><div class="dv r dw dx"><div class="rc dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_YuIuYxt0oYEvGMoTz_J59g.png" width="167" height="82" role="presentation"></div><img class="dq dr s t u ds ai ed" width="167" height="82" srcset="" sizes="167px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/334/1*YuIuYxt0oYEvGMoTz_J59g.png" width="167" height="82" role="presentation"/></noscript></div></div></div></figure><p id="c869" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In this equation, when <strong class="iy kp">a = 0 </strong>the information of <strong class="iy kp">c </strong>is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.</p><p id="c920" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Our (unfinished) cell state computational graph now looks like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rd"><div class="dv r dw dx"><div class="re dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1__mbUA8vdaTbYreXpdPJccA.png" width="419" height="158" role="presentation"></div><img class="dq dr s t u ds ai ed" width="419" height="158" srcset="https://miro.medium.com/max/552/1*_mbUA8vdaTbYreXpdPJccA.png 276w, https://miro.medium.com/max/838/1*_mbUA8vdaTbYreXpdPJccA.png 419w" sizes="419px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/838/1*_mbUA8vdaTbYreXpdPJccA.png" width="419" height="158" srcSet="https://miro.medium.com/max/552/1*_mbUA8vdaTbYreXpdPJccA.png 276w, https://miro.medium.com/max/838/1*_mbUA8vdaTbYreXpdPJccA.png 419w" sizes="419px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">This is closer to what an LSTM looks like, though we’re not exactly there yet.</figcaption></figure><p id="ba90" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Sidenote: don’t be scared whenever you see the word “multiplicative” and don’t immediately think of “vanishing” or “exploding”. It depends on the context. Here, as I’ll show mathematically in a bit, it’s fine.</p><p id="9705" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This concept in general is known as <strong class="iy kp">gating</strong>, because we “gate” what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the “gates”. There are four such gates:</p><ul class=""><li id="dafe" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph=""><strong class="iy kp">f</strong>: <em class="kq">forget gate. </em>This is the “reset” tool that wipes out, diminishes, or retains information from the previous cell state. It’s the first interaction we make, and it’s multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we “remember” something, and when it is 0 we “forget”. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive… I guess it should really be called the “<em class="kq">remember gate</em>”!</li><li id="aeb2" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph=""><strong class="iy kp">g</strong>:<strong class="iy kp"><em class="kq"> </em></strong><em class="kq">?.</em><strong class="iy kp"><em class="kq"> </em></strong>This gate doesn’t really have a name, but it’s partly responsible for the “write” process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It’s computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)</li><li id="0612" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph=""><strong class="iy kp">i</strong>:<strong class="iy kp"><em class="kq"> </em></strong><em class="kq">input gate.</em> This is the other gate responsible for the “write” process. It controls how much of <strong class="iy kp"><em class="kq">g</em></strong> we “let in”, and is thus between 0 and 1, computed with sigmoid. It’s similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply <strong class="iy kp"><em class="kq">i</em> </strong>by <strong class="iy kp"><em class="kq">g </em></strong>and add this to the cell state.<strong class="iy kp"> </strong>Since <strong class="iy kp"><em class="kq">i </em></strong>is in the range 0 to 1, and <strong class="iy kp"><em class="kq">g</em></strong> is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.</li><li id="57eb" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph=""><strong class="iy kp">o</strong>:<em class="kq"> output gate</em>.<strong class="iy kp"><em class="kq"> </em></strong>This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the “read from memory” operation. It multiplies with the tanh<strong class="iy kp"><em class="kq"> </em></strong>of the cell state to compute the hidden state. So, I didn’t bring this up before, but the cell state leaks into a tanh before <strong class="iy kp">h_t</strong> is computed.</li></ul><p id="5cb0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here’s our updated computational graph for the cell state:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rf"><div class="dv r dw dx"><div class="rg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_3xq3p-nVgxQXXPSXueVWdw.png" width="533" height="251" role="presentation"></div><img class="dq dr s t u ds ai ed" width="533" height="251" srcset="https://miro.medium.com/max/552/1*3xq3p-nVgxQXXPSXueVWdw.png 276w, https://miro.medium.com/max/1066/1*3xq3p-nVgxQXXPSXueVWdw.png 533w" sizes="533px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1066/1*3xq3p-nVgxQXXPSXueVWdw.png" width="533" height="251" srcSet="https://miro.medium.com/max/552/1*3xq3p-nVgxQXXPSXueVWdw.png 276w, https://miro.medium.com/max/1066/1*3xq3p-nVgxQXXPSXueVWdw.png 533w" sizes="533px" role="presentation"/></noscript></div></div></div></figure><p id="01f6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Looks like I’m starting to create a complex diagram of my own. Damn. 😞 I guess LSTMs and immediately interpretable diagrams just weren’t meant to be!</p><p id="7eb5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Basically, <strong class="iy kp">f </strong>interacts with the cell state through a multiplication. <strong class="iy kp">i </strong>interacts with <strong class="iy kp">g </strong>through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that’s the shape of the tanh function in the circle), the result of which then interacts with <strong class="iy kp">o </strong>through multiplication to compute <strong class="iy kp">h_t</strong>. This does not disrupt the cell state, which flows to the next timestep. <strong class="iy kp">h_t </strong>then flows forward (and it could flow upward as well).</p><p id="e614" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here’s the equation form:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rh"><div class="dv r dw dx"><div class="ri dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_B9Qd1pW1kYM_zcg0IhPfUA.png" width="222" height="84" role="presentation"></div><img class="dq dr s t u ds ai ed" width="222" height="84" srcset="" sizes="222px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/444/1*B9Qd1pW1kYM_zcg0IhPfUA.png" width="222" height="84" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Each gate should actually be indexed by timestep <strong class="aq kt">t </strong>— we’ll see why soon.</figcaption></figure><p id="2518" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn’t explode — it stays stable by “forgetting” and “writing”, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.</p><p id="b30a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep’s <em class="kq">hidden state </em>flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the <strong class="iy kp">g </strong>and <strong class="iy kp">i </strong>gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.</p><p id="27fb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Since every gate has a different value at each timestep, we index by timestep <strong class="iy kp">t </strong>just like for hidden states, cell states, or something similar.</p><p id="6fb5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We could generalize for multiple hidden layers as well:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rj"><div class="dv r dw dx"><div class="rk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_BWz6E_IFi6UTLkSNSoq1Yg.png" width="290" height="41" role="presentation"></div><img class="dq dr s t u ds ai ed" width="290" height="41" srcset="https://miro.medium.com/max/552/1*BWz6E_IFi6UTLkSNSoq1Yg.png 276w, https://miro.medium.com/max/580/1*BWz6E_IFi6UTLkSNSoq1Yg.png 290w" sizes="290px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/580/1*BWz6E_IFi6UTLkSNSoq1Yg.png" width="290" height="41" srcSet="https://miro.medium.com/max/552/1*BWz6E_IFi6UTLkSNSoq1Yg.png 276w, https://miro.medium.com/max/580/1*BWz6E_IFi6UTLkSNSoq1Yg.png 290w" sizes="290px" role="presentation"/></noscript></div></div></div></figure><p id="1ae0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, for simplicity’s sake, let’s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the <strong class="iy kp">ℓ </strong>term and ignore influence from hidden states in the previous depth. We’ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can’t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.</p><p id="c042" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.</strong></p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rl"><div class="dv r dw dx"><div class="rm dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_hP6I692iv7oc6AkcWINDaw.png" width="384" height="180" role="presentation"></div><img class="dq dr s t u ds ai ed" width="384" height="180" srcset="https://miro.medium.com/max/552/1*hP6I692iv7oc6AkcWINDaw.png 276w, https://miro.medium.com/max/768/1*hP6I692iv7oc6AkcWINDaw.png 384w" sizes="384px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/768/1*hP6I692iv7oc6AkcWINDaw.png" width="384" height="180" srcSet="https://miro.medium.com/max/552/1*hP6I692iv7oc6AkcWINDaw.png 276w, https://miro.medium.com/max/768/1*hP6I692iv7oc6AkcWINDaw.png 384w" sizes="384px" role="presentation"/></noscript></div></div></div></figure><p id="033c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, <strong class="iy kp">W_xf </strong>are the weights that map input <strong class="iy kp">x </strong>to the forget gate <strong class="iy kp">f</strong>. Each gate has weight matrices that map input and hidden states to itself, including biases.</p><p id="6989" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can <em class="kq">learn </em>when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it’s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db no"><div class="dv r dw dx"><div class="rn dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_VJL6ONtLK77GpO2XmFCH7g.png" width="551" height="321" role="presentation"></div><img class="dq dr s t u ds ai ed" width="551" height="321" srcset="https://miro.medium.com/max/552/1*VJL6ONtLK77GpO2XmFCH7g.png 276w, https://miro.medium.com/max/1102/1*VJL6ONtLK77GpO2XmFCH7g.png 551w" sizes="551px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1102/1*VJL6ONtLK77GpO2XmFCH7g.png" width="551" height="321" srcSet="https://miro.medium.com/max/552/1*VJL6ONtLK77GpO2XmFCH7g.png 276w, https://miro.medium.com/max/1102/1*VJL6ONtLK77GpO2XmFCH7g.png 551w" sizes="551px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><strong class="aq kt">😨 </strong>😱 😰 : perhaps your immediate reaction.</figcaption></figure><p id="b263" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Okay, this looks scarier, but it’s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we’re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we’re in the first layer and at some timestep &gt; 1 where input exists. We then show how the <strong class="iy kp">f</strong>, <strong class="iy kp">i</strong>, <strong class="iy kp">g, </strong>and <strong class="iy kp">o</strong> gates are computed from this information — the hidden state and inputs are fed into an activation function like sigmoid (or, for <strong class="iy kp">g</strong>, a tanh; you can tell because it’s double the height of the others) — and it’s expressed through the web of arrows. It’s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it’s not necessarily explicit in the diagram.</p><p id="b56c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s embed this into our overall LSTM diagram for a single timestep:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db ro"><div class="dv r dw dx"><div class="rp dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_0h88NXeFxkb-xD1rBq4lgA.png" width="770" height="349" role="presentation"></div><img class="dq dr s t u ds ai ed" width="770" height="349" srcset="https://miro.medium.com/max/552/1*0h88NXeFxkb-xD1rBq4lgA.png 276w, https://miro.medium.com/max/1104/1*0h88NXeFxkb-xD1rBq4lgA.png 552w, https://miro.medium.com/max/1280/1*0h88NXeFxkb-xD1rBq4lgA.png 640w, https://miro.medium.com/max/1400/1*0h88NXeFxkb-xD1rBq4lgA.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1540/1*0h88NXeFxkb-xD1rBq4lgA.png" width="770" height="349" srcSet="https://miro.medium.com/max/552/1*0h88NXeFxkb-xD1rBq4lgA.png 276w, https://miro.medium.com/max/1104/1*0h88NXeFxkb-xD1rBq4lgA.png 552w, https://miro.medium.com/max/1280/1*0h88NXeFxkb-xD1rBq4lgA.png 640w, https://miro.medium.com/max/1400/1*0h88NXeFxkb-xD1rBq4lgA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="147c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now let’s zoom out and view our entire unrolled single layer, three timestep LSTM:</p></div></div><div class="do ai"><figure class="jv jw jx jy jz do ai paragraph-image"><div class="kl km dw kn ai"><div class="dv r dw dx"><div class="rq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_-lhIk-yAsXk88gcPvEeIRQ.png" width="4225" height="1204" role="presentation"></div><img class="dq dr s t u ds ai ed" width="4225" height="1204" srcset="https://miro.medium.com/max/552/1*-lhIk-yAsXk88gcPvEeIRQ.png 276w, https://miro.medium.com/max/1104/1*-lhIk-yAsXk88gcPvEeIRQ.png 552w, https://miro.medium.com/max/1280/1*-lhIk-yAsXk88gcPvEeIRQ.png 640w, https://miro.medium.com/max/1456/1*-lhIk-yAsXk88gcPvEeIRQ.png 728w, https://miro.medium.com/max/1632/1*-lhIk-yAsXk88gcPvEeIRQ.png 816w, https://miro.medium.com/max/1808/1*-lhIk-yAsXk88gcPvEeIRQ.png 904w, https://miro.medium.com/max/1984/1*-lhIk-yAsXk88gcPvEeIRQ.png 992w, https://miro.medium.com/max/2160/1*-lhIk-yAsXk88gcPvEeIRQ.png 1080w, https://miro.medium.com/max/2700/1*-lhIk-yAsXk88gcPvEeIRQ.png 1350w, https://miro.medium.com/max/3240/1*-lhIk-yAsXk88gcPvEeIRQ.png 1620w, https://miro.medium.com/max/3780/1*-lhIk-yAsXk88gcPvEeIRQ.png 1890w, https://miro.medium.com/max/4320/1*-lhIk-yAsXk88gcPvEeIRQ.png 2160w, https://miro.medium.com/max/4800/1*-lhIk-yAsXk88gcPvEeIRQ.png 2400w" sizes="100vw" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/8450/1*-lhIk-yAsXk88gcPvEeIRQ.png" width="4225" height="1204" srcSet="https://miro.medium.com/max/552/1*-lhIk-yAsXk88gcPvEeIRQ.png 276w, https://miro.medium.com/max/1104/1*-lhIk-yAsXk88gcPvEeIRQ.png 552w, https://miro.medium.com/max/1280/1*-lhIk-yAsXk88gcPvEeIRQ.png 640w, https://miro.medium.com/max/1456/1*-lhIk-yAsXk88gcPvEeIRQ.png 728w, https://miro.medium.com/max/1632/1*-lhIk-yAsXk88gcPvEeIRQ.png 816w, https://miro.medium.com/max/1808/1*-lhIk-yAsXk88gcPvEeIRQ.png 904w, https://miro.medium.com/max/1984/1*-lhIk-yAsXk88gcPvEeIRQ.png 992w, https://miro.medium.com/max/2160/1*-lhIk-yAsXk88gcPvEeIRQ.png 1080w, https://miro.medium.com/max/2700/1*-lhIk-yAsXk88gcPvEeIRQ.png 1350w, https://miro.medium.com/max/3240/1*-lhIk-yAsXk88gcPvEeIRQ.png 1620w, https://miro.medium.com/max/3780/1*-lhIk-yAsXk88gcPvEeIRQ.png 1890w, https://miro.medium.com/max/4320/1*-lhIk-yAsXk88gcPvEeIRQ.png 2160w, https://miro.medium.com/max/4800/1*-lhIk-yAsXk88gcPvEeIRQ.png 2400w" sizes="100vw" role="presentation"/></noscript></div></div></div></figure></div><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="c9d0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It’s beautiful, isn’t it? The full screen width size just adds to the effect! <a href="https://drive.google.com/file/d/0BwbWRPtraa2zQUsydXRKSkd3YUU/view?usp=sharing" class="co hg is it iu iv" target="_blank" rel="noopener">Here’s</a> a link to the full res version.</p><p id="c678" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! 😍</p><h1 id="e117" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Fixing the problem with LSTMs (Part II)</h1><p id="f89a" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">You’ve come a long way, young padawan. But there’s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part—analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won’t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you’ll find in other current tutorials.</p><p id="a182" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Firstly, truncated BPTT is often used with LSTMs; it’s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it’s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rr"><div class="dv r dw dx"><div class="rs dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_UqC4IRIfcDfoiwD8zvqW2A.png" width="87" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="87" height="79" srcset="" sizes="87px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/174/1*UqC4IRIfcDfoiwD8zvqW2A.png" width="87" height="79" role="presentation"/></noscript></div></div></div></figure><p id="06fe" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">…which would include a <em class="kq">lot</em> of terms.</p><p id="792f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When we backprop the error, and add all the gradients up, this is what we get:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rt"><div class="dv r dw dx"><div class="ru dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_ucOvP6wOs9MHzH8WKiykbQ.png" width="675" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="675" height="79" srcset="https://miro.medium.com/max/552/1*ucOvP6wOs9MHzH8WKiykbQ.png 276w, https://miro.medium.com/max/1104/1*ucOvP6wOs9MHzH8WKiykbQ.png 552w, https://miro.medium.com/max/1280/1*ucOvP6wOs9MHzH8WKiykbQ.png 640w, https://miro.medium.com/max/1350/1*ucOvP6wOs9MHzH8WKiykbQ.png 675w" sizes="675px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1350/1*ucOvP6wOs9MHzH8WKiykbQ.png" width="675" height="79" srcSet="https://miro.medium.com/max/552/1*ucOvP6wOs9MHzH8WKiykbQ.png 276w, https://miro.medium.com/max/1104/1*ucOvP6wOs9MHzH8WKiykbQ.png 552w, https://miro.medium.com/max/1280/1*ucOvP6wOs9MHzH8WKiykbQ.png 640w, https://miro.medium.com/max/1350/1*ucOvP6wOs9MHzH8WKiykbQ.png 675w" sizes="675px" role="presentation"/></noscript></div></div></div></figure><p id="9c5f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Truncated BPTT does two things:</p><ul class=""><li id="5bfb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass “every so often”. That is — we compute <strong class="iy kp">h_1 </strong>and <strong class="iy kp">c_1</strong>, then <strong class="iy kp">h_2 </strong>and <strong class="iy kp">c_2</strong>, then <strong class="iy kp">h_3</strong> and <strong class="iy kp">c_3</strong>, and then at some point in time, quantified by <strong class="iy kp">k1, </strong>we do a backwards pass. Every <strong class="iy kp">k1 </strong>timesteps, we perform BPTT; if <strong class="iy kp">k1 = 10</strong>, for example, then once we compute <strong class="iy kp">h_10 </strong>and <strong class="iy kp">c_10 </strong>we perform BPTT. Same for <strong class="iy kp">h_20 </strong>and <strong class="iy kp">c_20</strong>, and so on so forth. When we perform the backwards pass, our error <strong class="iy kp">J </strong>won’t be the same as if we did a full forwards pass and full backwards pass, since we haven’t observed all the outputs yet—we wouldn’t have even computed all the potential outputs yet! Instead, the error describes what we’ve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it’s like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep <strong class="iy kp">t</strong> —where <strong class="iy kp">t </strong>is a multiple of <strong class="iy kp">k1</strong> — with truncated backprop as<strong class="iy kp"> J^t</strong>. So:</li></ul><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rv"><div class="dv r dw dx"><div class="rw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_Br6EoWvUmGTNoX3NqkZVpA.png" width="445" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="445" height="80" srcset="https://miro.medium.com/max/552/1*Br6EoWvUmGTNoX3NqkZVpA.png 276w, https://miro.medium.com/max/890/1*Br6EoWvUmGTNoX3NqkZVpA.png 445w" sizes="445px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/890/1*Br6EoWvUmGTNoX3NqkZVpA.png" width="445" height="80" srcSet="https://miro.medium.com/max/552/1*Br6EoWvUmGTNoX3NqkZVpA.png 276w, https://miro.medium.com/max/890/1*Br6EoWvUmGTNoX3NqkZVpA.png 445w" sizes="445px" role="presentation"/></noscript></div></div></div></figure><p id="eb5a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">For example, if <strong class="iy kp">t = 20</strong> and <strong class="iy kp">k1 = 10</strong>, our <em class="kq">second</em> (because 20 ÷ 10 = 2) round of BPTT would be:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rx"><div class="dv r dw dx"><div class="ry dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_pg91-TmNosH9B7Py0wjCdQ.png" width="667" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="667" height="80" srcset="https://miro.medium.com/max/552/1*pg91-TmNosH9B7Py0wjCdQ.png 276w, https://miro.medium.com/max/1104/1*pg91-TmNosH9B7Py0wjCdQ.png 552w, https://miro.medium.com/max/1280/1*pg91-TmNosH9B7Py0wjCdQ.png 640w, https://miro.medium.com/max/1334/1*pg91-TmNosH9B7Py0wjCdQ.png 667w" sizes="667px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1334/1*pg91-TmNosH9B7Py0wjCdQ.png" width="667" height="80" srcSet="https://miro.medium.com/max/552/1*pg91-TmNosH9B7Py0wjCdQ.png 276w, https://miro.medium.com/max/1104/1*pg91-TmNosH9B7Py0wjCdQ.png 552w, https://miro.medium.com/max/1280/1*pg91-TmNosH9B7Py0wjCdQ.png 640w, https://miro.medium.com/max/1334/1*pg91-TmNosH9B7Py0wjCdQ.png 667w" sizes="667px" role="presentation"/></noscript></div></div></div></figure><ul class=""><li id="f441" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">On top of this, instead of backpropagating from <strong class="iy kp">J^t </strong>all the way to the first timestep <strong class="iy kp">c_1</strong>, we set a cut-off point. This cut-off point, quantified by <strong class="iy kp">k2</strong>, is the timestep at which our cell states stop contributing to the overall gradient. For example, if <strong class="iy kp">k2 = 10</strong>, and we’re backpropagating at <strong class="iy kp">t = 20</strong>, then <strong class="iy kp">c_10 </strong>is the final cell state to contribute to the overall gradient. Everything before <strong class="iy kp">c_10 </strong>will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:</li></ul><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db rz"><div class="dv r dw dx"><div class="sa dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_GmP4nvsdBTyyRwo7ffRW2g.png" width="515" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="515" height="80" srcset="https://miro.medium.com/max/552/1*GmP4nvsdBTyyRwo7ffRW2g.png 276w, https://miro.medium.com/max/1030/1*GmP4nvsdBTyyRwo7ffRW2g.png 515w" sizes="515px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1030/1*GmP4nvsdBTyyRwo7ffRW2g.png" width="515" height="80" srcSet="https://miro.medium.com/max/552/1*GmP4nvsdBTyyRwo7ffRW2g.png 276w, https://miro.medium.com/max/1030/1*GmP4nvsdBTyyRwo7ffRW2g.png 515w" sizes="515px" role="presentation"/></noscript></div></div></div></figure><p id="956c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, with <strong class="iy kp">t = 20</strong>, <strong class="iy kp">k2 = 10</strong>, and <strong class="iy kp">k1 = 10</strong>, our second round of BPTT would follow:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sb"><div class="dv r dw dx"><div class="sc dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_WyHlRZljjmHEaFKsGg0JQg.png" width="674" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="674" height="80" srcset="https://miro.medium.com/max/552/1*WyHlRZljjmHEaFKsGg0JQg.png 276w, https://miro.medium.com/max/1104/1*WyHlRZljjmHEaFKsGg0JQg.png 552w, https://miro.medium.com/max/1280/1*WyHlRZljjmHEaFKsGg0JQg.png 640w, https://miro.medium.com/max/1348/1*WyHlRZljjmHEaFKsGg0JQg.png 674w" sizes="674px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1348/1*WyHlRZljjmHEaFKsGg0JQg.png" width="674" height="80" srcSet="https://miro.medium.com/max/552/1*WyHlRZljjmHEaFKsGg0JQg.png 276w, https://miro.medium.com/max/1104/1*WyHlRZljjmHEaFKsGg0JQg.png 552w, https://miro.medium.com/max/1280/1*WyHlRZljjmHEaFKsGg0JQg.png 640w, https://miro.medium.com/max/1348/1*WyHlRZljjmHEaFKsGg0JQg.png 674w" sizes="674px" role="presentation"/></noscript></div></div></div></figure><p id="a297" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Both <strong class="iy kp">k1</strong> and <strong class="iy kp">k2</strong> are hyperparameters. <strong class="iy kp">k1 </strong>does not have to equal <strong class="iy kp">k2</strong>.</p><p id="93ed" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here’s a formal definition:</p><blockquote class="ls lt lu"><p id="72ee" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">[Truncated BPTT] processes the sequence one timestep at a time, and every <strong class="iy kp">k1</strong> timesteps, it runs BPTT for <strong class="iy kp">k2</strong> timesteps, so a parameter update can be cheap if <strong class="iy kp">k2</strong> is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.</p><p id="ae91" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">— <a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">“Training Recurrent Neural Networks”</a>, 2.8.6, Page 23</p></blockquote><p id="28a4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The same paper gives nice pseudocode for truncated BPTT:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db sd"><div class="dv r dw dx"><div class="se dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_0SnUb2iYt1RNa7JsGG-7gQ.png" width="413" height="113" role="presentation"></div><img class="dq dr s t u ds ai ed" width="413" height="113" srcset="https://miro.medium.com/max/552/1*0SnUb2iYt1RNa7JsGG-7gQ.png 276w, https://miro.medium.com/max/826/1*0SnUb2iYt1RNa7JsGG-7gQ.png 413w" sizes="413px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/826/1*0SnUb2iYt1RNa7JsGG-7gQ.png" width="413" height="113" srcSet="https://miro.medium.com/max/552/1*0SnUb2iYt1RNa7JsGG-7gQ.png 276w, https://miro.medium.com/max/826/1*0SnUb2iYt1RNa7JsGG-7gQ.png 413w" sizes="413px" role="presentation"/></noscript></div></div></div></div></figure><p id="399f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The rest of the math in this section will not be in the context of using truncated backprop, because it’s a technique vs. something rooted in the mathematical foundation of LSTMs.</p><p id="d546" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Moving on — before, we saw this diagram:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db qr"><div class="dv r dw dx"><div class="qs dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_n26drGfEkc-Xnmqc2Lw7cw.png" width="2161" height="690" role="presentation"></div><img class="dq dr s t u ds ai ed" width="2161" height="690" srcset="https://miro.medium.com/max/552/1*n26drGfEkc-Xnmqc2Lw7cw.png 276w, https://miro.medium.com/max/1104/1*n26drGfEkc-Xnmqc2Lw7cw.png 552w, https://miro.medium.com/max/1280/1*n26drGfEkc-Xnmqc2Lw7cw.png 640w, https://miro.medium.com/max/1400/1*n26drGfEkc-Xnmqc2Lw7cw.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/4322/1*n26drGfEkc-Xnmqc2Lw7cw.png" width="2161" height="690" srcSet="https://miro.medium.com/max/552/1*n26drGfEkc-Xnmqc2Lw7cw.png 276w, https://miro.medium.com/max/1104/1*n26drGfEkc-Xnmqc2Lw7cw.png 552w, https://miro.medium.com/max/1280/1*n26drGfEkc-Xnmqc2Lw7cw.png 640w, https://miro.medium.com/max/1400/1*n26drGfEkc-Xnmqc2Lw7cw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="48f4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In this context, <strong class="iy kp">ƒw = i ⊙ g</strong>, because it’s the value we’re adding to the cell state.</p><p id="db0e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let’s bring up our cell state equation to see:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sf"><div class="dv r dw dx"><div class="sg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_i6rbrX0k9mKLXewD4korCw.png" width="236" height="39" role="presentation"></div><img class="dq dr s t u ds ai ed" width="236" height="39" srcset="" sizes="236px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/472/1*i6rbrX0k9mKLXewD4korCw.png" width="236" height="39" role="presentation"/></noscript></div></div></div></figure><p id="09ae" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db sh"><div class="dv r dw dx"><div class="si dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_UVx1vL6ADQGTBeKSaWX7bw.png" width="2436" height="986" role="presentation"></div><img class="dq dr s t u ds ai ed" width="2436" height="986" srcset="https://miro.medium.com/max/552/1*UVx1vL6ADQGTBeKSaWX7bw.png 276w, https://miro.medium.com/max/1104/1*UVx1vL6ADQGTBeKSaWX7bw.png 552w, https://miro.medium.com/max/1280/1*UVx1vL6ADQGTBeKSaWX7bw.png 640w, https://miro.medium.com/max/1400/1*UVx1vL6ADQGTBeKSaWX7bw.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/4872/1*UVx1vL6ADQGTBeKSaWX7bw.png" width="2436" height="986" srcSet="https://miro.medium.com/max/552/1*UVx1vL6ADQGTBeKSaWX7bw.png 276w, https://miro.medium.com/max/1104/1*UVx1vL6ADQGTBeKSaWX7bw.png 552w, https://miro.medium.com/max/1280/1*UVx1vL6ADQGTBeKSaWX7bw.png 640w, https://miro.medium.com/max/1400/1*UVx1vL6ADQGTBeKSaWX7bw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Do not confuse forget gate <strong class="aq kt">ƒ</strong> with function <strong class="aq kt">ƒw</strong> in this diagram. I know, it’s confusing… 😢</figcaption></figure><p id="bf2c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When our gradients flow back, they will be affected by this multiplicative interaction. So, let’s compute the new derivative:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sj"><div class="dv r dw dx"><div class="sk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_60XFfJvc0t9a0ekdMTAp_Q.png" width="113" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="113" height="79" srcset="" sizes="113px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/226/1*60XFfJvc0t9a0ekdMTAp_Q.png" width="113" height="79" role="presentation"/></noscript></div></div></div></figure><p id="f174" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This seems super neat, actually. <em class="kq">Obviously</em> the gradient will be <strong class="iy kp">f</strong>, because <strong class="iy kp">f </strong>acts as a blocker and controls how much <strong class="iy kp">c_t-1 </strong>influences <strong class="iy kp">c_t</strong>; it’s the gate that you can fully or partially open and close that lets information from <strong class="iy kp">c_t-1 </strong>flow through! It’s just intuitive that it would propagate back perfectly.</p><p id="d859" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, if you’ve payed close attention so far, you might be asking: “<em class="kq">wait, what happened to </em><strong class="iy kp">ƒw</strong><em class="kq">’s contribution to the gradient?”</em><strong class="iy kp"><em class="kq"> </em></strong>If you’re a hardcore mathematician, you might also be worried that we’re content with leaving the gradient as just <strong class="iy kp">f</strong>. This is because the gates <strong class="iy kp">f</strong>,<strong class="iy kp"> i</strong>, and <strong class="iy kp">g </strong>are all functions of <strong class="iy kp">c_t-1</strong>; they are functions of <strong class="iy kp">h_t-1</strong>, which is, in turn, a function of <strong class="iy kp">c_t-1</strong>! The diagram shows this visually, as well. It seems we’re failing to apply calculus properly. We’d need to backprop through <strong class="iy kp">f</strong> and through <strong class="iy kp">i ⊙ g </strong>to complete the derivative.</p><p id="3f0c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s walk through the differentiation to show why you’re actually not wrong<strong class="iy kp">,<em class="kq"> </em></strong>but neither am I:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sl"><div class="dv r dw dx"><div class="sm dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_x1mvDnbZOmZ1CnHJo23tZg.png" width="380" height="162" role="presentation"></div><img class="dq dr s t u ds ai ed" width="380" height="162" srcset="https://miro.medium.com/max/552/1*x1mvDnbZOmZ1CnHJo23tZg.png 276w, https://miro.medium.com/max/760/1*x1mvDnbZOmZ1CnHJo23tZg.png 380w" sizes="380px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/760/1*x1mvDnbZOmZ1CnHJo23tZg.png" width="380" height="162" srcSet="https://miro.medium.com/max/552/1*x1mvDnbZOmZ1CnHJo23tZg.png 276w, https://miro.medium.com/max/760/1*x1mvDnbZOmZ1CnHJo23tZg.png 380w" sizes="380px" role="presentation"/></noscript></div></div></div></figure><p id="80e0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, with the first derivative, we need to apply product rule. Why? Because we’re differentiating the product of two functions of <strong class="iy kp">c_t-1</strong>. The former being the forget gate, and the latter being just <strong class="iy kp">c_t-1</strong>. Let’s do it:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sn"><div class="dv r dw dx"><div class="so dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_TwjnkG6vtuIke1pkAzTzzA.png" width="271" height="124" role="presentation"></div><img class="dq dr s t u ds ai ed" width="271" height="124" srcset="" sizes="271px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/542/1*TwjnkG6vtuIke1pkAzTzzA.png" width="271" height="124" role="presentation"/></noscript></div></div></div></figure><p id="00fd" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Then, from product rule:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sp"><div class="dv r dw dx"><div class="sq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_cgq4UnWxun6rQ6H00gDzWg.png" width="381" height="164" role="presentation"></div><img class="dq dr s t u ds ai ed" width="381" height="164" srcset="https://miro.medium.com/max/552/1*cgq4UnWxun6rQ6H00gDzWg.png 276w, https://miro.medium.com/max/762/1*cgq4UnWxun6rQ6H00gDzWg.png 381w" sizes="381px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/762/1*cgq4UnWxun6rQ6H00gDzWg.png" width="381" height="164" srcSet="https://miro.medium.com/max/552/1*cgq4UnWxun6rQ6H00gDzWg.png 276w, https://miro.medium.com/max/762/1*cgq4UnWxun6rQ6H00gDzWg.png 381w" sizes="381px" role="presentation"/></noscript></div></div></div></figure><p id="7dd1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You’ll see why in a bit.</p><p id="e8f2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now let’s tackle the second one:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sr"><div class="dv r dw dx"><div class="ss dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_1T0iaNg6vY4pEOz-ybkjuw.png" width="150" height="77" role="presentation"></div><img class="dq dr s t u ds ai ed" width="150" height="77" srcset="" sizes="150px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/300/1*1T0iaNg6vY4pEOz-ybkjuw.png" width="150" height="77" role="presentation"/></noscript></div></div></div></figure><p id="10b0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You’ll notice that it’s also two functions of <strong class="iy kp">c_t-1 </strong>multiplied together, so we use the product rule again:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db st"><div class="dv r dw dx"><div class="su dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_tVfrKCc1T7eRgypliDe19A.png" width="320" height="124" role="presentation"></div><img class="dq dr s t u ds ai ed" width="320" height="124" srcset="https://miro.medium.com/max/552/1*tVfrKCc1T7eRgypliDe19A.png 276w, https://miro.medium.com/max/640/1*tVfrKCc1T7eRgypliDe19A.png 320w" sizes="320px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/640/1*tVfrKCc1T7eRgypliDe19A.png" width="320" height="124" srcSet="https://miro.medium.com/max/552/1*tVfrKCc1T7eRgypliDe19A.png 276w, https://miro.medium.com/max/640/1*tVfrKCc1T7eRgypliDe19A.png 320w" sizes="320px" role="presentation"/></noscript></div></div></div></figure><p id="618f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sv"><div class="dv r dw dx"><div class="sw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_s2nnva6Yhb2AuZDYYALbEA.png" width="361" height="164" role="presentation"></div><img class="dq dr s t u ds ai ed" width="361" height="164" srcset="https://miro.medium.com/max/552/1*s2nnva6Yhb2AuZDYYALbEA.png 276w, https://miro.medium.com/max/722/1*s2nnva6Yhb2AuZDYYALbEA.png 361w" sizes="361px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/722/1*s2nnva6Yhb2AuZDYYALbEA.png" width="361" height="164" srcSet="https://miro.medium.com/max/552/1*s2nnva6Yhb2AuZDYYALbEA.png 276w, https://miro.medium.com/max/722/1*s2nnva6Yhb2AuZDYYALbEA.png 361w" sizes="361px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Once again, we purposely do not simplify the gate derivative terms.</figcaption></figure><p id="41e0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Thus, our overall derivative becomes:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sx"><div class="dv r dw dx"><div class="sy dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_o0dzU_s9WxoTYfOkQo1a0A.png" width="461" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="461" height="79" srcset="https://miro.medium.com/max/552/1*o0dzU_s9WxoTYfOkQo1a0A.png 276w, https://miro.medium.com/max/922/1*o0dzU_s9WxoTYfOkQo1a0A.png 461w" sizes="461px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/922/1*o0dzU_s9WxoTYfOkQo1a0A.png" width="461" height="79" srcSet="https://miro.medium.com/max/552/1*o0dzU_s9WxoTYfOkQo1a0A.png 276w, https://miro.medium.com/max/922/1*o0dzU_s9WxoTYfOkQo1a0A.png 461w" sizes="461px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">Notice that the first term in this derivative is our forget gate.</figcaption></figure><p id="9307" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Pay attention to the caption of the diagram.</p><p id="4712" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is actually our <em class="kq">real </em>derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they’ll probably come up with this. However, <em class="kq">effectively </em>(or, rather, approximately), our gradient is just the forget gate, because the other three terms tend towards zero. Yup — they vanish. Why?</p><p id="b5a4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time <strong class="iy kp">t</strong> down <strong class="iy kp">k </strong>timesteps, then we need to compute the derivative of the cell state at time <strong class="iy kp">t </strong>to the cell state at time <strong class="iy kp">t-k</strong>. Look what happens when we do that:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db sz"><div class="dv r dw dx"><div class="ta dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_dBFbl6NCqp94Lnyb0taFWg.png" width="642" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="642" height="79" srcset="https://miro.medium.com/max/552/1*dBFbl6NCqp94Lnyb0taFWg.png 276w, https://miro.medium.com/max/1104/1*dBFbl6NCqp94Lnyb0taFWg.png 552w, https://miro.medium.com/max/1280/1*dBFbl6NCqp94Lnyb0taFWg.png 640w, https://miro.medium.com/max/1284/1*dBFbl6NCqp94Lnyb0taFWg.png 642w" sizes="642px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1284/1*dBFbl6NCqp94Lnyb0taFWg.png" width="642" height="79" srcSet="https://miro.medium.com/max/552/1*dBFbl6NCqp94Lnyb0taFWg.png 276w, https://miro.medium.com/max/1104/1*dBFbl6NCqp94Lnyb0taFWg.png 552w, https://miro.medium.com/max/1280/1*dBFbl6NCqp94Lnyb0taFWg.png 640w, https://miro.medium.com/max/1284/1*dBFbl6NCqp94Lnyb0taFWg.png 642w" sizes="642px" role="presentation"/></noscript></div></div></div></figure><p id="57e3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We didn’t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db tb"><div class="dv r dw dx"><div class="tc dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_Q9BJ7JxQ08YvfBW_OsJabw.png" width="458" height="71" role="presentation"></div><img class="dq dr s t u ds ai ed" width="458" height="71" srcset="https://miro.medium.com/max/552/1*Q9BJ7JxQ08YvfBW_OsJabw.png 276w, https://miro.medium.com/max/916/1*Q9BJ7JxQ08YvfBW_OsJabw.png 458w" sizes="458px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/916/1*Q9BJ7JxQ08YvfBW_OsJabw.png" width="458" height="71" srcSet="https://miro.medium.com/max/552/1*Q9BJ7JxQ08YvfBW_OsJabw.png 276w, https://miro.medium.com/max/916/1*Q9BJ7JxQ08YvfBW_OsJabw.png 458w" sizes="458px" role="presentation"/></noscript></div></div></div></figure><p id="e607" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The rationale behind this is pretty simple, and we don’t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid/tanh in them. But, just because we don’t <em class="kq">need</em> to use math to show this, doesn’t mean we don’t <em class="kq">want </em>to 😏 :</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db td"><div class="dv r dw dx"><div class="te dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_-mUDovQ8ovejmWPNoFSI1g.png" width="541" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="541" height="79" srcset="https://miro.medium.com/max/552/1*-mUDovQ8ovejmWPNoFSI1g.png 276w, https://miro.medium.com/max/1082/1*-mUDovQ8ovejmWPNoFSI1g.png 541w" sizes="541px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1082/1*-mUDovQ8ovejmWPNoFSI1g.png" width="541" height="79" srcSet="https://miro.medium.com/max/552/1*-mUDovQ8ovejmWPNoFSI1g.png 276w, https://miro.medium.com/max/1082/1*-mUDovQ8ovejmWPNoFSI1g.png 541w" sizes="541px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">I obfuscated the input to the sigmoid function for the input gate, just for simplicity.</figcaption></figure><p id="f6be" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Recall from our vanishing gradient article that the max output of sigmoid’s first order derivative is 0.25, and it’s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don’t vanish, they’ll be super minor contributions, so we can just leave them out for brevity.</p><blockquote class="ls lt lu"><p id="95ec" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Sidenote: one person reached out to me unsure of why gradients with long terms — aka, that are equal to the product of a lot of terms — usually vanishes/explodes. Here’s what I said in response:</p><p id="eabd" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">“If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they’re not, it’ll explode or vanish. And, given the nature of the problems where this is an issue, it’s very unlikely they’ll be around 1 each. Especially if they are the output some non-linear function like sigmoid/tanh or their derivatives.</p><p id="944e" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">For example, let’s say the gradient term = k_1 × k_2 × k_3 × … × k_100. 100 terms in this product.</p><p id="1583" class="iw ji em kq iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If each of these terms is, let’s say, around 0.5, then you have 0.5¹⁰⁰ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¹⁰⁰ which is some absurdly high number.<br><br>When we introduce tanh/sigmoid and/or their derivatives in these huge products, you can guarantee that they’ll saturate and die off. As mentioned, the max for sigmoid’s first order derivative is 0.25, so just imagine something like 0.25¹⁰⁰.</p></blockquote><p id="59c9" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.</p><p id="e361" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Because <strong class="iy kp">ƒw = i ⊙ g</strong>, we can redraw our diagram showing that <strong class="iy kp">ƒw </strong>won’t make any contributions to the gradient flow back. Again — <strong class="iy kp">ƒw</strong> does, but it’s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db tf"><div class="dv r dw dx"><div class="tg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_RsHULZCgY6p-5bKkE99Q-Q.png" width="1095" height="453" role="presentation"></div><img class="dq dr s t u ds ai ed" width="1095" height="453" srcset="https://miro.medium.com/max/552/1*RsHULZCgY6p-5bKkE99Q-Q.png 276w, https://miro.medium.com/max/1104/1*RsHULZCgY6p-5bKkE99Q-Q.png 552w, https://miro.medium.com/max/1280/1*RsHULZCgY6p-5bKkE99Q-Q.png 640w, https://miro.medium.com/max/1400/1*RsHULZCgY6p-5bKkE99Q-Q.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/2190/1*RsHULZCgY6p-5bKkE99Q-Q.png" width="1095" height="453" srcSet="https://miro.medium.com/max/552/1*RsHULZCgY6p-5bKkE99Q-Q.png 276w, https://miro.medium.com/max/1104/1*RsHULZCgY6p-5bKkE99Q-Q.png 552w, https://miro.medium.com/max/1280/1*RsHULZCgY6p-5bKkE99Q-Q.png 640w, https://miro.medium.com/max/1400/1*RsHULZCgY6p-5bKkE99Q-Q.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="8811" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But wait! This doesn’t look good; the gradients have to multiply by this <strong class="iy kp">f_t</strong> gate at each timestep. Before, they didn’t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.</p><p id="9feb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is <strong class="iy kp">1.0</strong>: “Constant Error Carousel” (CEC). With our new function, the derivative is equal to <strong class="iy kp">f</strong>. You’ll see this referred to as a “linear carousel” in papers.</p><p id="fdbb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Before we introduced a forget gate — where all we had was the additive interaction from <strong class="iy kp">ƒw </strong>— our cell state function was a CEC:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qn"><div class="dv r dw dx"><div class="th dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_9O__qOVOK1wxJDFy6m4YAQ.png" width="208" height="84" role="presentation"></div><img class="dq dr s t u ds ai ed" width="208" height="84" srcset="" sizes="208px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/416/1*9O__qOVOK1wxJDFy6m4YAQ.png" width="208" height="84" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">A CEC — same as before, but no forget gate.</figcaption></figure><p id="61b2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The derivative of this cell state w.r.t. the previous one, again as long as we don’t backprop through the <strong class="iy kp">i </strong>and <strong class="iy kp">g</strong> gates, is just 1. That’s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of <strong class="iy kp">c_t-1 </strong>needs to be 1.</p><p id="7ab2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of <strong class="iy kp">c_t-1 </strong>is <strong class="iy kp">f</strong>. So, in our case, when <strong class="iy kp">f = 1</strong> (when we’re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it’s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it’s constant with a value of 1 and then drops off to zero/dies when we have <strong class="iy kp">f ≈ 0</strong>.</p><p id="3337" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Intuitively, this seems problematic. Let’s do some math to investigate:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ti"><div class="dv r dw dx"><div class="tj dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_UbqEhAyW7bMv_tDf-cvuWg.png" width="372" height="164" role="presentation"></div><img class="dq dr s t u ds ai ed" width="372" height="164" srcset="https://miro.medium.com/max/552/1*UbqEhAyW7bMv_tDf-cvuWg.png 276w, https://miro.medium.com/max/744/1*UbqEhAyW7bMv_tDf-cvuWg.png 372w" sizes="372px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/744/1*UbqEhAyW7bMv_tDf-cvuWg.png" width="372" height="164" srcSet="https://miro.medium.com/max/552/1*UbqEhAyW7bMv_tDf-cvuWg.png 276w, https://miro.medium.com/max/744/1*UbqEhAyW7bMv_tDf-cvuWg.png 372w" sizes="372px" role="presentation"/></noscript></div></div></div></figure><p id="2abc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The derivative of a cell state to the previous is <strong class="iy kp">f_t</strong>. The derivative of a cell state to two prior cell states is <strong class="iy kp">f_t ⊙ f_t-1</strong>. Thus:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db tk"><div class="dv r dw dx"><div class="tl dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_p9OndETS7tR-zUU-1TuaTw.png" width="603" height="83" role="presentation"></div><img class="dq dr s t u ds ai ed" width="603" height="83" srcset="https://miro.medium.com/max/552/1*p9OndETS7tR-zUU-1TuaTw.png 276w, https://miro.medium.com/max/1104/1*p9OndETS7tR-zUU-1TuaTw.png 552w, https://miro.medium.com/max/1206/1*p9OndETS7tR-zUU-1TuaTw.png 603w" sizes="603px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1206/1*p9OndETS7tR-zUU-1TuaTw.png" width="603" height="83" srcSet="https://miro.medium.com/max/552/1*p9OndETS7tR-zUU-1TuaTw.png 276w, https://miro.medium.com/max/1104/1*p9OndETS7tR-zUU-1TuaTw.png 552w, https://miro.medium.com/max/1206/1*p9OndETS7tR-zUU-1TuaTw.png 603w" sizes="603px" role="presentation"/></noscript></div></div></div></div></figure><p id="8238" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.</p><p id="6f12" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like <strong class="iy kp">W_xi</strong>, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db tm"><div class="dv r dw dx"><div class="tn dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_rhb_2DO5MulJvzV9QxasMg.png" width="692" height="101" role="presentation"></div><img class="dq dr s t u ds ai ed" width="692" height="101" srcset="https://miro.medium.com/max/552/1*rhb_2DO5MulJvzV9QxasMg.png 276w, https://miro.medium.com/max/1104/1*rhb_2DO5MulJvzV9QxasMg.png 552w, https://miro.medium.com/max/1280/1*rhb_2DO5MulJvzV9QxasMg.png 640w, https://miro.medium.com/max/1384/1*rhb_2DO5MulJvzV9QxasMg.png 692w" sizes="692px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1384/1*rhb_2DO5MulJvzV9QxasMg.png" width="692" height="101" srcSet="https://miro.medium.com/max/552/1*rhb_2DO5MulJvzV9QxasMg.png 276w, https://miro.medium.com/max/1104/1*rhb_2DO5MulJvzV9QxasMg.png 552w, https://miro.medium.com/max/1280/1*rhb_2DO5MulJvzV9QxasMg.png 640w, https://miro.medium.com/max/1384/1*rhb_2DO5MulJvzV9QxasMg.png 692w" sizes="692px" role="presentation"/></noscript></div></div></div></figure><p id="a73a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">OK. Now let’s look at an early (in time) term, like the gradient propagated from the error to the third cell:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db to"><div class="dv r dw dx"><div class="tp dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_fAaYlJPgsPjRGJGoyc8XCw.png" width="106" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="106" height="79" srcset="" sizes="106px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/212/1*fAaYlJPgsPjRGJGoyc8XCw.png" width="106" height="79" role="presentation"/></noscript></div></div></div></figure><p id="9972" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Remember that <strong class="iy kp">J </strong>is an addition of errors from<strong class="iy kp"> Y </strong>individual outputs, so we backpropagate through each of the outputs first:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db tq"><div class="dv r dw dx"><div class="tr dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_vsybuvtlGl-cQqUI1Pqbag.png" width="609" height="164" role="presentation"></div><img class="dq dr s t u ds ai ed" width="609" height="164" srcset="https://miro.medium.com/max/552/1*vsybuvtlGl-cQqUI1Pqbag.png 276w, https://miro.medium.com/max/1104/1*vsybuvtlGl-cQqUI1Pqbag.png 552w, https://miro.medium.com/max/1218/1*vsybuvtlGl-cQqUI1Pqbag.png 609w" sizes="609px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1218/1*vsybuvtlGl-cQqUI1Pqbag.png" width="609" height="164" srcSet="https://miro.medium.com/max/552/1*vsybuvtlGl-cQqUI1Pqbag.png 276w, https://miro.medium.com/max/1104/1*vsybuvtlGl-cQqUI1Pqbag.png 552w, https://miro.medium.com/max/1218/1*vsybuvtlGl-cQqUI1Pqbag.png 609w" sizes="609px" role="presentation"/></noscript></div></div></div></figure><p id="4355" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The first few terms, where we backprop <strong class="iy kp">y_k </strong>to <strong class="iy kp">c_3 </strong>where <strong class="iy kp">k &lt; 3</strong>, would just be equal to zero because <strong class="iy kp">c_3 </strong>only exists after these outputs have been computed.</p><p id="e686" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s assume that <strong class="iy kp">Y = 100 </strong>and continue with our assumption that <strong class="iy kp">t = 100 </strong>(so each timestep gives rise to an output), for simplicity. With this, let’s now look at the last term in this sum.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ts"><div class="dv r dw dx"><div class="tt dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_JJIQxpb1mHjDn5KaoOKQkA.png" width="501" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="501" height="79" srcset="https://miro.medium.com/max/552/1*JJIQxpb1mHjDn5KaoOKQkA.png 276w, https://miro.medium.com/max/1002/1*JJIQxpb1mHjDn5KaoOKQkA.png 501w" sizes="501px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1002/1*JJIQxpb1mHjDn5KaoOKQkA.png" width="501" height="79" srcSet="https://miro.medium.com/max/552/1*JJIQxpb1mHjDn5KaoOKQkA.png 276w, https://miro.medium.com/max/1002/1*JJIQxpb1mHjDn5KaoOKQkA.png 501w" sizes="501px" role="presentation"/></noscript></div></div></div></figure><p id="6208" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s a lot of forget gates chained together. <strong class="iy kp">If one of these forget gates is [approximately] zero, the whole gradient dies</strong>. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and <strong class="iy kp">c_3 </strong>won’t make any contributions to the gradient here.</p><p id="fb13" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This isn’t <em class="kq">intrinsically </em>an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If <strong class="iy kp">f_4 </strong>is zero, then any <strong class="iy kp">y</strong> outputs at/past timestep 4 won’t be influenced by <strong class="iy kp">c_3 </strong>(as well as <strong class="iy kp">c_2 </strong>and <strong class="iy kp">c_1</strong>) because we “erased” it from memory. Therefore that particular gradient should be zero. If <strong class="iy kp">y_80</strong> is zero, then any outputs at/past timestep 80 won’t be influenced by <strong class="iy kp">c_1, c_2, … , c_79</strong>. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they’ll reflect that. <a href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">Gers 1999</a> calls this “releasing resources”.</p><p id="b6f8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Cell <strong class="iy kp">c_3 </strong>will still contribute to the overall gradient, though. For example, take this term:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db tu"><div class="dv r dw dx"><div class="tv dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_KZjK3wcZpYG_qnjkMB1HkA.png" width="474" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="474" height="79" srcset="https://miro.medium.com/max/552/1*KZjK3wcZpYG_qnjkMB1HkA.png 276w, https://miro.medium.com/max/948/1*KZjK3wcZpYG_qnjkMB1HkA.png 474w" sizes="474px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/948/1*KZjK3wcZpYG_qnjkMB1HkA.png" width="474" height="79" srcSet="https://miro.medium.com/max/552/1*KZjK3wcZpYG_qnjkMB1HkA.png 276w, https://miro.medium.com/max/948/1*KZjK3wcZpYG_qnjkMB1HkA.png 474w" sizes="474px" role="presentation"/></noscript></div></div></div></figure><p id="8736" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here, we’re looking at <strong class="iy kp">y_12 </strong>instead of <strong class="iy kp">y_100</strong>. Chances are that, if you have a sequence of length 100, your 100th cell state isn’t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.</p><p id="5cd8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">If we decide not to forget in the first 12 timesteps, ie. <strong class="iy kp">f_1 … f_12 </strong>are each not far from 1, then <strong class="iy kp">c_3 </strong>would have more influence over <strong class="iy kp">y_12 </strong>and the error that stems from <strong class="iy kp">y_12</strong>. Thus, the gradient would not vanish and <strong class="iy kp">c_3</strong> still contributes to update <strong class="iy kp">W_xi</strong>, it just doesn’t contribute a gradient where it’s not warranted to (that is, where it doesn’t actually contribute to any activation, because it’s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a <em class="kq">benefit </em>for LSTMs.</p><p id="7b24" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we’re multiplying by 1 at each timestep — effectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db tw"><div class="dv r dw dx"><div class="tx dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_rBJm9F6zz8drQnDlWd7wvQ.png" width="2190" height="694" role="presentation"></div><img class="dq dr s t u ds ai ed" width="2190" height="694" srcset="https://miro.medium.com/max/552/1*rBJm9F6zz8drQnDlWd7wvQ.png 276w, https://miro.medium.com/max/1104/1*rBJm9F6zz8drQnDlWd7wvQ.png 552w, https://miro.medium.com/max/1280/1*rBJm9F6zz8drQnDlWd7wvQ.png 640w, https://miro.medium.com/max/1400/1*rBJm9F6zz8drQnDlWd7wvQ.png 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/4380/1*rBJm9F6zz8drQnDlWd7wvQ.png" width="2190" height="694" srcSet="https://miro.medium.com/max/552/1*rBJm9F6zz8drQnDlWd7wvQ.png 276w, https://miro.medium.com/max/1104/1*rBJm9F6zz8drQnDlWd7wvQ.png 552w, https://miro.medium.com/max/1280/1*rBJm9F6zz8drQnDlWd7wvQ.png 640w, https://miro.medium.com/max/1400/1*rBJm9F6zz8drQnDlWd7wvQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">It’s… it’s beautiful!</figcaption></figure><p id="f12e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The gradient will have literally zero interactions or disturbances, and will just flow through like it’s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they’re <em class="kq">always </em>like this.</p><p id="87cf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But, let’s get back to reality. LSTMs aren’t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won’t happen at all.</p><p id="3db3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because <strong class="iy kp">y = 1</strong> is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling/approximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.</p><p id="d763" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it’s because we chose to forget that cell — so it’s not necessarily a bad thing. We just need to make sure the forget gates don’t block learning in initial stages of training; in such a case, we shouldn’t need to bother about vanishing gradients too much.</p><p id="60bc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here’s a more technical explanation:</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="mg dz r"></div></div></figure></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="e1c7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We can try computing some more derivatives, just for fun! Let’s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.</p><p id="62f7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We’ll expand <strong class="iy kp">c_4 </strong>and express it in terms of our gates only. In the process, each <strong class="iy kp">c_t, </strong>except <strong class="iy kp">c_1</strong>, will collapse into a few interactions between the <strong class="iy kp">f</strong>, <strong class="iy kp">i</strong>,<strong class="iy kp"> </strong>and<strong class="iy kp"> g </strong>gate:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ty"><div class="dv r dw dx"><div class="tz dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_2LZxa4YAMGCJqOrirI1-_w.png" width="669" height="166" role="presentation"></div><img class="dq dr s t u ds ai ed" width="669" height="166" srcset="https://miro.medium.com/max/552/1*2LZxa4YAMGCJqOrirI1-_w.png 276w, https://miro.medium.com/max/1104/1*2LZxa4YAMGCJqOrirI1-_w.png 552w, https://miro.medium.com/max/1280/1*2LZxa4YAMGCJqOrirI1-_w.png 640w, https://miro.medium.com/max/1338/1*2LZxa4YAMGCJqOrirI1-_w.png 669w" sizes="669px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1338/1*2LZxa4YAMGCJqOrirI1-_w.png" width="669" height="166" srcSet="https://miro.medium.com/max/552/1*2LZxa4YAMGCJqOrirI1-_w.png 276w, https://miro.medium.com/max/1104/1*2LZxa4YAMGCJqOrirI1-_w.png 552w, https://miro.medium.com/max/1280/1*2LZxa4YAMGCJqOrirI1-_w.png 640w, https://miro.medium.com/max/1338/1*2LZxa4YAMGCJqOrirI1-_w.png 669w" sizes="669px" role="presentation"/></noscript></div></div></div></figure><p id="5092" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now, let’s get the derivative of <strong class="iy kp">c_4</strong> with respect to one of the earliest possible gates, like <strong class="iy kp">g_2</strong>. In the expression above, this turns out to just be the coefficient of <strong class="iy kp">g_2</strong>:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db ua"><div class="dv r dw dx"><div class="ub dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_rHDurdaN9SKnChWfyVk38w.png" width="205" height="79" role="presentation"></div><img class="dq dr s t u ds ai ed" width="205" height="79" srcset="" sizes="205px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/410/1*rHDurdaN9SKnChWfyVk38w.png" width="205" height="79" role="presentation"/></noscript></div></div></div></figure><p id="708a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be <strong class="iy kp">i_2 ⊙</strong> <strong class="iy kp">f_3 ⊙ f_4</strong>, since <strong class="iy kp">i_2 </strong>controls what influence <strong class="iy kp">g_2 </strong>has over <strong class="iy kp">c_2</strong>, <strong class="iy kp">f_3 </strong>controls what influence <strong class="iy kp">c_2 </strong>has on <strong class="iy kp">c_3</strong>, and <strong class="iy kp">f_4 </strong>controls what influence <strong class="iy kp">c_3 </strong>has over <strong class="iy kp">c_4</strong>. Notice the chaining up of the forget gates 👻; everything about the carousels I just talked about — and what they imply about vanishing gradients — applies here.</p><p id="6229" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I’ll leave it up to you to derive something similar for the other gates.</p><p id="f15d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And that’s it! That’s why LSTMs rock their socks off when it comes to keeping their gradients in check.</p><p id="627a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here’s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:</p></div></div><div class="do"><div class="n p"><div class="uc ud ue uf ug uh ae ui af uj ah ai"><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="uk dz r"></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av">Super highway indeed. <a href="http://imgur.com/gallery/vaNahKE" class="co hg is it iu iv" target="_blank" rel="noopener">imgur.com/gallery/vaNahKE</a>.</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="868a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As you can see, the vanilla RNN’s gradients die off way quicker than the LSTM’s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is <em class="kq">learnable</em>. (I’m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don’t know the context of this GIF.) Also, part of the gradient signal definitely vanishes—it’s the signals that pass through the <strong class="iy kp">f/i/g </strong>gates that we looked at earlier and obfuscated from the cell state→cell state derivative. We showed they would vanish because of tanh/sigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they’ll get smaller and smaller. That’s the explanation for this GIF.</p><p id="2b66" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + <strong class="iy kp">∞</strong> = <strong class="iy kp">∞</strong>. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we’d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, <code class="dx oq or os ot b">grad = min(grad, clip_threshold)</code>. This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.</p><p id="69b6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.</p></div></div></section><hr class="hu cj hv hw hx hy ef hz ia ib ic id"><section class="dj dk dl dm dn"><div class="n p"><div class="z ab ac ae af ej ah ai"><p id="3679" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so <strong class="iy kp">h_t = o ⊙ c_t</strong>) or ditching the <strong class="iy kp">i </strong>input gate and only using <strong class="iy kp">g</strong>, since that would still satisfy the -1 to 1 range. The results didn’t change by much.</p><p id="dc3e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.</p><p id="c7f8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This highlights an issue with LSTMs — they are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there’s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they’re biologically inspired and that they’re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts/architectures that work but aren’t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better/simpler architectures is a hot topic of research right now.</p><p id="4bca" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">There are also other variants of RNNs, similar to LSTMs, like <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" class="co hg is it iu iv" target="_blank" rel="noopener">GRUs</a> (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It’s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they’re fairly new. (See, told you “coming up with better/simpler architectures is a hot topic of research right now” is true!)</p><h1 id="c57a" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Yay RNNs!</h1><p id="8a09" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that’ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.</p><p id="7337" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Sidenote: now, don’t be frightened by “RNNs”. <strong class="iy kp"><em class="kq">Do </em></strong>be frightened by “vanilla RNNs”, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.</p><p id="f30f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Many if not all of these are taken from Andrej Karpathy’s <a href="https://www.youtube.com/watch?v=cO0a0QYmFm8&amp;index=10&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA" class="co hg is it iu iv" target="_blank" rel="noopener">CS231n lecture</a>, or his blog post on the same subject:</p><div class="ow ox oy oz pa pb"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener"><div class="pc n an"><div class="pd n de p pe pf"><h2 class="aq kt pg as br ph gh gi pi gk em">The Unreasonable Effectiveness of Recurrent Neural Networks</h2><div class="pj r"><h3 class="aq cj ei as br ph gh gi pi gk av">Musings of a Computer Scientist.</h3></div><div class="pk r"><h4 class="aq cj ck as br ph gh gi pi gk av">karpathy.github.io</h4></div></div><div class="pl r"><div class="ul r pn po pp pl pq pr ps"></div></div></div></a></div><p id="83d4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">You should <strong class="iy kp">most certainly</strong> visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‘Visualizing the predictions and the “neuron” firings in the RNN’ section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.</p><p id="c051" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A recurrent neural network generated this body of text, after it “read” a bunch of Shakespeare:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db um"><div class="dv r dw dx"><div class="un dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_BkvFHx8nYL1-NHmzZ_BbCQ.png" width="574" height="619" role="presentation"></div><img class="dq dr s t u ds ai ed" width="574" height="619" srcset="https://miro.medium.com/max/552/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 276w, https://miro.medium.com/max/1104/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 552w, https://miro.medium.com/max/1148/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 574w" sizes="574px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1148/1*BkvFHx8nYL1-NHmzZ_BbCQ.png" width="574" height="619" srcSet="https://miro.medium.com/max/552/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 276w, https://miro.medium.com/max/1104/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 552w, https://miro.medium.com/max/1148/1*BkvFHx8nYL1-NHmzZ_BbCQ.png 574w" sizes="574px" role="presentation"/></noscript></div></div></div></figure><p id="62d6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Similarly, Karpathy gave an LSTM a lot of Paul Graham’s startup advice and life wisdom to read, and it produced this:</p><blockquote class="ie"><p id="3654" class="ip ig av ar en b iq mx my mz na nb io" data-selectable-paragraph="">“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”</p></blockquote><p id="ee9f" class="iw ji em ar iy b ff nc jj fh nd jk jb ne fs jd nf ft jf ng fu io dj" data-selectable-paragraph="">A lot of relevant terminology, but it doesn’t really… come together 😖.</p><p id="1fc3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">An LSTM can even generate valid XML, after reading Wikipedia!:</p><pre class="jv jw jx jy jz uo up ca"><span id="89df" class="uq ks em ar ot b ei ur us r ut" data-selectable-paragraph="">&lt;page&gt;<br>  &lt;title&gt;Antichrist&lt;/title&gt;<br>  &lt;id&gt;865&lt;/id&gt;<br>  &lt;revision&gt;<br>    &lt;id&gt;15900676&lt;/id&gt;<br>    &lt;timestamp&gt;2002-08-03T18:14:12Z&lt;/timestamp&gt;<br>    &lt;contributor&gt;<br>      &lt;username&gt;Paris&lt;/username&gt;<br>      &lt;id&gt;23&lt;/id&gt;<br>    &lt;/contributor&gt;<br>    &lt;minor /&gt;<br>    &lt;comment&gt;Automated conversion&lt;/comment&gt;<br>    &lt;text xml:space="preserve"&gt;#REDIRECT [[Christianity]]&lt;/text&gt;<br>  &lt;/revision&gt;<br>&lt;/page&gt;</span></pre><p id="7056" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this — put frankly — fancy looking bogus. Let’s be real, you could definitely believe this was actual math 😜:</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="uu dz r"></div></div></figure><p id="e129" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">An LSTM also read the Linux source code, and tried to write some code of its own:</p><pre class="jv jw jx jy jz uo up ca"><span id="ce46" class="uq ks em ar ot b ei ur us r ut" data-selectable-paragraph=""><em class="kq">/*<br> * Increment the size file of the new incorrect UI_FILTER group information<br> * of the size generatively.<br> */</em><br><strong class="ot kp">static</strong> <strong class="ot kp">int</strong> <strong class="ot kp">indicate_policy</strong>(<strong class="ot kp">void</strong>)<br>{<br>  <strong class="ot kp">int</strong> error;<br>  <strong class="ot kp">if</strong> (fd <strong class="ot kp">==</strong> MARN_EPT) {<br>    <em class="kq">/*<br>     * The kernel blank will coeld it to userspace.<br>     */</em><br>    <strong class="ot kp">if</strong> (ss<strong class="ot kp">-&gt;</strong>segment <strong class="ot kp">&lt;</strong> mem_total)<br>      unblock_graph_and_set_blocked();<br>    <strong class="ot kp">else</strong><br>      ret <strong class="ot kp">=</strong> 1;<br>    <strong class="ot kp">goto</strong> bail;<br>  }<br>  segaddr <strong class="ot kp">=</strong> in_SB(in.addr);<br>  selector <strong class="ot kp">=</strong> seg <strong class="ot kp">/</strong> 16;<br>  setup_works <strong class="ot kp">=</strong> true;<br>  <strong class="ot kp">for</strong> (i <strong class="ot kp">=</strong> 0; i <strong class="ot kp">&lt;</strong> blocks; i<strong class="ot kp">++</strong>) {<br>    seq <strong class="ot kp">=</strong> buf[i<strong class="ot kp">++</strong>];<br>    bpf <strong class="ot kp">=</strong> bd<strong class="ot kp">-&gt;</strong>bd.next <strong class="ot kp">+</strong> i <strong class="ot kp">*</strong> search;<br>    <strong class="ot kp">if</strong> (fd) {<br>      current <strong class="ot kp">=</strong> blocked;<br>    }<br>  }<br>  rw<strong class="ot kp">-&gt;</strong>name <strong class="ot kp">=</strong> "Getjbbregs";<br>  bprm_self_clearl(<strong class="ot kp">&amp;</strong>iv<strong class="ot kp">-&gt;</strong>version);<br>  regs<strong class="ot kp">-&gt;</strong>new <strong class="ot kp">=</strong> blocks[(BPF_STATS <strong class="ot kp">&lt;&lt;</strong> info<strong class="ot kp">-&gt;</strong>historidac)] <strong class="ot kp">|</strong> PFMR_CLOBATHINC_SECONDS <strong class="ot kp">&lt;&lt;</strong> 12;<br>  <strong class="ot kp">return</strong> segtable;<br>}</span></pre><p id="da41" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp"><em class="kq">SUPERINTELLIGENCE MUCH‽ SELF-RECURSIVE IMPROVEMENT MUCH‽ THE END OF THE UNIVERSE MUCH‽</em></strong></p><p id="7094" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Nope. Just some code doesn’t compile or make any sense. It even has its own bogus comments!</p><p id="b3fc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Generating music? Easy! A fun watch:</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="uk dz r"></div></div></figure><p id="c80a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A more informative watch:</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="uk dz r"></div></div></figure><p id="6518" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Something even cooler and… creepier (seriously, the results after the first couple iterations of training are so unsettling):</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="uk dz r"></div></div></figure><h1 id="7eb1" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">In Practice</h1><p id="a21c" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">So we’ve seen how RNNs work in theory; now where do they fit in in practice?</p><p id="e928" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">As it turns out, recurrent neural networks can do a whole lot. I’ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.</p><h2 id="e30a" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Bidirectional Recurrent Neural Networks</h2><p id="5985" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Problem</strong>: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time <strong class="iy kp">t</strong> to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time <strong class="iy kp">t</strong> and the output is the predicted <a href="https://en.wikipedia.org/wiki/Phoneme" class="co hg is it iu iv" target="_blank" rel="noopener">phoneme</a><em class="kq"> </em>at that time. In our traditional RNN architecture, the output at time <strong class="iy kp">t</strong> is conditioned <em class="kq">only</em> on input vectors <strong class="iy kp">1..t</strong>, but as it turns out future information might be useful too. The sounds at time step <strong class="iy kp">t+1 </strong>(and maybe <strong class="iy kp">t+2</strong>, <strong class="iy kp">t+3</strong>, …) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won’t have access to them until we already output a prediction at time <strong class="iy kp">t</strong>. That’s bad.</p><p id="60ef" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Solution</strong>: We essentially “double up” each RNN neuron into two independent neurons — a “forward” neuron and a “backward” neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs <strong class="iy kp">0..T</strong> sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.</p><p id="e2ef" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We’ll look at an example to make sense of all this.</p><figure class="jv jw jx jy jz do ed ka he kb jl kc kd ke be kf kg kh ki kj kk paragraph-image"><div class="da db ob"><div class="dv r dw dx"><div class="vg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_Vsvw39SW0xEwRLijLRb3qg.png" width="404" height="504" role="presentation"></div><img class="dq dr s t u ds ai ed" width="404" height="504" srcset="https://miro.medium.com/max/552/1*Vsvw39SW0xEwRLijLRb3qg.png 276w, https://miro.medium.com/max/808/1*Vsvw39SW0xEwRLijLRb3qg.png 404w" sizes="404px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/808/1*Vsvw39SW0xEwRLijLRb3qg.png" width="404" height="504" srcSet="https://miro.medium.com/max/552/1*Vsvw39SW0xEwRLijLRb3qg.png 276w, https://miro.medium.com/max/808/1*Vsvw39SW0xEwRLijLRb3qg.png 404w" sizes="404px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest input.</figcaption></figure><figure class="dp do ed ka he kb jl kc kd ke be kf kg kh ki kj kk paragraph-image"><div class="kl km dw kn ai"><div class="da db vh"><div class="dv r dw dx"><div class="vi dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_JZNjmHjYFVcHrPKTDmvoXQ.png" width="606" height="504" role="presentation"></div><img class="dq dr s t u ds ai ed" width="606" height="504" srcset="https://miro.medium.com/max/552/1*JZNjmHjYFVcHrPKTDmvoXQ.png 276w, https://miro.medium.com/max/1000/1*JZNjmHjYFVcHrPKTDmvoXQ.png 500w" sizes="500px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1212/1*JZNjmHjYFVcHrPKTDmvoXQ.png" width="606" height="504" srcSet="https://miro.medium.com/max/552/1*JZNjmHjYFVcHrPKTDmvoXQ.png 276w, https://miro.medium.com/max/1000/1*JZNjmHjYFVcHrPKTDmvoXQ.png 500w" sizes="500px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph="">This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one output.</figcaption></figure><p id="ea03" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s walk through this timestep-by-timestep. At <strong class="iy kp">t=0</strong>, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let’s look at the BiRNN: the “forward” half of our BiRNN neuron does exactly the same thing, but the “backward” half looks through all of our inputs — in reverse order, <strong class="iy kp">t=T..0</strong> — and updates its hidden state with each one. Then when we get to the <strong class="iy kp">t=0</strong> input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the “forward” half (“combine” is pretty loosely-defined, usually just by concatenation or addition). Moving on to <strong class="iy kp">t=1</strong>, our “forward” part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our “backward” counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.</p><p id="1bf8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And that’s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we’ll see them popping up in some of the other case studies that we’ll be looking at.</p><h2 id="7549" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Autoencoders</h2><p id="cdb3" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Remember when we talked about <a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a">autoencoders</a>? Turns out we can use RNNs there too!</p><p id="107c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s refresh: what is an autoencoder? Put simply, it’s a clever way of tricking a neural network to learn a useful representation of some data. Let’s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vj"><div class="dv r dw dx"><div class="vk dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_M1bVZtZ6UPTyXoiy_" width="677" height="506" role="presentation"></div><img class="dq dr s t u ds ai ed" width="677" height="506" srcset="https://miro.medium.com/max/552/0*M1bVZtZ6UPTyXoiy. 276w, https://miro.medium.com/max/1104/0*M1bVZtZ6UPTyXoiy. 552w, https://miro.medium.com/max/1280/0*M1bVZtZ6UPTyXoiy. 640w, https://miro.medium.com/max/1354/0*M1bVZtZ6UPTyXoiy. 677w" sizes="677px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1354/0*M1bVZtZ6UPTyXoiy." width="677" height="506" srcSet="https://miro.medium.com/max/552/0*M1bVZtZ6UPTyXoiy. 276w, https://miro.medium.com/max/1104/0*M1bVZtZ6UPTyXoiy. 552w, https://miro.medium.com/max/1280/0*M1bVZtZ6UPTyXoiy. 640w, https://miro.medium.com/max/1354/0*M1bVZtZ6UPTyXoiy. 677w" sizes="677px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><a href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" class="co hg is it iu iv" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png</a></figcaption></figure><p id="fe44" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">…and train it to reproduce the input in the output.</p><p id="46d5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vl"><div class="dv r dw dx"><div class="vm dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_RP5VZyqDJ9JI5wBk_" width="201" height="34" role="presentation"></div><img class="dq dr s t u ds ai ed" width="201" height="34" srcset="" sizes="201px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/402/0*RP5VZyqDJ9JI5wBk." width="201" height="34" role="presentation"/></noscript></div></div></div></figure><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vn"><div class="dv r dw dx"><div class="vo dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_Tc8mc_NWMmQZ15ND_" width="200" height="34" role="presentation"></div><img class="dq dr s t u ds ai ed" width="200" height="34" srcset="" sizes="200px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/400/0*Tc8mc_NWMmQZ15ND." width="200" height="34" role="presentation"/></noscript></div></div></div></figure><p id="c1de" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s make this a tad more concrete. We have two functions, <strong class="iy kp">f</strong> and <strong class="iy kp">g</strong>. <strong class="iy kp">f</strong> is our encoder, mapping from an <strong class="iy kp">n</strong>-long vector to an <strong class="iy kp">m</strong>-long vector. (<strong class="iy kp">n</strong> is the size of our input, <strong class="iy kp">m</strong> is the size of our latent representation.) <strong class="iy kp">g</strong> is our decoder, which maps back from an <strong class="iy kp">m</strong>-long vector to an <strong class="iy kp">n</strong>-long vector. In the normal autoencoder setting, both <strong class="iy kp">f</strong> and <strong class="iy kp">g</strong> are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vp"><div class="dv r dw dx"><div class="vq dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_1R_heM-ujpUvlbGM_" width="182" height="37" role="presentation"></div><img class="dq dr s t u ds ai ed" width="182" height="37" srcset="" sizes="182px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/364/0*1R_heM-ujpUvlbGM." width="182" height="37" role="presentation"/></noscript></div></div></div></figure><p id="1680" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So, where do RNNs fit in? Let’s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here’s how it works: we feed our input sequence into the <em class="kq">encoder RNN</em>. With each input vector of the sequence, this <em class="kq">encoder</em> updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our <em class="kq">decoder RNN</em> the initial hidden state of our <em class="kq">encoder</em>, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.</p><p id="b8c2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have <strong class="iy kp">q</strong> n-long vectors going into <strong class="iy kp">f</strong> and coming out of <strong class="iy kp">g</strong>. So <strong class="iy kp">q</strong> <strong class="iy kp">n</strong>-long vectors go in to <strong class="iy kp">f</strong>, and a single <strong class="iy kp">m</strong>-long vector comes out. We then give this <strong class="iy kp">m</strong>-long vector back to <strong class="iy kp">g</strong>, which spits out <strong class="iy kp">q</strong> <strong class="iy kp">n</strong>-long vectors.</p><p id="5a64" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That was a lot of letters, but you get the idea (I hope).</p><p id="b829" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence — a variable-length sequence, mind you — and convert it into a fixed-size vector. And then convert that back to a variable-length sequence.</p><p id="5546" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It turns out this model is actually incredibly powerful, so let’s take a look at one particularly useful (and successful) application: machine translation.</p><h2 id="4be2" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Neural Machine Translation</h2><p id="9d22" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Let’s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?</p><p id="cd0e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The model we’re going to look at specifically is Google’s implementation of NMT. You can read all the gory details <a href="https://arxiv.org/pdf/1609.08144.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">in their paper</a>, but for now why don’t I give you the watered-down version.</p><p id="3f1b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">At it’s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of — we’ll talk more about that later), which we sample from to get our [translated] sentence. 🎉</p><p id="afc6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here’s a scary diagram from the paper:</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vr"><div class="dv r dw dx"><div class="vs dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_mk1BeF8ANMbAVOzD_" width="656" height="363" role="presentation"></div><img class="dq dr s t u ds ai ed" width="656" height="363" srcset="https://miro.medium.com/max/552/0*mk1BeF8ANMbAVOzD. 276w, https://miro.medium.com/max/1104/0*mk1BeF8ANMbAVOzD. 552w, https://miro.medium.com/max/1280/0*mk1BeF8ANMbAVOzD. 640w, https://miro.medium.com/max/1312/0*mk1BeF8ANMbAVOzD. 656w" sizes="656px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1312/0*mk1BeF8ANMbAVOzD." width="656" height="363" srcSet="https://miro.medium.com/max/552/0*mk1BeF8ANMbAVOzD. 276w, https://miro.medium.com/max/1104/0*mk1BeF8ANMbAVOzD. 552w, https://miro.medium.com/max/1280/0*mk1BeF8ANMbAVOzD. 640w, https://miro.medium.com/max/1312/0*mk1BeF8ANMbAVOzD. 656w" sizes="656px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><a href="https://arxiv.org/abs/1609.08144" class="co hg is it iu iv" target="_blank" rel="noopener">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p id="a450" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">But there are a few other aspects to the GNMT that are important to note (there’s actually lots of interesting stuff going on in this architecture, so I really recommend you do <a href="https://arxiv.org/pdf/1609.08144.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">read the paper</a>).</p><p id="1fbc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Let’s turn our <em class="kq">attention</em> to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder’s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does <em class="kq">not</em><strong class="iy kp"><em class="kq"> </em></strong>produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one <em class="kq">context</em> vector using something called <em class="kq">soft attention</em>.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db vt"><div class="dv r dw dx"><div class="vu dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_ua03RdgdNWPw1_Jd_" width="704" height="244" role="presentation"></div><img class="dq dr s t u ds ai ed" width="704" height="244" srcset="https://miro.medium.com/max/552/0*ua03RdgdNWPw1_Jd. 276w, https://miro.medium.com/max/1104/0*ua03RdgdNWPw1_Jd. 552w, https://miro.medium.com/max/1280/0*ua03RdgdNWPw1_Jd. 640w, https://miro.medium.com/max/1400/0*ua03RdgdNWPw1_Jd. 700w" sizes="700px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/1408/0*ua03RdgdNWPw1_Jd." width="704" height="244" srcSet="https://miro.medium.com/max/552/0*ua03RdgdNWPw1_Jd. 276w, https://miro.medium.com/max/1104/0*ua03RdgdNWPw1_Jd. 552w, https://miro.medium.com/max/1280/0*ua03RdgdNWPw1_Jd. 640w, https://miro.medium.com/max/1400/0*ua03RdgdNWPw1_Jd. 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><a href="https://arxiv.org/abs/1609.08144" class="co hg is it iu iv" target="_blank" rel="noopener">https://arxiv.org/abs/1609.08144</a></figcaption></figure><p id="ddb5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the <em class="kq">last</em> time step. Following the notation from the paper, we’ll call that <strong class="iy kp">yi-1</strong>. We also have a series of encoder outputs, <strong class="iy kp">x1…xM</strong>, one for each encoder timestep. For each <em class="kq">encoder</em> timestep, we give our special attention function <strong class="iy kp">yi-1 </strong>and <strong class="iy kp">xt</strong> and get back a <em class="kq">single fixed-size vector</em> <strong class="iy kp">st</strong>, which we then run through a softmax. So, we’ve converted our encoder information from that timestep (and some decoder information) into a single attention vector — this attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output <strong class="iy kp">xt</strong>, which has the effect of “focusing” more on certain values and less on others. Finally, we take the sum of those “focused” vectors over each encoder timestep to produce our attention context for this timestep <strong class="iy kp">ai</strong>, which is fed to every decoder layer.</p><p id="8681" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Oh yeah, that attention function? That’s just yet <em class="kq">another</em> neural network.</p><p id="145a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called <em class="kq">hard attention</em>, in which we select just one of the possible inputs and “focus” solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.</p><p id="10b2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller “wordpieces” to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.</p><p id="818e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">A few months ago, <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" class="co hg is it iu iv" target="_blank" rel="noopener">Google put their GNMT model into production</a>. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.</p><h2 id="0c13" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Long-Term Recurrent Convolutional Networks</h2><p id="c3b8" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">(Not to be confused with LCRNs.)</p><p id="e750" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Problem</strong>: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences…how do we put the two together?</p><p id="606b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Solution</strong>: The solution proposed in <a href="https://arxiv.org/pdf/1411.4389.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">this paper</a> is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vv"><div class="dv r dw dx"><div class="vw dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_qiQ7DvCkHydXAFZ1_" width="355" height="319" role="presentation"></div><img class="dq dr s t u ds ai ed" width="355" height="319" srcset="https://miro.medium.com/max/552/0*qiQ7DvCkHydXAFZ1. 276w, https://miro.medium.com/max/710/0*qiQ7DvCkHydXAFZ1. 355w" sizes="355px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/710/0*qiQ7DvCkHydXAFZ1." width="355" height="319" srcSet="https://miro.medium.com/max/552/0*qiQ7DvCkHydXAFZ1. 276w, https://miro.medium.com/max/710/0*qiQ7DvCkHydXAFZ1. 355w" sizes="355px" role="presentation"/></noscript></div></div></div><figcaption class="ee ef dc da db eg eh aq cj ei as av" data-selectable-paragraph=""><a href="https://arxiv.org/abs/1411.4389" class="co hg is it iu iv" target="_blank" rel="noopener">https://arxiv.org/abs/1411.4389</a></figcaption></figure><p id="1c9f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s really all there is to it, and the reason it works is because (<a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58">as we’ve seen before</a>) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what’s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It’s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.</p><h2 id="b189" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Image Captioning</h2><p id="3e15" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">(To be confused with LCRNs!)</p><p id="7c86" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">this 2015 paper</a> from Karpathy <em class="kq">et al</em>. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that’s cool too.</p><p id="ca5d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The idea behind image captioning is kind of self-explanatory, but I’ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it — a computer can go from pixels to interpreting what it’s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can’t really believe stuff like this actually works, but somehow it does.</p><p id="3370" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that’s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.</p><p id="de23" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.</p><p id="0330" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">It’s not strictly necessary to feed the word that we sampled back to the network, but that’s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" class="co hg is it iu iv" target="_blank" rel="noopener">here</a>.</p><h2 id="6ec4" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">Neural Machine Translation, Again</h2><p id="aba2" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Yes, NMTs are just that cool that I need to talk about them again.</p><p id="7a0e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Problem</strong>: With our good ol’ GNMT architecture, we can train a massive model to convert from language A to language B. That’s great — except, if we support more than a hundred languages, we need to train more than <strong class="iy kp">10,000</strong> different language-pair models, each of which can take months to converge. That’s no good, and it’s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But…what if we didn’t need to train a separate model for each language pair? What if we could train one model for all the language pairs — impossible, right?</p><p id="1717" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">The Solution</strong>: Apparently it’s not impossible, and to make things even crazier, <strong class="iy kp">we can use the original GNMT architecture without modification</strong>. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)</p><p id="90b7" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">So we’ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. <a href="https://arxiv.org/pdf/1611.04558.pdf" class="co hg is it iu iv" target="_blank" rel="noopener">The paper</a> elaborates on the implications and benefits of this more than I will, but to summarize:</p><ul class=""><li id="2db1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io lv lj lk" data-selectable-paragraph="">One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters — simplicity wins out over complexity.</li><li id="66a7" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).</li><li id="2a4f" class="iw ji em ar iy b ff ll jj fh lm jk jb ln fs jd lo ft jf lp fu io lv lj lk" data-selectable-paragraph="">This one is absolutely nuts. If we train our network to translate English → Spanish and Spanish → French, <strong class="iy kp">our network automatically knows how to translate English → French</strong> (reasonably well).</li></ul><p id="0314" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Expanding on that last point some more: the authors of the paper even found evidence of an <em class="kq">interlingua</em>, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder/decoder to convert to/from the interlingua for each language, and we immediately know how to translate to and from that language. We aren’t <em class="kq">quite</em> there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.</p><h2 id="e74d" class="uq ks em ar aq kt uv uw ux uy uz va vb vc vd ve vf" data-selectable-paragraph="">So, yeah</h2><p id="1de9" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">RNNs are pretty awesome. There are new RNN papers published literally every day and it’s impossible to cover everything — if you think I missed something important, definitely <a href="https://twitter.com/LennyKhazan" class="co hg is it iu iv" target="_blank" rel="noopener">let me know</a>. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we’re going to be covering them soon!)</p><h1 id="449c" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Building a Vanilla Recurrent Neural Network</h1><p id="55e8" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Let’s get practical for a minute and see how we can build one of these things in practice. We’ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you’re using one of these in practice <em class="kq">there are much better solutions!</em> For out-of-the-box functional deep learning models <a href="https://keras.io/" class="co hg is it iu iv" target="_blank" rel="noopener">Keras</a> is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I’m a fan of the newly-released <a href="http://pytorch.org/" class="co hg is it iu iv" target="_blank" rel="noopener">PyTorch</a>, or the “older” <a href="https://www.tensorflow.org/" class="co hg is it iu iv" target="_blank" rel="noopener">TensorFlow</a>.</p><p id="5997" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">I’m going to walk us through <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" class="co hg is it iu iv" target="_blank" rel="noopener">this implementation</a> line by line so we can see exactly what’s going on. It’s really well-commented, so feel free to peruse it on your own too.</p><p id="8745" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Afterwards, I challenge you to code an LSTM!</p><figure class="jv jw jx jy jz do"><div class="dv r dw"><div class="vx dz r"></div></div></figure><p id="8a37" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">import numpy as np</code></p><p id="54ad" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Well, duh.</p><p id="a14a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">data = open(‘input.txt’, ‘r’).read()<br>chars = list(set(data))<br>data_size, vocab_size = len(data), len(chars)<br>print ‘data has %d characters, %d unique.’ % (data_size, vocab_size)<br>char_to_ix = { ch:i for i,ch in enumerate(chars) }<br>ix_to_char = { i:ch for i,ch in enumerate(chars) }</code></p><p id="3f08" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We’ll use this when converting characters to/from a one-hot encoding later on.</p><p id="f0a4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">hidden_size = 100<br>seq_length = 25<br>learning_rate = 1e-1</code></p><p id="b15a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we’ll train our network on batches of 25 characters at a time. Since we’ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to .1.</p><p id="a836" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden</code><br><code class="dx oq or os ot b">Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden<br>Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output<br>bh = np.zeros((hidden_size, 1)) # hidden bias<br>by = np.zeros((vocab_size, 1)) # output bias</code></p><p id="7dce" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We set up our parameters — note that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.</p><p id="c19b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Now let’s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.</p><p id="08d8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">xs, hs, ys, ps = {}, {}, {}, {}<br>hs[-1] = np.copy(hprev)<br>loss = 0</code></p><p id="7017" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.</p><p id="03ad" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">for t in xrange(len(inputs)):</code></p><p id="7bca" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Go through each timestep, and for each timestep…</p><p id="d87e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation<br>xs[t][inputs[t]] = 1</code></p><p id="0962" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Convert our input character at this timestep to a one-hot vector.</p><p id="ea9f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state</code></p><p id="de7c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Update our hidden state. We saw this formula already — use our <strong class="iy kp">Wxh</strong> and <strong class="iy kp">Whh</strong> matrices to update our hidden state based on the last state and our input, and add a bias.</p><p id="15c1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">ys[t] = np.dot(Why, hs[t]) + by</code></p><p id="26df" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Compute our output…</p><p id="4308" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars</code></p><p id="46cd" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">…and convert it to a probability distribution with a softmax.</p><p id="b3bb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)</code></p><p id="011a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the <em class="kq">actual</em> next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.</p><p id="d366" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">That’s it for the forward pass (not bad, right? Boiled down, it’s like six lines of code. Piece of cake).</p><p id="b1db" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code><br><code class="dx oq or os ot b"> dbh, dby = np.zeros_like(bh), np.zeros_like(by)</code><br><code class="dx oq or os ot b"> dhnext = np.zeros_like(hs[0])</code></p><p id="cdb0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Setting up some variables for our backward pass — the gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we’ll see how that works in a bit).</p><p id="2fd8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">for t in reversed(xrange(len(inputs))):</code></p><p id="14bf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Go through our sequence in reverse as we back up the gradients.</p><p id="9148" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dy = np.copy(ps[t])<br> dy[targets[t]] -= 1 # backprop into y. see <a href="http://cs231n.github.io/neural-networks-case-study/#grad" class="co hg is it iu iv" target="_blank" rel="noopener">http://cs231n.github.io/neural-networks-case-study/#grad</a> if confused here</code></p><p id="3fb3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">First, get the gradient of the output, dy. <a href="http://cs231n.github.io/neural-networks-case-study/#grad" class="co hg is it iu iv" target="_blank" rel="noopener">As it turns out</a>, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.</p><p id="b8b8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><a class="co hg is it iu iv" target="_blank" rel="noopener" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d">Remember backpropogation</a>? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db vy"><div class="dv r dw dx"><div class="vz dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_TVvKSJJqaM9CDjlk_" width="426" height="86" role="presentation"></div><img class="dq dr s t u ds ai ed" width="426" height="86" srcset="https://miro.medium.com/max/552/0*TVvKSJJqaM9CDjlk. 276w, https://miro.medium.com/max/852/0*TVvKSJJqaM9CDjlk. 426w" sizes="426px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/852/0*TVvKSJJqaM9CDjlk." width="426" height="86" srcSet="https://miro.medium.com/max/552/0*TVvKSJJqaM9CDjlk. 276w, https://miro.medium.com/max/852/0*TVvKSJJqaM9CDjlk. 426w" sizes="426px" role="presentation"/></noscript></div></div></div></figure><p id="247c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dWhy += np.dot(dy, hs[t].T)</code></p><p id="2ff0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="kl km dw kn ai"><div class="da db wa"><div class="dv r dw dx"><div class="wb dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/0_QKDwSXVEJ9fHQ4hT_" width="321" height="86" role="presentation"></div><img class="dq dr s t u ds ai ed" width="321" height="86" srcset="https://miro.medium.com/max/552/0*QKDwSXVEJ9fHQ4hT. 276w, https://miro.medium.com/max/642/0*QKDwSXVEJ9fHQ4hT. 321w" sizes="321px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/642/0*QKDwSXVEJ9fHQ4hT." width="321" height="86" srcSet="https://miro.medium.com/max/552/0*QKDwSXVEJ9fHQ4hT. 276w, https://miro.medium.com/max/642/0*QKDwSXVEJ9fHQ4hT. 321w" sizes="321px" role="presentation"/></noscript></div></div></div></div></figure><p id="1c4f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dby += dy</code></p><p id="a781" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.</p><p id="6890" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dh = np.dot(Why.T, dy) + dhnext # backprop into h</code></p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db wc"><div class="dv r dw dx"><div class="wd dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_cVr1t2s7gsC4Sd6vowSkHA.png" width="265" height="74" role="presentation"></div><img class="dq dr s t u ds ai ed" width="265" height="74" srcset="" sizes="265px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/530/1*cVr1t2s7gsC4Sd6vowSkHA.png" width="265" height="74" role="presentation"/></noscript></div></div></div></figure><p id="6dbe" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We compute dL/dh using the chain rule, and accumulate it over all timesteps (hence <code class="dx oq or os ot b">+ dhnext</code>). We’ll need this for the next step.</p><p id="6d9e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dhraw = (1 — hs[t] * hs[t]) * dh # backprop through tanh nonlinearity</code></p><p id="796c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This computes the derivative of the <code class="dx oq or os ot b">np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)</code> line from earlier.</p><p id="7fa1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dbh += dhraw</code></p><p id="f74d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Which is also our bh derivative, for the same reason that the by derivative was just dy.</p><p id="a0f2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dWxh += np.dot(dhraw, xs[t].T)<br>dWhh += np.dot(dhraw, hs[t-1].T)</code></p><p id="cea8" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We accumulate our weight gradients.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db we"><div class="dv r dw dx"><div class="wf dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_uf-YEbf0258UhbDc5QZLRw.png" width="286" height="77" role="presentation"></div><img class="dq dr s t u ds ai ed" width="286" height="77" srcset="https://miro.medium.com/max/552/1*uf-YEbf0258UhbDc5QZLRw.png 276w, https://miro.medium.com/max/572/1*uf-YEbf0258UhbDc5QZLRw.png 286w" sizes="286px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/572/1*uf-YEbf0258UhbDc5QZLRw.png" width="286" height="77" srcSet="https://miro.medium.com/max/552/1*uf-YEbf0258UhbDc5QZLRw.png 276w, https://miro.medium.com/max/572/1*uf-YEbf0258UhbDc5QZLRw.png 286w" sizes="286px" role="presentation"/></noscript></div></div></div></figure><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db qx"><div class="dv r dw dx"><div class="wg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_Vl1LVzPJSZKDplA9J5cElg.png" width="287" height="77" role="presentation"></div><img class="dq dr s t u ds ai ed" width="287" height="77" srcset="https://miro.medium.com/max/552/1*Vl1LVzPJSZKDplA9J5cElg.png 276w, https://miro.medium.com/max/574/1*Vl1LVzPJSZKDplA9J5cElg.png 287w" sizes="287px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/574/1*Vl1LVzPJSZKDplA9J5cElg.png" width="287" height="77" srcSet="https://miro.medium.com/max/552/1*Vl1LVzPJSZKDplA9J5cElg.png 276w, https://miro.medium.com/max/574/1*Vl1LVzPJSZKDplA9J5cElg.png 287w" sizes="287px" role="presentation"/></noscript></div></div></div></figure><p id="012f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">dhnext = np.dot(Whh.T, dhraw)</code></p><p id="bb22" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And finally, store dh for this timestep so we can use it for the previous one.</p><p id="1ea0" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">for dparam in [dWxh, dWhh, dWhy, dbh, dby]:<br>np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients</code></p><p id="a901" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Last but not least, a little gradient clipping so we don’t get no exploding gradients.</p><p id="8392" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]</code></p><p id="4dc5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And then return all the gradients so we can apply an optimizer step. And that’s it for the backprop code; not <em class="kq">too</em> bad, right?</p><p id="4eaf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">def sample(h, seed_ix, n):</code></p><p id="ed15" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This method is used for sampling a generated sequence from the network, starting with state <code class="dx oq or os ot b">h</code>, first letter <code class="dx oq or os ot b">seed_ix</code>, with length <code class="dx oq or os ot b">n</code>.</p><p id="d445" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">x = np.zeros((vocab_size, 1)) <br>x[seed_ix] = 1</code></p><p id="a8f6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Set up our one-hot encoded input vector based on the seed character.</p><p id="8a82" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">ixes = []</code></p><p id="7e2b" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And an array to keep track of our sequence.</p><p id="3cae" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">for t in xrange(n):</code></p><p id="c8e6" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">To generate each character in our sequence…</p><p id="18cf" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</code></p><p id="6d86" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Update our hidden state! We saw this formula in the last function, too.</p><p id="ce6f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">y = np.dot(Why, h) + by<br>p = np.exp(y) / np.sum(np.exp(y))</code></p><p id="d108" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Generate our output and run it through a softmax. Again, straight from the last function.</p><p id="62ae" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">ix = np.random.choice(range(vocab_size), p=p.ravel())</code></p><p id="fd39" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Sample from our output distribution using some numpy magic.</p><p id="cec1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">x = np.zeros((vocab_size, 1))<br>x[ix] = 1<br>ixes.append(ix)</code></p><p id="e8b3" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Convert the sampled value into a one-hot encoding and append it to the array.</p><p id="4928" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">return ixes</code></p><p id="e074" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">…and of course, return the final sequence when we’re done.</p><p id="6968" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">n, p = 0, 0</code></p><p id="2eb1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">n</strong> is the number of training iterations we’ve done. <strong class="iy kp">p</strong> is the index into our training data for where we are now.</p><p id="ee12" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</code></p><p id="9951" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time — it’s just a variant on gradient descent).</p><p id="ffc4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">while True:</code></p><p id="bcde" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Training loop.</p><p id="a04e" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">if p+seq_length+1 &gt;= len(data) or n == 0:</code></p><p id="3444" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">This is a little check to see if we need to reset our memory because we’re starting back at the beginning of our data.</p><p id="a0be" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">hprev = np.zeros((hidden_size,1)) # reset RNN memory</code></p><p id="038a" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">…and if we are, reset the memory.</p><p id="9c3d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">p = 0</code></p><p id="bd23" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And reset the data pointer.</p><p id="0161" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]<br>targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]</code></p><p id="70c2" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We grab a <code class="dx oq or os ot b">seq_length</code>-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our “targets” will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.</p><p id="3f42" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">if n % 100 == 0:<br>sample_ix = sample(hprev, inputs[0], 200)<br>txt = ‘’.join(ix_to_char[ix] for ix in sample_ix)<br>print ‘ — — \n %s \n — — ‘ % (txt, )</code></p><p id="a7eb" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.</p><p id="e412" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</code></p><p id="538d" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Do a forward pass, backward pass, and get the gradients.</p><p id="9d99" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">smooth_loss = smooth_loss * 0.999 + loss * 0.001</code></p><p id="4ab1" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Adagrad stuff.</p><p id="3823" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">if n % 100 == 0: print ‘iter %d, loss: %f’ % (n, smooth_loss) # print progress</code></p><p id="b38c" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Keep up with progress.</p><p id="995f" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):<br>mem += dparam * dparam<br>param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update</code></p><p id="9509" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">More Adagrad. We should really do an article on optimization algorithms.</p><p id="a2fc" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><code class="dx oq or os ot b">p += seq_length # move data pointer<br>n += 1 # iteration counter</code></p><p id="a889" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Annnddd finally, we update our data pointer and iteration counter.</p><p id="7850" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">And that’s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM… and TensorFlow doesn’t count!</p><h1 id="5a53" class="kr ks em ar aq kt eo ku eq kv kw kx ky kz la lb lc" data-selectable-paragraph="">Conclusion</h1><p id="8a00" class="iw ji em ar iy b ff ld jj fh le jk jb lf fs jd lg ft jf lh fu io dj" data-selectable-paragraph="">Wow. That was a <strong class="iy kp"><em class="kq">lot</em></strong>. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don’t just know about something cool; you know about something <em class="kq">very important</em> — something that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.</p><p id="eec5" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">Something this article didn’t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs/LSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It’s not something you need to worry about, but you might want to look into.</p><p id="f6a4" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph="">We’re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I’m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There’s very little compulsory content or “groundwork” we need to cover anymore. So, now, we’re officially onto the cool stuff.</p><p id="4f33" class="iw ji em ar iy b ff iz jj fh ja jk jb jc fs jd je ft jf jg fu io dj" data-selectable-paragraph=""><strong class="iy kp">That’s right.</strong> A Year Of AI is officially… <em class="kq">cool</em>.</p><figure class="jv jw jx jy jz do da db paragraph-image"><div class="da db wh"><div class="dv r dw dx"><div class="mg dz r"><div class="zx ahv s t u ds ai br dt du"><img class="s t u ds ai ea eb ec" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_necEBipfgD-Z3_9R9usFgg.png" width="80" height="80" role="presentation"></div><img class="dq dr s t u ds ai ed" width="80" height="80" srcset="" sizes="80px" role="presentation"><noscript><img class="s t u ds ai" src="https://miro.medium.com/max/160/1*necEBipfgD-Z3_9R9usFgg.png" width="80" height="80" role="presentation"/></noscript></div></div></div></figure></div></div></section></div></article><div class="dq di wi wj ai wq wo wr" data-test-id="post-sidebar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="ws n de"><div class="di"><div class="wt wu r"><a href="https://ayearofai.com/?source=post_sidebar--------------------------post_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><h2 class="aq kt pg as em">A Year of Artificial Intelligence</h2></a><div class="wv ww r"><h4 class="aq cj ei as br wx gh gi wy gk av">Our ongoing effort to make the mathematics, science…</h4></div><div class="hf" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_sidebar--------------------------follow_sidebar-" class="wz gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Follow</a></div></div><div class="xc xd xe n"><div class="n o"><div class="xf r dw"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_sidebar-----10300100899b---------------------clap_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><div class="be xg xh xi xj xk xl xm xn xo xp"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="xq r"><div class="xr"><h4 class="aq cj ei as av"><button class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn">2.1K </button></h4></div></div></div></div><div class="xd r"></div><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_sidebar--------------------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div><div class="dq di wi wj wk wl wm wn wo wp"></div><div><div class="xs do n de p"><div class="n p"><div class="z ab ac ae af ej ah ai"><div class="n xt"></div><div class="n o xt"></div><div class="xu r"><ul class="be bf"><li class="hf bx hs xv"><a href="https://ayearofai.com/tagged/machine-learning" class="xw xx hg av r up xy a b ck">Machine Learning</a></li><li class="hf bx hs xv"><a href="https://ayearofai.com/tagged/artificial-intelligence" class="xw xx hg av r up xy a b ck">Artificial Intelligence</a></li><li class="hf bx hs xv"><a href="https://ayearofai.com/tagged/data-science" class="xw xx hg av r up xy a b ck">Data Science</a></li><li class="hf bx hs xv"><a href="https://ayearofai.com/tagged/deep-learning" class="xw xx hg av r up xy a b ck">Deep Learning</a></li><li class="hf bx hs xv"><a href="https://ayearofai.com/tagged/algorithms" class="xw xx hg av r up xy a b ck">Algorithms</a></li></ul></div><div class="xz n fw y"><div class="n ya"><div class="yb r"><div class="n o"><div class="yc r dw"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_actions_footer-----10300100899b---------------------clap_footer-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><div class="c yd ga n o ye dw yf yg yh yi yj yk yl ym yn yo yp yq yr ys"><div class="be xg xh xi xj xk yt xm o ed ga n p yu u ds s t ai xn xo xp yv"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></div></a></div><div class="xq r"><div class="xr"><h4 class="aq cj ei as em"><button class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn">2.1K claps</button></h4></div></div></div></div><div class="r yw yx yy yz za"></div></div><div class="n o"><div class="hp r an"><a href="https://medium.com/p/10300100899b/share/twitter?source=post_actions_footer---------------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hp r an"><button class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hp r an"><a href="https://medium.com/p/10300100899b/share/facebook?source=post_actions_footer---------------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="zb r an"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_actions_footer--------------------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div><div class="zc zd ze xu r y"><div class="zf zg r dw"><span class="r zh al zi"><div class="r s zj zk"><a href="https://ayearofai.com/@mckapur?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Rohan Kapur" class="r ga ce zl" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/2_sT3MBbbeiRa900wKofa4_w(1).png" width="80" height="80"></a></div><span class="r"><div class="zm r zn"><p class="aq cj ck as av cm zo">Written by</p></div><div class="zm zp n zn"><div class="ai n o fw"><h2 class="aq kt hv zq em"><a href="https://ayearofai.com/@mckapur?source=follow_footer--------------------------follow_footer-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener">Rohan Kapur</a></h2><div class="r g"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=follow_footer-cb55958ea3bb-------------------------follow_footer-" class="wz gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Follow</a></div></div></div></span></span><div class="zm zr r zn bp"><div class="ql r"><h4 class="aq cj pg zs av">mckapur.com</h4></div><div class="zt zu bp"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=follow_footer-cb55958ea3bb-------------------------follow_footer-" class="wz gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Follow</a></div></div></div><div class="zc r"></div><div class="zf zg r dw"><span class="r zh al zi"><div class="r s zj zk"><a href="https://ayearofai.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="A Year of Artificial Intelligence" class="hb zl ce" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_NZsNSuNxe_O2YW1ybboOvA(1).jpeg" width="80" height="80"></a></div><span class="r"><div class="zm zp n zn"><div class="ai n o fw"><h2 class="aq kt hv zq em"><a href="https://ayearofai.com/?source=follow_footer--------------------------follow_footer-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener">A Year of Artificial Intelligence</a></h2><div class="r g"><div class="hf" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=follow_footer--------------------------follow_footer-" class="wz gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Follow</a></div></div></div></div></span></span><div class="zm zv r zn bp"><div class="ql r"><h4 class="aq cj pg zs av">Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.</h4></div><div class="zt zu bp"><div class="hf" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=follow_footer--------------------------follow_footer-" class="wz gq ax ay xa bh bi xb bg hb aq b ar as at au hd he dd hf hg bj" rel="noopener">Follow</a></div></div></div></div></div><div class="zw zd r y"><a href="https://medium.com/p/10300100899b/responses/show?source=follow_footer--------------------------follow_footer-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><span class="ih zx xj"><div class="uo zy hb r ef bp"><span class="ax">See responses (23)</span></div></span></a></div></div></div><div class="zz r aba y"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="abb r abc"><div class="abd wu zf r"><h2 class="aq kt abe abf em">More From Medium</h2></div><div class="cb n ya xt abg abh abi abj abk abl abm abn abo abp abq abr abs abt abu"><div class="abv abw abx ud aby abz aca uf acb acc acd uh ace acf acg ach aci acj ack acl acm"><div class="ai ds"><div class="r acn"><div class="aco acp abg abh abi acq acr abj abk abl acs act abm abn abo acu acv abp abq abr acw acx abs abt abu n xt"><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="aeg r aeh f"><h4 class="aq cj ei as av">Related reads</h4></div><div class="kk r aei abc"><a href="https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c?source=post_recirc---------0------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn r" rel="noopener"></a></div></div><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="kk r"><div class="aej zt h aek"><h4 class="aq cj ei as av">Related reads</h4></div><a href="https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c?source=post_recirc---------0------------------" rel="noopener"><h3 class="em q en ael ar ii aem aen">All you need to know about RNNs</h3></a></div><div class="n o fw"><div class="aeo r bw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r em q"><div class="cu n o gf"><span class="aq cj ei as br gg gh gi gj gk em"><a href="https://ayearofai.com/@sulekahelmini9628?source=post_recirc---------0------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Suleka Helmini</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_recirc---------0------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq cj ei as br gg gh gi gj gk av"><div><a href="https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c?source=post_recirc---------0------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Mar 6, 2019</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div></div><div class="n o"><div class="n o"><div class="xf r dw"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc-----e514f0b00c7c----0-----------------clap_preview-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><div class="be xg xh xi xj xk xl xm xn xo xp"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="xq r"><div class="xr"><h4 class="aq cj ei as av">234</h4></div></div></div><div class="aep gd aeo cf aeq r"></div><div><div class="hf" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc---------0-----------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></div><div class="abv abw abx ud aby abz aca uf acb acc acd uh ace acf acg ach aci acj ack acl acm"><div class="ai ds"><div class="r acn"><div class="aco acp abg abh abi acq acr abj abk abl acs act abm abn abo acu acv abp abq abr acw acx abs abt abu n xt"><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="aeg r aeh f"><h4 class="aq cj ei as av">Related reads</h4></div><div class="kk r aei abc"><a href="https://medium.com/x8-the-ai-community/building-a-recurrent-neural-network-from-scratch-f7e94251cc80?source=post_recirc---------1------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn r" rel="noopener"></a></div></div><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="kk r"><div class="aej zt h aek"><h4 class="aq cj ei as av">Related reads</h4></div><a href="https://medium.com/x8-the-ai-community/building-a-recurrent-neural-network-from-scratch-f7e94251cc80?source=post_recirc---------1------------------" rel="noopener"><h3 class="em q en ael ar ii aem aen">Building a Recurrent Neural Network from Scratch</h3></a></div><div class="n o fw"><div class="aeo r bw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r em q"><div class="cu n o gf"><span class="aq cj ei as br gg gh gi gj gk em"><a href="https://ayearofai.com/@prateekkarkare?source=post_recirc---------1------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Prateek Karkare</a><span> <!-- -->in<!-- --> <a class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener" href="https://ayearofai.com/x8-the-ai-community?source=post_recirc---------1------------------">AI Graduate</a></span></span></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq cj ei as br gg gh gi gj gk av"><div><a href="https://medium.com/x8-the-ai-community/building-a-recurrent-neural-network-from-scratch-f7e94251cc80?source=post_recirc---------1------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Oct 5, 2019</a> <!-- -->·<!-- --> <!-- -->6<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="xf r dw"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc-----f7e94251cc80----1-----------------clap_preview-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><div class="be xg xh xi xj xk xl xm xn xo xp"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="xq r"><div class="xr"><h4 class="aq cj ei as av">125</h4></div></div></div><div class="aep gd aeo cf aeq r"></div><div><div class="hf" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc---------1-----------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></div><div class="abv abw abx ud aby abz aca uf acb acc acd uh ace acf acg ach aci acj ack acl acm"><div class="ai ds"><div class="r acn"><div class="aco acp abg abh abi acq acr abj abk abl acs act abm abn abo acu acv abp abq abr acw acx abs abt abu n xt"><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="aeg r aeh f"><h4 class="aq cj ei as av">Related reads</h4></div><div class="kk r aei abc"><a href="https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85?source=post_recirc---------2------------------" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn r" rel="noopener"></a></div></div><div class="abv abw abx ud aby abz acy acz acb acc aea aeb ace acf aec aed aci acj aee aef acm"><div class="kk r"><div class="aej zt h aek"><h4 class="aq cj ei as av">Related reads</h4></div><a href="https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85?source=post_recirc---------2------------------" rel="noopener"><h3 class="em q en ael ar ii aem aen">Recurrent Neural Networks (RNNs)</h3></a></div><div class="n o fw"><div class="aeo r bw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r em q"><div class="cu n o gf"><span class="aq cj ei as br gg gh gi gj gk em"><a href="https://ayearofai.com/@javaid.nabi?source=post_recirc---------2------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Javaid Nabi</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq cj ei as br gg gh gi gj gk av"><div><a href="https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85?source=post_recirc---------2------------------" class="co cp az ba bb bc bd be bf bg gl bj bk gm gn" rel="noopener">Jul 11, 2019</a> <!-- -->·<!-- --> <!-- -->11<!-- --> min read</div></span></span></div></div></div><div class="n o"><div class="n o"><div class="xf r dw"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc-----3f06d7653a85----2-----------------clap_preview-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><div class="be xg xh xi xj xk xl xm xn xo xp"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="xq r"><div class="xr"><h4 class="aq cj ei as av">180</h4></div></div></div><div class="aep gd aeo cf aeq r"></div><div><div class="hf" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-3-recurrent-neural-networks-10300100899b&amp;source=post_recirc---------2-----------------bookmark_sidebar-" class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="aer r aes aet"><section class="da db ai dd r aeu aev aew aex aey aez afa afb afc afd afe aff afg afh afi"><div class="afj afk zf n fw g"><div class="afl n fw"><div class="afm r afn"><div class="afo r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><h4 class="aft afu afv aq kt ar zs afw afx r">Discover <!-- -->Medium</h4></a></div><span class="aq b ar as at au r afy afz">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg bj bk afr afs aga" rel="noopener">Watch</a></span></div><div class="afm r afn"><div class="abd r"><a href="https://medium.com/topics?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><h4 class="aft afu afv aq kt ar zs afw afx r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="aq b ar as at au r afy afz">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg bj bk afr afs aga" rel="noopener">Explore</a></span></div><div class="afm r afn"><div class="afo r"><a href="https://medium.com/membership?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><h4 class="aft afu afv aq kt ar zs afw afx r">Become a member</h4></a></div><span class="aq b ar as at au r afy afz">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg bj bk afr afs aga" rel="noopener">Upgrade</a></span></div></div></div><div class="n de"><div class="n o fw"><a href="https://medium.com/?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="afu"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="aq b ar as at au r afy afz"><div class="agb agc n fw agd al"><h4 class="aq cj pg zs aft"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg gl bj bk afr afs" rel="noopener">About</a></h4><h4 class="aq cj pg zs aft"><a href="https://help.medium.com/?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg gl bj bk afr afs" rel="noopener">Help</a></h4><h4 class="aq cj pg zs aft"><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg gl bj bk afr afs" rel="noopener">Legal</a></h4></div></span></div><div class="zt age agf al"><h4 class="aq cj pg zs afy">Get the Medium app</h4></div><div class="zt age agg al agh"><div class="yc r"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><img alt="A button that says &#39;Download on the App Store&#39;, and if clicked it will lead you to the iOS App store" class="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_M2FVPPidy2x386MRAE-EeA.png" width="135" height="41"></a></div><div class="r"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----10300100899b----------------------" class="co cp az ba bb bc bd be bf bg afp afq bj bk afr afs" rel="noopener"><img alt="A button that says &#39;Get it on, Google Play&#39;, and if clicked it will lead you to the Google Play store" class="" src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/1_HyH8oIcJvXp7xzu5oF6dTg.png" width="135" height="41"></a></div></div></div></section></div></div></div><script>window.__BUILD_ID__ = "master-20200505-002528-1b98a29fef"</script><script>window.__GRAPHQL_URI__ = "https://ayearofai.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200505-002528-1b98a29fef","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200505-002528-1b98a29fef"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"master-20200505-002528-1b98a29fef","commit":"1b98a29fefacb71bf14b1325a274d3b99c02887e"}},"datacenter":"us"},"sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["00","01"],"webpMiroImageIds":["1*y7gegIZOYlsnhWFJwIyDJw","1*ByGRQD1zlYXGS4YBYAoLVA","1*orNowUCqCER-BwaAXOZx0A","1*itOsotYFripvvRKY1itrVQ","1*b_LB1ifqWQ2x3JG_m3MJsg","1*AD7jcqVRun0Hhwmg0-Vqfg","1*0kJoJveqoxkYXEmdM2FZ3A","1*pTq8R2lALVUytg_k4y5CpQ","1*1_tSnwIHb_oPsU9vucJijQ","1*EBN0PWXjvaF2gRdk9fCvzA","1*Uxc2_wlnoVQNMQUhQaLVZg","1*ABOw4ARUQ90kwKfXqeVdXA","1*Ok2A1h7LmAtYjWVG9c8IMA","1*Kw1AUMFyy3AGJ1BbTdeyWg","1*2RZldaiJQXadc5zjscYncg","1*hPIJUpxe2QMvOMNcTlnOlQ","1*MZyvxFpPUvfBUfoNvFhRzg","1*2ROzqt2hXcYs6BuKjG2_nQ","1*NO9eMccT-vPrY8nylJ4PVw","2*6cf2Ep3P-r1vCrc-6Bc-vA","1*hVxgUA6kP-PgL5TJjuyePg","1*VGtACZSU6AxT3ugiNr-WGg","0*8dBf1Vy9mkDdcuwQ","1*5ciI2lDFX8sJanIJa6ppnA","1*Hqtfw2Juvf6Zb9uGimLLMg","1*suPSqLiNrJPCUbtdUwLnGw","1*dIANAeHtMxPlVO9awEN0Jw","1*N2KcM3GCLymsKxnSBXyG_g","1*fgNVzsUlPl9tA8ladAT-KA","1*h-La0GVOrPo6SrFpNWQLtw","1*3IPJZVYg-95RkV8H4DRjvA","1*DgnF3PmTVG14Oz_-8BWX_g","1*x_SKDZtUCcWMH9FB092srw","1*oQ4U4pCo7OUPMXXphwqzTg","1*fjC3jxxcmOwXLwqxraNHfw","1*DXy0NEVftDaLKDVG8dS3YQ","1*KbxEajPgdT9GhcWWGG8JmQ","1*e6oTrX0jQU0lPM_0Tt-oYw","1*oQ4U4pCo7OUPMXXphwqzTg"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"covidCollectionId":"8a9336e5bb4","sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":["pandemic","epidemic","coronavirus","covid19","co-vid-19","containment","self-care","flatten-the-curve","public-health","virus","public-health-crisis","quarantine","self-quarantine","zika","corona","disease-prevention","wuhan","chinavirus","outbreak","influenza","socialdistancing","social-distance","flu","vaccines","healthcare","medicine","conspiracy-theories","conspiracy","virality","epidemia","pandemia","salud","corona-e-virus","coronavirus-covid19","covid-19","covid-19-symptoms","covid-19-crisis","covid-19-testing","covid-19-treatment","coronavirus-update","coronavirus-diaries"],"COVID_APPLICABLE_TOPIC_NAMES":["coronavirus"],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":["coronavirus","health"],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4"},"debug":{"requestId":"4e3d0a2e-94e9-4393-8c68-63ab426807da","originalSpanCarrier":{"ot-tracer-spanid":"652b098d6d028b3e","ot-tracer-traceid":"72e181c26a22782a","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_cUNm3HkI16Ig"},"xsrf":"","isSpoofed":false},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-3-recurrent-neural-networks-10300100899b","host":"ayearofai.com","hostname":"ayearofai.com","referrer":"https:\u002F\u002Freview.udacity.com\u002F","susiModal":{"step":null,"operation":"register"},"postRead":false},"client":{"isBot":false,"isCustomDomain":true,"isEu":true,"isNativeMedium":false,"inAppBrowserName":"","supportsWebp":true},"multiVote":{"clapsPerPost":{}},"tracing":{}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"bane_add_user","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.10":{"name":"coronavirus_topic_recirc","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"covid_19_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_android_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_apple_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_cdc_banner_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_cleansweep_cachev2_reads","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_cleansweep_double_writes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_confirm_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_digest_feature_logging","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_digest_tagline","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_ev_mission_email_v3","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_filter_expire_processor","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_footer_app_buttons","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_free_corona_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_global_susi_modal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_highlander_member_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.52":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_li_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_lite_about_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_lo_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_lo_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_meter_payment_link","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_minimal_meter_v2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_more_on_coronavirus","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_mute","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"enable_post_page_nav_stickiness_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"enable_sepia_to_olsen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"enable_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"enable_starspace_digest_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.100":{"name":"enable_starspace_ranker_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.103":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.104":{"name":"enable_topic_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.105":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.105.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.105.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.106":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.106.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.106.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.107":{"name":"enable_utc_fix_on_partner_program_dashboard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.107.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.107.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.108":{"name":"exclude_curated_in_popular_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.108.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.108.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.109":{"name":"featured_fc_and_ydr","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.109.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.109.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.110":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.110.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.110.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.111":{"name":"glyph_embed_commands","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.111.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.111.valueType":{"__typename":"VariantFlagString","value":"none"},"ROOT_QUERY.variantFlags.112":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.112.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.112.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.113":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.113.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.113.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.114":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.114.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.114.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.115":{"name":"limit_post_referrers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.115.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.115.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.116":{"name":"make_nav_sticky","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.116.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.116.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.117":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.117.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.117.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.118":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.118.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.118.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.119":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.119.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.119.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.120":{"name":"remove_post_post_similarity","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.120.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.120.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.121":{"name":"rex_refactor_2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.121.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.121.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.122":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.122.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.122.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.123":{"name":"sign_up_with_email_button","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.123.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.123.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.124":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.124.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.124.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.125":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.125.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.125.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.126":{"name":"skip_sign_in_recaptcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.126.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.126.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.127":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.127.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.127.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.128":{"name":"xgboost_auto_suspend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.128.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.128.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.105","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.106","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.107","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.108","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.109","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.110","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.111","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.112","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.113","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.114","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.115","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.116","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.117","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.118","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.119","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.120","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.121","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.122","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.123","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.124","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.125","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.126","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.127","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.128","typename":"VariantFlag"}],"viewer":null,"meterPost({\"postId\":\"10300100899b\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"10300100899b\"})":{"type":"id","generated":false,"id":"Post:10300100899b","typename":"Post"}},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:10300100899b":{"__typename":"Post","id":"10300100899b","creator":{"type":"id","generated":false,"id":"User:cb55958ea3bb","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","collection":{"type":"id","generated":false,"id":"Collection:bb87da25612c","typename":"Collection"},"sequence":null,"mediumUrl":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-3-recurrent-neural-networks-10300100899b","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1492072051153,"isPublished":true,"layerCake":0,"primaryTopic":null,"title":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","latestPublishedVersion":"5c5cdcc6bd59","visibility":"PUBLIC","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":84.19811320754717,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":2194,"viewerClapCount":0,"voterCount":444,"recommenders":[],"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:data-science","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:algorithms","typename":"Tag"}],"topics":[],"recirc({\"paging\":{\"limit\":3}})":{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}})","typename":"RecircItemConnection"},"postResponses":{"type":"id","generated":true,"id":"$Post:10300100899b.postResponses","typename":"PostResponses"},"responsesCount":23,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1493213116686,"previewContent":{"type":"id","generated":true,"id":"$Post:10300100899b.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*khIKl9t4XmZGSsKhW_Yg2w.png","typename":"ImageMetadata"},"isShortform":false,"seoTitle":"","updatedAt":1529432784937,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false},"User:cb55958ea3bb":{"id":"cb55958ea3bb","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Rohan Kapur","isFollowing":false,"username":"mckapur","bio":"mckapur.com","imageId":"2*sT3MBbbeiRa900wKofa4_w.png","mediumMemberAt":0,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"mckapur"},"Collection:bb87da25612c":{"id":"bb87da25612c","__typename":"Collection","isAuroraVisible":false,"domain":"ayearofai.com","googleAnalyticsId":null,"slug":"a-year-of-artificial-intelligence","customStyleSheet":null,"colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","favicon":{"type":"id","generated":false,"id":"ImageMetadata:","typename":"ImageMetadata"},"name":"A Year of Artificial Intelligence","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*NZsNSuNxe_O2YW1ybboOvA.jpeg","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*NZsNSuNxe_O2YW1ybboOvA.jpeg","typename":"ImageMetadata"},"isAuroraEligible":false,"isEnrolledInHightower":false,"isNewsletterV3Enabled":false,"newsletterV3":null,"creator":{"type":"id","generated":false,"id":"User:cb55958ea3bb","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:bb87da25612c.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:bb87da25612c.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:bb87da25612c.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:bb87da25612c.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:bb87da25612c.navItems.4","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"viewerIsMuting":false,"description":"Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.","tintColor":"#FF000000","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"mckapur","facebookPageId":null,"tagline":"Our ongoing effort to make the mathematics, science…"},"ImageMetadata:":{"id":"","__typename":"ImageMetadata"},"ImageMetadata:1*NZsNSuNxe_O2YW1ybboOvA.jpeg":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"__typename":"ImageMetadata"},"Collection:bb87da25612c.navItems.0":{"title":"Algorithms","url":"https:\u002F\u002Fayearofai.com\u002Ftagged\u002Falgorithms","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:bb87da25612c.navItems.1":{"title":"Today I Learned","url":"https:\u002F\u002Fayearofai.com\u002Ftagged\u002Ftoday-i-learned","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:bb87da25612c.navItems.2":{"title":"Case Studies","url":"https:\u002F\u002Fayearofai.com\u002Ftagged\u002Fcase-studies","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:bb87da25612c.navItems.3":{"title":"Philosophical","url":"https:\u002F\u002Fayearofai.com\u002Ftagged\u002Fphilosophical","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:bb87da25612c.navItems.4":{"title":"Meta","url":"https:\u002F\u002Fayearofai.com\u002Ftagged\u002Fmeta","type":"TAG_NAV_ITEM","__typename":"NavItem"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF000000","colorPoints":[{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF000000","point":0,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF1E1D1D","point":0.1,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF3C3B3B","point":0.2,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF565555","point":0.3,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF6F6D6D","point":0.4,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF868484","point":0.5,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FF9C9A99","point":0.6,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFB1AEAE","point":0.7,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFC5C3C2","point":0.8,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFD9D6D6","point":0.9,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFECE9E9","point":1,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum","typename":"ColorSpectrum"},"defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"}},"$Collection:bb87da25612c.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFF5F2F1","point":0,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFF3F0EF","point":0.1,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFF1EEED","point":0.2,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFEFECEC","point":0.3,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFEDEAEA","point":0.4,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFEBE8E8","point":0.5,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFE9E6E6","point":0.6,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFE7E5E4","point":0.7,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFE5E3E2","point":0.8,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFE4E1E0","point":0.9,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFE2DFDE","point":1,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF868484","point":0,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF7C7B7A","point":0.1,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF737171","point":0.2,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF696867","point":0.3,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF5F5E5E","point":0.4,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF555454","point":0.5,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF4A4949","point":0.6,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF3F3E3E","point":0.7,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF343333","point":0.8,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF272727","point":0.9,"__typename":"ColorPoint"},"$Collection:bb87da25612c.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF1A1A1A","point":1,"__typename":"ColorPoint"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel","typename":"RichText"}},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.0":{"name":"20d1","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.1":{"name":"4110","startIndex":3,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.2":{"name":"6590","startIndex":4,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.3":{"name":"da40","startIndex":258,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.4":{"name":"5df4","startIndex":297,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.5":{"name":"ccfc","startIndex":434,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.6":{"name":"70a1","startIndex":447,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.1","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.2","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.3","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.4","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.5","typename":"Section"},{"type":"id","generated":true,"id":"$Post:10300100899b.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Freview.udacity.com\u002F\"}}).bodyModel.sections.6","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_97","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_98","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_99","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_100","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_101","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_102","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_103","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_104","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_105","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_106","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_107","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_108","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_109","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_110","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_111","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_112","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_113","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_114","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_115","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_116","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_117","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_118","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_119","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_120","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_121","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_122","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_123","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_124","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_125","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_126","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_127","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_128","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_129","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_130","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_131","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_132","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_133","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_134","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_135","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_136","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_137","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_138","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_139","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_140","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_141","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_142","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_143","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_144","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_145","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_146","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_147","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_148","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_149","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_150","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_151","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_152","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_153","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_154","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_155","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_156","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_157","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_158","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_159","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_160","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_161","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_162","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_163","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_164","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_165","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_166","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_167","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_168","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_169","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_170","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_171","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_172","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_173","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_174","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_175","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_176","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_177","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_178","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_179","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_180","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_181","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_182","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_183","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_184","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_185","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_186","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_187","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_188","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_189","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_190","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_191","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_192","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_193","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_194","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_195","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_196","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_197","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_198","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_199","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_200","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_201","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_202","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_203","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_204","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_205","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_206","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_207","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_208","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_209","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_210","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_211","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_212","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_213","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_214","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_215","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_216","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_217","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_218","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_219","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_220","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_221","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_222","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_223","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_224","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_225","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_226","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_227","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_228","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_229","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_230","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_231","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_232","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_233","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_234","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_235","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_236","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_237","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_238","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_239","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_240","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_241","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_242","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_243","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_244","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_245","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_246","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_247","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_248","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_249","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_250","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_251","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_252","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_253","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_254","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_255","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_256","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_257","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_258","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_259","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_260","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_261","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_262","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_263","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_264","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_265","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_266","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_267","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_268","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_269","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_270","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_271","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_272","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_273","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_274","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_275","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_276","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_277","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_278","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_279","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_280","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_281","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_282","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_283","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_284","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_285","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_286","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_287","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_288","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_289","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_290","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_291","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_292","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_293","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_294","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_295","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_296","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_297","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_298","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_299","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_300","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_301","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_302","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_303","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_304","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_305","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_306","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_307","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_308","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_309","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_310","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_311","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_312","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_313","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_314","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_315","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_316","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_317","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_318","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_319","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_320","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_321","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_322","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_323","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_324","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_325","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_326","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_327","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_328","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_329","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_330","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_331","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_332","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_333","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_334","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_335","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_336","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_337","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_338","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_339","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_340","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_341","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_342","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_343","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_344","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_345","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_346","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_347","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_348","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_349","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_350","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_351","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_352","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_353","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_354","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_355","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_356","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_357","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_358","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_359","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_360","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_361","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_362","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_363","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_364","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_365","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_366","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_367","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_368","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_369","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_370","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_371","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_372","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_373","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_374","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_375","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_376","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_377","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_378","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_379","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_380","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_381","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_382","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_383","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_384","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_385","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_386","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_387","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_388","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_389","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_390","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_391","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_392","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_393","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_394","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_395","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_396","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_397","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_398","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_399","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_400","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_401","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_402","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_403","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_404","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_405","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_406","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_407","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_408","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_409","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_410","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_411","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_412","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_413","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_414","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_415","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_416","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_417","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_418","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_419","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_420","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_421","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_422","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_423","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_424","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_425","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_426","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_427","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_428","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_429","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_430","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_431","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_432","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_433","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_434","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_435","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_436","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_437","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_438","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_439","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_440","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_441","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_442","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_443","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_444","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_445","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_446","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_447","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_448","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_449","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_450","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_451","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_452","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_453","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_454","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_455","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_456","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_457","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_458","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_459","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_460","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_461","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_462","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_463","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_464","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_465","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_466","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_467","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_468","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_469","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_470","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_471","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_472","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_473","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_474","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_475","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_476","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_477","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_478","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_479","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_480","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_481","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_482","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_483","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_484","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_485","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_486","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_487","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_488","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_489","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_490","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_491","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_492","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_493","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_494","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_495","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_496","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_497","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_498","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_499","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_500","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_501","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_502","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_503","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_504","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_505","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_506","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_507","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_508","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_509","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_510","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_511","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_512","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_513","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_514","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_515","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_516","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_517","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_518","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_519","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_520","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_521","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_522","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_523","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_524","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_525","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_526","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_527","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_528","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_529","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_530","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_531","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_532","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_533","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_534","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_535","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_536","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_537","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_538","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_539","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_540","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_541","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_542","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_543","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_544","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_545","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_546","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_547","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_548","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_549","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_550","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_551","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_552","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_553","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_554","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_555","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_556","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_557","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_558","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_559","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_560","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_561","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_562","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_563","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_564","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_565","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_566","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_567","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_568","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_569","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_570","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_571","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_572","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_573","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_574","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_575","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_576","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_577","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_578","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_579","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_580","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_581","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_582","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_583","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_584","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_585","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_586","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_587","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_588","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_589","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_590","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_591","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_592","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_593","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_594","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_595","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_596","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_597","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_598","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_599","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_600","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_601","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_602","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_603","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_604","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_605","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_606","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_607","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_608","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_609","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_610","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_611","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_612","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_613","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_614","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_615","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_616","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_617","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_618","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_619","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_620","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_621","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_622","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_623","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_624","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_625","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_626","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_627","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_628","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_629","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_630","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_631","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_632","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_633","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_634","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_635","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_636","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_637","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_638","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_639","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_640","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_641","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_642","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_643","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_644","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_645","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_646","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_647","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_648","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:5c5cdcc6bd59_649","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:5c5cdcc6bd59_0":{"id":"5c5cdcc6bd59_0","name":"be8e","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*khIKl9t4XmZGSsKhW_Yg2w.png","typename":"ImageMetadata"},"text":"Sequences upon sequences upon sequences. Sequen-ception.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*khIKl9t4XmZGSsKhW_Yg2w.png":{"id":"1*khIKl9t4XmZGSsKhW_Yg2w.png","originalHeight":907,"originalWidth":2000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_1":{"id":"5c5cdcc6bd59_1","name":"84b9","type":"H3","href":null,"layout":null,"metadata":null,"text":"Rohan & Lenny #3: Recurrent Neural Networks & LSTMs","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_2":{"id":"5c5cdcc6bd59_2","name":"9485","type":"H4","href":null,"layout":null,"metadata":null,"text":"The ultimate guide to machine learning’s favorite child.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_3":{"id":"5c5cdcc6bd59_3","name":"4569","type":"PQ","href":null,"layout":null,"metadata":null,"text":"This is the third group (Lenny and Rohan) entry in our journey to extend our knowledge of artificial intelligence and convey that knowledge in a simple, fun, and accessible manner. Learn more about our motives in this introduction post.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_3.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_3.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_3.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_3.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_3.markups.0":{"type":"A","start":25,"end":30,"href":null,"anchorType":"USER","userId":"de8e2540b759","linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_3.markups.1":{"type":"A","start":35,"end":40,"href":null,"anchorType":"USER","userId":"cb55958ea3bb","linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_3.markups.2":{"type":"A","start":55,"end":62,"href":"https:\u002F\u002Fmedium.com\u002Fa-year-of-artificial-intelligence","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_3.markups.3":{"type":"A","start":218,"end":230,"href":"https:\u002F\u002Fmedium.com\u002Fa-year-of-artificial-intelligence\u002F0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_4":{"id":"5c5cdcc6bd59_4","name":"a16a","type":"P","href":null,"layout":null,"metadata":null,"text":"It seems like most of our posts on this blog start with “We’re back!”, so… you know the drill. It’s been a while since our last post — just over 5 months — but it certainly doesn’t feel that way. Whether our articles are more spaced out than we’d like them to be, well, we haven’t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we’ve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time.","hasDropCap":true,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_4.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_4.markups.0":{"type":"A","start":485,"end":491,"href":"http:\u002F\u002Fgetcontra.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_5":{"id":"5c5cdcc6bd59_5","name":"ac01","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg":{"id":"1*N5l-Uj5yUVPIDwIuGUvnfg.jpeg","originalHeight":389,"originalWidth":570,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_6":{"id":"5c5cdcc6bd59_6","name":"93ae","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s 2017. We started A Year Of AI in 2016. Last year. Don’t panic, though. If you’ve read our letter, you’ll know that, despite our name and inception date, we’re not going anywhere anytime soon. There’s a good chance we’ll move off Medium, but we’re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_6.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_6.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_6.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_6.markups.0":{"type":"A","start":95,"end":101,"href":"https:\u002F\u002Fayearofai.com\u002Fthe-goal-of-our-blog-c104d7b6377a#.75o5qyayi","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_6.markups.1":{"type":"STRONG","start":24,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_6.markups.2":{"type":"EM","start":24,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_7":{"id":"5c5cdcc6bd59_7","name":"f6d1","type":"P","href":null,"layout":null,"metadata":null,"text":"I wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I’ll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn 😀.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_7.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_7.markups.0":{"type":"A","start":249,"end":265,"href":"https:\u002F\u002Fsymsys.stanford.edu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_8":{"id":"5c5cdcc6bd59_8","name":"b842","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy’s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there’s space to simplify the topic even more, though. As usual, that’s our aim for the article — to teach you RNNs in a fun, simple manner. We’re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don’t worry, you’ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize\u002Fsimplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_8.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_8.markups.0":{"type":"A","start":88,"end":100,"href":"http:\u002F\u002Fkarpathy.github.io\u002F2015\u002F05\u002F21\u002Frnn-effectiveness\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_9":{"id":"5c5cdcc6bd59_9","name":"60e2","type":"P","href":null,"layout":null,"metadata":null,"text":"Before we get started, you should try to familiarize yourself with “vanilla” neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training\u002Ftest sets, activation functions\u002Fwhat they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_9.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_9.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_9.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_9.markups.0":{"type":"A","start":133,"end":178,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.quwnoqtot","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_9.markups.1":{"type":"A","start":430,"end":459,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_9.markups.2":{"type":"A","start":572,"end":576,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_10":{"id":"5c5cdcc6bd59_10","name":"81ec","type":"P","href":null,"layout":null,"metadata":null,"text":"Rule of thumb: the more you know, the better!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_11":{"id":"5c5cdcc6bd59_11","name":"3288","type":"H3","href":null,"layout":null,"metadata":null,"text":"Table of Contents","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_12":{"id":"5c5cdcc6bd59_12","name":"003f","type":"P","href":null,"layout":null,"metadata":null,"text":"I can’t link to each section, but here’s what we cover in this article (save the intro and conclusion):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_13":{"id":"5c5cdcc6bd59_13","name":"8cfa","type":"OLI","href":null,"layout":null,"metadata":null,"text":"What can RNNs do? Where we look at… what RNNs can do!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_13.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_13.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_14":{"id":"5c5cdcc6bd59_14","name":"7f98","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Why? Where we talk about the gap that RNNs fill in machine learning’s suite of algorithms.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_14.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_14.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_15":{"id":"5c5cdcc6bd59_15","name":"de84","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Show me. Where we visualize RNNs for the first time.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_15.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_15.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_16":{"id":"5c5cdcc6bd59_16","name":"f2f4","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Formalism. Where we walk through how an RNN mathematically works with proper notation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_16.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_16.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_17":{"id":"5c5cdcc6bd59_17","name":"c163","type":"OLI","href":null,"layout":null,"metadata":null,"text":"An example? Okay! Where we walk through, qualitatively, a simple application of RNNs and how the RNN operates in this application, including techniques we can use.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_17.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_17.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_18":{"id":"5c5cdcc6bd59_18","name":"394e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Training (or, why vanilla RNNs suck.) Where we talk about how to train RNNs, and why vanilla RNNs are bad at learning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_18.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_18.markups.0":{"type":"STRONG","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_19":{"id":"5c5cdcc6bd59_19","name":"5756","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Fixing the problem with LSTMs (Part I). Where we introduce the solution to vanilla RNNs’ inability to learn: LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_19.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_19.markups.0":{"type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_20":{"id":"5c5cdcc6bd59_20","name":"654d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Fixing the problem with LSTMs (Part II). Where we analyze on a close, technical level, the reasons LSTMs don’t suffer from vanishing gradients as much (and why they still do, to an extent). Then we conclude LSTMs with final thoughts on and facts about them.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_20.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_20.markups.0":{"type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_21":{"id":"5c5cdcc6bd59_21","name":"2dfc","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Yay RNNs! Where you get to see neat little things RNNs have done!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_21.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_21.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_21.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_21.markups.1":{"type":"EM","start":16,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_22":{"id":"5c5cdcc6bd59_22","name":"e56c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"In Practice. Where we look at more technical and important applications and case studies of RNNs, including other variations of RNNs, especially as relevant in hot\u002Frecent research papers.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_22.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_22.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_23":{"id":"5c5cdcc6bd59_23","name":"7bc0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Building a Vanilla Recurrent Neural Network. Where you get to code your very first RNN! Woohoo!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_23.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_23.markups.0":{"type":"STRONG","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_24":{"id":"5c5cdcc6bd59_24","name":"02a8","type":"H3","href":null,"layout":null,"metadata":null,"text":"What can RNNs do?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_25":{"id":"5c5cdcc6bd59_25","name":"1c1d","type":"P","href":null,"layout":null,"metadata":null,"text":"There are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example… learning to learn by gradient descent by gradient descent!).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_25.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_25.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_25.markups.0":{"type":"A","start":375,"end":432,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.04474.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_25.markups.1":{"type":"EM","start":282,"end":286,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_26":{"id":"5c5cdcc6bd59_26","name":"1d0f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*X5dk-xGw2yNYsEB3QvHWIA.png","typename":"ImageMetadata"},"text":"Image captioning, taken from CS231n slides: http:\u002F\u002Fcs231n.stanford.edu\u002Fslides\u002F2016\u002Fwinter1516_lecture10.pdf","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_26.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*X5dk-xGw2yNYsEB3QvHWIA.png":{"id":"1*X5dk-xGw2yNYsEB3QvHWIA.png","originalHeight":400,"originalWidth":713,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_26.markups.0":{"type":"A","start":44,"end":107,"href":"http:\u002F\u002Fcs231n.stanford.edu\u002Fslides\u002F2016\u002Fwinter1516_lecture10.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_27":{"id":"5c5cdcc6bd59_27","name":"02ef","type":"P","href":null,"layout":null,"metadata":null,"text":"RNNs are very powerful. Y’know how regular neural networks have been proved to be “universal function approximators” ? If you didn’t:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_28":{"id":"5c5cdcc6bd59_28","name":"8206","type":"BQ","href":null,"layout":null,"metadata":null,"text":"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n, under mild assumptions on the activation function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_29":{"id":"5c5cdcc6bd59_29","name":"314e","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it’s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it’s true.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_29.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_29.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_29.markups.0":{"type":"A","start":343,"end":347,"href":"http:\u002F\u002Fneuralnetworksanddeeplearning.com\u002Fchap4.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_29.markups.1":{"type":"EM","start":407,"end":408,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_30":{"id":"5c5cdcc6bd59_30","name":"a351","type":"P","href":null,"layout":null,"metadata":null,"text":"So, that’s great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute\u002Fdescribe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_30.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_30.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_30.markups.0":{"type":"A","start":98,"end":132,"href":"http:\u002F\u002Fstats.stackexchange.com\u002Fa\u002F221142\u002F98975","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_30.markups.1":{"type":"EM","start":124,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_31":{"id":"5c5cdcc6bd59_31","name":"8f10","type":"BQ","href":null,"layout":null,"metadata":null,"text":"A Turing Complete system means a system in which a program can be written that will find an answer (although with no guarantees regarding runtime or memory).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_32":{"id":"5c5cdcc6bd59_32","name":"c6a2","type":"BQ","href":null,"layout":null,"metadata":null,"text":"So, if somebody says “my new thing is Turing Complete” that means in principle (although often not in practice) it could be used to solve any computation problem.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_32.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_32.markups.0":{"type":"STRONG","start":89,"end":110,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_33":{"id":"5c5cdcc6bd59_33","name":"1429","type":"BQ","href":null,"layout":null,"metadata":null,"text":"— http:\u002F\u002Fstackoverflow.com\u002Fa\u002F7320\u002F1260708","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_33.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_33.markups.0":{"type":"A","start":2,"end":41,"href":"http:\u002F\u002Fstackoverflow.com\u002Fa\u002F7320\u002F1260708","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_34":{"id":"5c5cdcc6bd59_34","name":"2ba5","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s cool, isn’t it? Now, this is all theoretical, and in practice means less than you think, so don’t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning — and why you should read on.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_35":{"id":"5c5cdcc6bd59_35","name":"662c","type":"P","href":null,"layout":null,"metadata":null,"text":"At this point, if you weren’t previously hooked on learning what the heck these things are, you should be now. (If you still aren’t, just bare with me. Things will get spicy soon.) So, let’s dive in.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_36":{"id":"5c5cdcc6bd59_36","name":"72f6","type":"H3","href":null,"layout":null,"metadata":null,"text":"Why?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_37":{"id":"5c5cdcc6bd59_37","name":"54dc","type":"P","href":null,"layout":null,"metadata":null,"text":"We took a bit of a detour to talk about how great RNNs are, but haven’t focused on why ANNs can’t perform well in the tasks that RNNs can.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_37.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_37.markups.0":{"type":"EM","start":83,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_38":{"id":"5c5cdcc6bd59_38","name":"b88b","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Why do we need another neural network model? Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_39":{"id":"5c5cdcc6bd59_39","name":"95c9","type":"P","href":null,"layout":null,"metadata":null,"text":"It boils down to a few things:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_40":{"id":"5c5cdcc6bd59_40","name":"3d40","type":"ULI","href":null,"layout":null,"metadata":null,"text":"ANNs can’t deal with sequential or “temporal” data","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_41":{"id":"5c5cdcc6bd59_41","name":"2a22","type":"ULI","href":null,"layout":null,"metadata":null,"text":"ANNs lack memory","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_42":{"id":"5c5cdcc6bd59_42","name":"0d57","type":"ULI","href":null,"layout":null,"metadata":null,"text":"ANNs have a fixed architecture","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_43":{"id":"5c5cdcc6bd59_43","name":"fbb9","type":"ULI","href":null,"layout":null,"metadata":null,"text":"RNNs are more “biologically realistic” because of the recurrent connectivity found in the visual cortex of the brain","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_44":{"id":"5c5cdcc6bd59_44","name":"341a","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and\u002For output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_44.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_44.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_44.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_44.markups.0":{"type":"EM","start":103,"end":120,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_44.markups.1":{"type":"EM","start":126,"end":143,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_44.markups.2":{"type":"EM","start":333,"end":341,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_45":{"id":"5c5cdcc6bd59_45","name":"605a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*BQ0SxdqC9Pl_3ZQtd3e45A.png","typename":"ImageMetadata"},"text":"We might choose this architecture for our ANN, with 4 inputs and 1 output. But that’s it — we can’t input a vector with 5 values, for example. https:\u002F\u002Fqph.ec.quoracdn.net\u002Fmain-qimg-050d12c5da82f4f97fdd942d7777b8e4.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_45.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BQ0SxdqC9Pl_3ZQtd3e45A.png":{"id":"1*BQ0SxdqC9Pl_3ZQtd3e45A.png","originalHeight":309,"originalWidth":500,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_45.markups.0":{"type":"A","start":143,"end":213,"href":"https:\u002F\u002Fqph.ec.quoracdn.net\u002Fmain-qimg-050d12c5da82f4f97fdd942d7777b8e4","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_46":{"id":"5c5cdcc6bd59_46","name":"1c8e","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ll give you a couple examples of why this matters.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_47":{"id":"5c5cdcc6bd59_47","name":"e228","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence — a list of words in a specific order — which is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word\u002Flabel, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn’t sound like a good idea.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_47.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_47.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_47.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_47.markups.0":{"type":"EM","start":190,"end":199,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_47.markups.1":{"type":"EM","start":200,"end":201,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_47.markups.2":{"type":"EM","start":436,"end":443,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_48":{"id":"5c5cdcc6bd59_48","name":"8f28","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GFVoFpD6cdCY_PGqnjhOlQ.png","typename":"ImageMetadata"},"text":"A reminder of what the output of an ANN looks like — a probability distribution over classes — and how we convert that into a single final result (one-hot encoding): by taking the label with the greatest probability and making it 1, with the rest 0.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GFVoFpD6cdCY_PGqnjhOlQ.png":{"id":"1*GFVoFpD6cdCY_PGqnjhOlQ.png","originalHeight":182,"originalWidth":305,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_49":{"id":"5c5cdcc6bd59_49","name":"f8ec","type":"P","href":null,"layout":null,"metadata":null,"text":"Wow, that was a lot of words. Nevertheless, I hope it’s clear that, with ANNs, there’s no feasible way to output a sequence.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_50":{"id":"5c5cdcc6bd59_50","name":"f308","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, what about inputting a sequence into an ANN? In other words, “temporal” data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it’s just one neuron that’s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn’t we input each “set of values” separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_50.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_50.markups.0":{"type":"EM","start":16,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_51":{"id":"5c5cdcc6bd59_51","name":"8029","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s take the case of this utterly false, and most certainly negative sentence, to evaluate:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_52":{"id":"5c5cdcc6bd59_52","name":"6e25","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*yq_zmka1ssikrmD9GkWmnw.png","typename":"ImageMetadata"},"text":"This is just an alternative fact, believe me! Lenny is actually a great coder. The best I know of. The best.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_52.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_52.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*yq_zmka1ssikrmD9GkWmnw.png":{"id":"1*yq_zmka1ssikrmD9GkWmnw.png","originalHeight":139,"originalWidth":625,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_52.markups.0":{"type":"STRONG","start":71,"end":72,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_52.markups.1":{"type":"EM","start":66,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_53":{"id":"5c5cdcc6bd59_53","name":"322d","type":"P","href":null,"layout":null,"metadata":null,"text":"We’d input “Lenny” first, then “Khazan”, then “is”, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We’d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_53.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_53.markups.0":{"type":"EM","start":179,"end":184,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_54":{"id":"5c5cdcc6bd59_54","name":"13ff","type":"P","href":null,"layout":null,"metadata":null,"text":"Think of it this way — this means you’re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren’t linked in any way; they’re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it’s a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_54.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_54.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_54.markups.0":{"type":"EM","start":436,"end":443,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_54.markups.1":{"type":"EM","start":606,"end":612,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_55":{"id":"5c5cdcc6bd59_55","name":"e71f","type":"P","href":null,"layout":null,"metadata":null,"text":"I like this quote from another article on RNNs:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_56":{"id":"5c5cdcc6bd59_56","name":"9cd3","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_57":{"id":"5c5cdcc6bd59_57","name":"b760","type":"BQ","href":null,"layout":null,"metadata":null,"text":"— http:\u002F\u002Fcolah.github.io\u002Fposts\u002F2015–08-Understanding-LSTMs\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_57.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_57.markups.0":{"type":"A","start":2,"end":59,"href":"http:\u002F\u002Fcolah.github.io\u002Fposts\u002F2015-08-Understanding-LSTMs\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_58":{"id":"5c5cdcc6bd59_58","name":"e0e4","type":"P","href":null,"layout":null,"metadata":null,"text":"(Furthermore, take the case where we had sequential data in both the input and the output. Translating one language to another is a good example of this. Clearly, ANNs aren’t the answer.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_58.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_58.markups.0":{"type":"EM","start":60,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_59":{"id":"5c5cdcc6bd59_59","name":"fa2f","type":"P","href":null,"layout":null,"metadata":null,"text":"RNNs don’t just need memory; they need long term memory. Let’s take the example of predictive typing. Let’s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_59.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_59.markups.0":{"type":"EM","start":39,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_60":{"id":"5c5cdcc6bd59_60","name":"2c67","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TugitPvwm_IZqAdAPR-7UA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TugitPvwm_IZqAdAPR-7UA.png":{"id":"1*TugitPvwm_IZqAdAPR-7UA.png","originalHeight":90,"originalWidth":543,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_61":{"id":"5c5cdcc6bd59_61","name":"3077","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*7l0aNgpXDXZnY-P9C8K4IA.jpeg","typename":"ImageMetadata"},"text":"The face of a criminal?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*7l0aNgpXDXZnY-P9C8K4IA.jpeg":{"id":"1*7l0aNgpXDXZnY-P9C8K4IA.jpeg","originalHeight":250,"originalWidth":250,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_62":{"id":"5c5cdcc6bd59_62","name":"325c","type":"P","href":null,"layout":null,"metadata":null,"text":"Here, if the RNN wasn’t able to look back much (ie. before “should”), then many different options could arise:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_63":{"id":"5c5cdcc6bd59_63","name":"da28","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*aMJu60wscb9m4A-Zn_xyqQ.png","typename":"ImageMetadata"},"text":"Lenny in the military? Make it into a TV show! I’d watch it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*aMJu60wscb9m4A-Zn_xyqQ.png":{"id":"1*aMJu60wscb9m4A-Zn_xyqQ.png","originalHeight":266,"originalWidth":552,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_64":{"id":"5c5cdcc6bd59_64","name":"930e","type":"P","href":null,"layout":null,"metadata":null,"text":"The word “sent” would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word “criminal”, then it would be much more confident that:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_65":{"id":"5c5cdcc6bd59_65","name":"0990","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4CZskdiGqIx29BQylqnYuA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4CZskdiGqIx29BQylqnYuA.png":{"id":"1*4CZskdiGqIx29BQylqnYuA.png","originalHeight":102,"originalWidth":557,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_66":{"id":"5c5cdcc6bd59_66","name":"c9f1","type":"P","href":null,"layout":null,"metadata":null,"text":"The probability of outputting “jail” drastically increases when it sees the word “criminal” is present. That’s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to “forget” or “retain” (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_67":{"id":"5c5cdcc6bd59_67","name":"eb76","type":"P","href":null,"layout":null,"metadata":null,"text":"As it turns out, RNNs — especially deep ones — are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That’s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_68":{"id":"5c5cdcc6bd59_68","name":"c00c","type":"P","href":null,"layout":null,"metadata":null,"text":"To address the third point, one more constraint with ANNs is that they have a fixed number of computation\u002Fprocessing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_68.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_68.markups.0":{"type":"STRONG","start":408,"end":423,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_69":{"id":"5c5cdcc6bd59_69","name":"4abf","type":"H3","href":null,"layout":null,"metadata":null,"text":"Show me.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_70":{"id":"5c5cdcc6bd59_70","name":"0160","type":"P","href":null,"layout":null,"metadata":null,"text":"OK, that’s enough teasing. Three sections into the article, and you’re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_71":{"id":"5c5cdcc6bd59_71","name":"d391","type":"P","href":null,"layout":null,"metadata":null,"text":"The first thing I’m going to do is show you what a normal ANN diagram looks like:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_72":{"id":"5c5cdcc6bd59_72","name":"ce30","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GapzcZDrwnVbflhlRoWZ9g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GapzcZDrwnVbflhlRoWZ9g.png":{"id":"1*GapzcZDrwnVbflhlRoWZ9g.png","originalHeight":220,"originalWidth":310,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_73":{"id":"5c5cdcc6bd59_73","name":"deda","type":"P","href":null,"layout":null,"metadata":null,"text":"Each neuron stores a single scalar value. Thus, each layer can be considered a vector.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_74":{"id":"5c5cdcc6bd59_74","name":"f077","type":"P","href":null,"layout":null,"metadata":null,"text":"Now I’m going to show you what this ANN looks like in our RNN visual notation:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_75":{"id":"5c5cdcc6bd59_75","name":"c5e9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ntKLnv52DCUnkseNcm91iQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ntKLnv52DCUnkseNcm91iQ.png":{"id":"1*ntKLnv52DCUnkseNcm91iQ.png","originalHeight":65,"originalWidth":300,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_76":{"id":"5c5cdcc6bd59_76","name":"f1fc","type":"P","href":null,"layout":null,"metadata":null,"text":"The two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That’s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term “cell” is also used, and is interchangeable with neuron. (I’ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron’s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_76.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_76.markups.0":{"type":"STRONG","start":211,"end":218,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_77":{"id":"5c5cdcc6bd59_77","name":"abee","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s flip it the other way:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_78":{"id":"5c5cdcc6bd59_78","name":"535f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*HewvhrMCcdy-oQcpeeNJ9w.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*HewvhrMCcdy-oQcpeeNJ9w.png":{"id":"1*HewvhrMCcdy-oQcpeeNJ9w.png","originalHeight":231,"originalWidth":50,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_79":{"id":"5c5cdcc6bd59_79","name":"8a15","type":"P","href":null,"layout":null,"metadata":null,"text":"This is in fact a type of recurrent neural network — a one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_79.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_79.markups.0":{"type":"STRONG","start":55,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_80":{"id":"5c5cdcc6bd59_80","name":"b960","type":"P","href":null,"layout":null,"metadata":null,"text":"We can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning — the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_80.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_80.markups.0":{"type":"EM","start":21,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_81":{"id":"5c5cdcc6bd59_81","name":"7fff","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-Jv3TxauJBwBgWwjoe_UkA.png","typename":"ImageMetadata"},"text":"Changed the shades of the green nodes… hope that’s OK!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-Jv3TxauJBwBgWwjoe_UkA.png":{"id":"1*-Jv3TxauJBwBgWwjoe_UkA.png","originalHeight":230,"originalWidth":190,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_82":{"id":"5c5cdcc6bd59_82","name":"ea3c","type":"P","href":null,"layout":null,"metadata":null,"text":"This may be confusing at first, so I’m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth\u002Flayers:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_83":{"id":"5c5cdcc6bd59_83","name":"c4eb","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OEyIsiEi5SJ9l3GrB5DpuA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OEyIsiEi5SJ9l3GrB5DpuA.png":{"id":"1*OEyIsiEi5SJ9l3GrB5DpuA.png","originalHeight":287,"originalWidth":323,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_84":{"id":"5c5cdcc6bd59_84","name":"22b2","type":"P","href":null,"layout":null,"metadata":null,"text":"When I refer to “time” on the x-axis, I’m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say “depth” on the y-axis, I’m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_85":{"id":"5c5cdcc6bd59_85","name":"f94b","type":"P","href":null,"layout":null,"metadata":null,"text":"It may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple “timesteps” where they take on different values, which are, again, vectors. The input neuron in our example above doesn’t, because it’s not representing sequential data (one to many), but for other architectures it could.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_86":{"id":"5c5cdcc6bd59_86","name":"e200","type":"P","href":null,"layout":null,"metadata":null,"text":"The hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron’s vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_86.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_86.markups.0":{"type":"STRONG","start":48,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.1":{"type":"STRONG","start":64,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.2":{"type":"STRONG","start":81,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.3":{"type":"STRONG","start":131,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.4":{"type":"STRONG","start":188,"end":193,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.5":{"type":"STRONG","start":206,"end":210,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.6":{"type":"STRONG","start":247,"end":250,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.7":{"type":"STRONG","start":322,"end":325,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_86.markups.8":{"type":"EM","start":229,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_87":{"id":"5c5cdcc6bd59_87","name":"64eb","type":"P","href":null,"layout":null,"metadata":null,"text":"As we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron — be it input, hidden, or output — at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_88":{"id":"5c5cdcc6bd59_88","name":"5402","type":"P","href":null,"layout":null,"metadata":null,"text":"The RNN would execute like so:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_89":{"id":"5c5cdcc6bd59_89","name":"3623","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Input x_1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_89.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_89.markups.0":{"type":"STRONG","start":6,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_90":{"id":"5c5cdcc6bd59_90","name":"1923","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute h_1 based on x_1 (the arrow implies functional dependency)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_90.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_90.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_90.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_90.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_91":{"id":"5c5cdcc6bd59_91","name":"00ff","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute h_2 based on h_1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_91.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_91.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_91.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_91.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_92":{"id":"5c5cdcc6bd59_92","name":"ca4c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute h_3 based on h_2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_92.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_92.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_92.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_92.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_93":{"id":"5c5cdcc6bd59_93","name":"9fac","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute y_1 based on h_1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_93.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_93.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_93.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_93.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_94":{"id":"5c5cdcc6bd59_94","name":"649b","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute y_2 based on h_2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_94.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_94.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_94.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_94.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_95":{"id":"5c5cdcc6bd59_95","name":"ba63","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute y_3 based on h_3","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_95.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_95.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_95.markups.0":{"type":"STRONG","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_95.markups.1":{"type":"STRONG","start":21,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_96":{"id":"5c5cdcc6bd59_96","name":"bba5","type":"P","href":null,"layout":null,"metadata":null,"text":"You could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I’m not entirely sure which is more common in practice.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_96.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_96.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_96.markups.0":{"type":"STRONG","start":18,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_96.markups.1":{"type":"STRONG","start":47,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_97":{"id":"5c5cdcc6bd59_97","name":"6c68","type":"P","href":null,"layout":null,"metadata":null,"text":"This allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_98":{"id":"5c5cdcc6bd59_98","name":"e78f","type":"P","href":null,"layout":null,"metadata":null,"text":"The value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It’s actually 2, because the RNN would need to output a period or \u003CEND\u003E marker at the final timestep, but we’ll get into that later.)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_99":{"id":"5c5cdcc6bd59_99","name":"a009","type":"P","href":null,"layout":null,"metadata":null,"text":"In case you don’t understand yet exactly why RNNs work, I’ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_99.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_99.markups.0":{"type":"EM","start":41,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_100":{"id":"5c5cdcc6bd59_100","name":"2856","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iBtLegQFwfsqWZpTVAjrEw.jpeg","typename":"ImageMetadata"},"text":"Lenny and I on student scholarship at WWDC 2013. Good times!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iBtLegQFwfsqWZpTVAjrEw.jpeg":{"id":"1*iBtLegQFwfsqWZpTVAjrEw.jpeg","originalHeight":782,"originalWidth":1298,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_101":{"id":"5c5cdcc6bd59_101","name":"6b74","type":"P","href":null,"layout":null,"metadata":null,"text":"When you combine an RNN and CNN, you — in practice — get an “LCRN”. The architecture for LCRNs are more complex than what I’m going to present in the next paragraph; rather, I’m going to simplify it to convey my point. We’ll actually get fully into how they work later.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_102":{"id":"5c5cdcc6bd59_102","name":"e029","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine an RNN tries to caption this image. An accurate result might be:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_103":{"id":"5c5cdcc6bd59_103","name":"f14e","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Two people happily posing for a photo inside a building.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_104":{"id":"5c5cdcc6bd59_104","name":"482a","type":"P","href":null,"layout":null,"metadata":null,"text":"The input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification\u002Fsoftmax layer — that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state¹ of the recurrent neural network to be one where the most likely candidate word is “two”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_104.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_104.markups.0":{"type":"STRONG","start":431,"end":432,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_105":{"id":"5c5cdcc6bd59_105","name":"f5ec","type":"P","href":null,"layout":null,"metadata":null,"text":"Pro-tip¹: The term “hidden state” refers to the vector of a hidden neuron at a given timestep. “First hidden state” refers to the hidden state at timestep 1.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_105.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_105.markups.0":{"type":"STRONG","start":7,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_106":{"id":"5c5cdcc6bd59_106","name":"10db","type":"P","href":null,"layout":null,"metadata":null,"text":"The first output, which represents the word “two”, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, “two” was ultimately determined from the information that the CNN gave us and the experience\u002Fweights of the RNN. Now, the second word, “people”, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word “people” was the most likely candidate given the hidden state where “two” was likely. In other words, the RNN recognized that, given the word “two”, the word “people” should be next, based on the RNN’s experience from training and the initial image [analysis] we inputted.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_106.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_106.markups.0":{"type":"EM","start":435,"end":441,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_107":{"id":"5c5cdcc6bd59_107","name":"b004","type":"P","href":null,"layout":null,"metadata":null,"text":"The same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_108":{"id":"5c5cdcc6bd59_108","name":"d51d","type":"P","href":null,"layout":null,"metadata":null,"text":"To put it bluntly, you can boil down what the RNN is “thinking” to this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_109":{"id":"5c5cdcc6bd59_109","name":"4a94","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Based on what I’ve seen from the input, based on the current timestep I’m at, and based on what I know from all my training, I need to output: “x”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_109.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_109.markups.0":{"type":"STRONG","start":143,"end":147,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_110":{"id":"5c5cdcc6bd59_110","name":"a918","type":"P","href":null,"layout":null,"metadata":null,"text":"Thus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It’s indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn’t deduce “people” from “two”, it deduces “people”, partly, from the information — the hidden state — that gave rise to “two”). In LCRNs, though, this is explicit instead of implicit; we “sample” the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_110.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_110.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_110.markups.0":{"type":"EM","start":220,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_110.markups.1":{"type":"EM","start":363,"end":372,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_111":{"id":"5c5cdcc6bd59_111","name":"cdc3","type":"P","href":null,"layout":null,"metadata":null,"text":"The exact quantitative relationships depend on the RNN’s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_112":{"id":"5c5cdcc6bd59_112","name":"0d89","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Yep, I went to France for a holiday. And I actually learned to speak some \u003Cwait, shit, what was the language again? oh yea, “France”…\u003E French!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_113":{"id":"5c5cdcc6bd59_113","name":"5857","type":"P","href":null,"layout":null,"metadata":null,"text":"Obviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren’t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_114":{"id":"5c5cdcc6bd59_114","name":"34be","type":"P","href":null,"layout":null,"metadata":null,"text":"Perhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_115":{"id":"5c5cdcc6bd59_115","name":"05f6","type":"P","href":null,"layout":null,"metadata":null,"text":"So far we’ve looked at one to one and one to many recurrent networks. We can also have many to one:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_115.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_115.markups.0":{"type":"EM","start":87,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_116":{"id":"5c5cdcc6bd59_116","name":"6c8d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*KjYQyc-JD_zs5ERQypM9EA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*KjYQyc-JD_zs5ERQypM9EA.png":{"id":"1*KjYQyc-JD_zs5ERQypM9EA.png","originalHeight":228,"originalWidth":190,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_117":{"id":"5c5cdcc6bd59_117","name":"da0a","type":"P","href":null,"layout":null,"metadata":null,"text":"With many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That’s why, in the image above, the second hidden state has two arrows directed at it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_117.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_117.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_117.markups.0":{"type":"STRONG","start":258,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_117.markups.1":{"type":"EM","start":132,"end":137,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_118":{"id":"5c5cdcc6bd59_118","name":"4715","type":"P","href":null,"layout":null,"metadata":null,"text":"Only one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_119":{"id":"5c5cdcc6bd59_119","name":"a55f","type":"P","href":null,"layout":null,"metadata":null,"text":"The final type of recurrent net is many to many, where both the input and output are sequential:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_120":{"id":"5c5cdcc6bd59_120","name":"86b5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*MPpLCBI1J6r6VmsDm7G4_g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*MPpLCBI1J6r6VmsDm7G4_g.png":{"id":"1*MPpLCBI1J6r6VmsDm7G4_g.png","originalHeight":214,"originalWidth":310,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_121":{"id":"5c5cdcc6bd59_121","name":"7038","type":"P","href":null,"layout":null,"metadata":null,"text":"A use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_122":{"id":"5c5cdcc6bd59_122","name":"2538","type":"P","href":null,"layout":null,"metadata":null,"text":"We can also go deeper and have multiple hidden layers, and\u002For a greater number of timesteps:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_123":{"id":"5c5cdcc6bd59_123","name":"7896","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*vfUWsAgW-c5hUntaPhb1aQ.png","typename":"ImageMetadata"},"text":"We’re getting deeper and deeper!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*vfUWsAgW-c5hUntaPhb1aQ.png":{"id":"1*vfUWsAgW-c5hUntaPhb1aQ.png","originalHeight":305,"originalWidth":313,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_124":{"id":"5c5cdcc6bd59_124","name":"2703","type":"P","href":null,"layout":null,"metadata":null,"text":"Really, this could be considered as multiple RNNs. Technically, you can consider each “hidden layer” as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I’ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_124.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_124.markups.0":{"type":"EM","start":36,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_125":{"id":"5c5cdcc6bd59_125","name":"a0dc","type":"P","href":null,"layout":null,"metadata":null,"text":"When we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn’t just dependent on the first word of the inputted English; it’s dependent on the entire sentence.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_125.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_125.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_125.markups.0":{"type":"EM","start":371,"end":376,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_125.markups.1":{"type":"EM","start":576,"end":583,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_126":{"id":"5c5cdcc6bd59_126","name":"98d9","type":"P","href":null,"layout":null,"metadata":null,"text":"One way to demonstrate why this matters is to use Google Translate:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_127":{"id":"5c5cdcc6bd59_127","name":"af3c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mptNrzbgaDT3YuQL-tpEOw.png","typename":"ImageMetadata"},"text":"One of my favorite Green Day lyrics, from the song “Fashion Victim” on WARNING:. Side-note: Based on my experience with Google Translate in Chinese class over the last 8 years, this translation is probably off.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_127.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_127.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mptNrzbgaDT3YuQL-tpEOw.png":{"id":"1*mptNrzbgaDT3YuQL-tpEOw.png","originalHeight":145,"originalWidth":680,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_127.markups.0":{"type":"STRONG","start":19,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_127.markups.1":{"type":"EM","start":197,"end":206,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_128":{"id":"5c5cdcc6bd59_128","name":"3229","type":"P","href":null,"layout":null,"metadata":null,"text":"Now I’ll input “He’s a victim” and “of his own time” separately. You’ll notice that when you join the two translated outputs, this won’t be equal to the corresponding phrase in the first translation:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_129":{"id":"5c5cdcc6bd59_129","name":"aef0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*lO5oCsSXTy4Loic3SASMlw.png","typename":"ImageMetadata"},"text":"What happens if we break up the English into different parts, translate, and join together the translated Chinese parts?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*lO5oCsSXTy4Loic3SASMlw.png":{"id":"1*lO5oCsSXTy4Loic3SASMlw.png","originalHeight":245,"originalWidth":276,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_130":{"id":"5c5cdcc6bd59_130","name":"44aa","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*oJRFlstyAI-MkBguW6otfA.png","typename":"ImageMetadata"},"text":"They’re not equal.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_130.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*oJRFlstyAI-MkBguW6otfA.png":{"id":"1*oJRFlstyAI-MkBguW6otfA.png","originalHeight":110,"originalWidth":551,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_130.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_131":{"id":"5c5cdcc6bd59_131","name":"a661","type":"P","href":null,"layout":null,"metadata":null,"text":"What gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it’s used. It all depends on the context and the entire sentence as a whole — the meaning you’re trying to convey. This is the exact approach a human translator would take.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_131.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_131.markups.0":{"type":"STRONG","start":223,"end":231,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_132":{"id":"5c5cdcc6bd59_132","name":"fad3","type":"P","href":null,"layout":null,"metadata":null,"text":"Another type of many to many architecture exists where each neuron has a state at every timestep, in a “synchronized” fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn’t be suitable for translation.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_133":{"id":"5c5cdcc6bd59_133","name":"5a39","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*84IkP_dqLUfImZ5SyZLwjA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*84IkP_dqLUfImZ5SyZLwjA.png":{"id":"1*84IkP_dqLUfImZ5SyZLwjA.png","originalHeight":231,"originalWidth":190,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_134":{"id":"5c5cdcc6bd59_134","name":"cef1","type":"P","href":null,"layout":null,"metadata":null,"text":"An application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note — an RNN is better at this task than CNNs are because what’s going on in a scene is much easier to understand if you’ve watched the video up to that point and thus can contextualize it. That’s what humans do!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_134.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_134.markups.0":{"type":"STRONG","start":325,"end":347,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_135":{"id":"5c5cdcc6bd59_135","name":"f0a6","type":"P","href":null,"layout":null,"metadata":null,"text":"Quick note: we can “wrap” the RNN into a much more succinct form, where we collapse the depth and time properties, like so:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_136":{"id":"5c5cdcc6bd59_136","name":"e5dd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*7POP9GXAsRlbRRrsrhr-jA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*7POP9GXAsRlbRRrsrhr-jA.png":{"id":"1*7POP9GXAsRlbRRrsrhr-jA.png","originalHeight":250,"originalWidth":190,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_137":{"id":"5c5cdcc6bd59_137","name":"fc78","type":"P","href":null,"layout":null,"metadata":null,"text":"This notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it’s sort of like a loop that feeds itself.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_138":{"id":"5c5cdcc6bd59_138","name":"3ba7","type":"P","href":null,"layout":null,"metadata":null,"text":"When you ever read about “unrolling” an RNN into a feedforward network that looks like it’s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_139":{"id":"5c5cdcc6bd59_139","name":"0d76","type":"P","href":null,"layout":null,"metadata":null,"text":"Another quick note: when somebody or a research paper mentions that they are using “512 RNN units”, this translates to: “1 RNN neuron that outputs a 512-wide vector”; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it’s luckily much simpler than that… albeit strangely worded.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_140":{"id":"5c5cdcc6bd59_140","name":"2ae1","type":"P","href":null,"layout":null,"metadata":null,"text":"Furthermore, one “RNN unit” usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: “stacking RNNs on top of each other”. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers ℓ, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_140.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_140.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_140.markups.0":{"type":"STRONG","start":425,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_140.markups.1":{"type":"STRONG","start":444,"end":445,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_141":{"id":"5c5cdcc6bd59_141","name":"e5b5","type":"H3","href":null,"layout":null,"metadata":null,"text":"Formalism","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_142":{"id":"5c5cdcc6bd59_142","name":"a93d","type":"P","href":null,"layout":null,"metadata":null,"text":"So, now, let’s walk through the formal mathematical notation involved in RNNs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_143":{"id":"5c5cdcc6bd59_143","name":"ad6a","type":"P","href":null,"layout":null,"metadata":null,"text":"If an input or output neuron has a value at timestep t, we denote the vector as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_143.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_143.markups.0":{"type":"STRONG","start":53,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_144":{"id":"5c5cdcc6bd59_144","name":"a4f8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_D9bOLepOSSbC2zgK7wreQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_D9bOLepOSSbC2zgK7wreQ.png":{"id":"1*_D9bOLepOSSbC2zgK7wreQ.png","originalHeight":77,"originalWidth":141,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_145":{"id":"5c5cdcc6bd59_145","name":"a6fd","type":"P","href":null,"layout":null,"metadata":null,"text":"For the hidden neurons it’s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer ℓ as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_145.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_145.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_145.markups.0":{"type":"STRONG","start":133,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_145.markups.1":{"type":"STRONG","start":152,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_146":{"id":"5c5cdcc6bd59_146","name":"7987","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*QJFWCdOVxGAge0ZT17hw1g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*QJFWCdOVxGAge0ZT17hw1g.png":{"id":"1*QJFWCdOVxGAge0ZT17hw1g.png","originalHeight":41,"originalWidth":157,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_147":{"id":"5c5cdcc6bd59_147","name":"7ba4","type":"P","href":null,"layout":null,"metadata":null,"text":"The input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_147.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_147.markups.0":{"type":"EM","start":89,"end":93,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_148":{"id":"5c5cdcc6bd59_148","name":"982f","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s start with hidden states. First, we’ll revisit the most complex recurrent net we came across earlier — the many to many architecture:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_149":{"id":"5c5cdcc6bd59_149","name":"44bd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TJxcM6GI8dMHEq6sK3Ky8Q.png","typename":"ImageMetadata"},"text":"Many to many, non-synchronized.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TJxcM6GI8dMHEq6sK3Ky8Q.png":{"id":"1*TJxcM6GI8dMHEq6sK3Ky8Q.png","originalHeight":206,"originalWidth":250,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_150":{"id":"5c5cdcc6bd59_150","name":"5f3c","type":"P","href":null,"layout":null,"metadata":null,"text":"This RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_151":{"id":"5c5cdcc6bd59_151","name":"af44","type":"P","href":null,"layout":null,"metadata":null,"text":"First, let’s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_152":{"id":"5c5cdcc6bd59_152","name":"5021","type":"ULI","href":null,"layout":null,"metadata":null,"text":"An input","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_153":{"id":"5c5cdcc6bd59_153","name":"8970","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Hidden state at the previous timestep, same layer","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_154":{"id":"5c5cdcc6bd59_154","name":"429d","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Hidden state at the current timestep, previous layer","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_155":{"id":"5c5cdcc6bd59_155","name":"dc21","type":"P","href":null,"layout":null,"metadata":null,"text":"A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_156":{"id":"5c5cdcc6bd59_156","name":"b721","type":"P","href":null,"layout":null,"metadata":null,"text":"If this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_157":{"id":"5c5cdcc6bd59_157","name":"5a0a","type":"P","href":null,"layout":null,"metadata":null,"text":"Because of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_158":{"id":"5c5cdcc6bd59_158","name":"8095","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*n5QR9Q9ZGnWFRf7pROtliA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*n5QR9Q9ZGnWFRf7pROtliA.png":{"id":"1*n5QR9Q9ZGnWFRf7pROtliA.png","originalHeight":107,"originalWidth":400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_159":{"id":"5c5cdcc6bd59_159","name":"7d12","type":"P","href":null,"layout":null,"metadata":null,"text":"This probably looks a bit confusing; let me break it down for you. The function ƒw computes the numeric hidden state vector for timestep t and layer ℓ; it contains the “activation function” you’re used to hearing about with ANNs. W are the weights of the recurrent net, and thus ƒ is conditioned on W. We haven’t exactly defined ƒ just yet, but what’s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_159.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_159.markups.0":{"type":"STRONG","start":80,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.1":{"type":"STRONG","start":137,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.2":{"type":"STRONG","start":149,"end":150,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.3":{"type":"STRONG","start":230,"end":232,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.4":{"type":"STRONG","start":279,"end":281,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.5":{"type":"STRONG","start":299,"end":300,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_159.markups.6":{"type":"STRONG","start":329,"end":330,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160":{"id":"5c5cdcc6bd59_160","name":"a42e","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Where ℓ = 1, the hidden state at time t and layer ℓ is a function of the hidden state vector at time t-1 and layer ℓ as well as the input vector at time t. Where ℓ \u003E 1, this hidden state is a function of the hidden state vector at time t-1 and layer ℓ as well as the hidden state vector at time t, layer ℓ-1.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_160.markups.10","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_160.markups.0":{"type":"STRONG","start":6,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.1":{"type":"STRONG","start":38,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.2":{"type":"STRONG","start":50,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.3":{"type":"STRONG","start":101,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.4":{"type":"STRONG","start":115,"end":117,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.5":{"type":"STRONG","start":153,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.6":{"type":"STRONG","start":162,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.7":{"type":"STRONG","start":236,"end":240,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.8":{"type":"STRONG","start":250,"end":252,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.9":{"type":"STRONG","start":295,"end":296,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_160.markups.10":{"type":"STRONG","start":304,"end":307,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_161":{"id":"5c5cdcc6bd59_161","name":"7713","type":"P","href":null,"layout":null,"metadata":null,"text":"You might notice that we have a couple issues:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_162":{"id":"5c5cdcc6bd59_162","name":"69f7","type":"ULI","href":null,"layout":null,"metadata":null,"text":"When t = 1 — that is, when each neuron is at the initial timestep — then no previous timestep exists. However, we still attempt to pass h_0 as a parameter to ƒw.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_162.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_162.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_162.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_162.markups.0":{"type":"STRONG","start":5,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_162.markups.1":{"type":"STRONG","start":136,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_162.markups.2":{"type":"STRONG","start":158,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_163":{"id":"5c5cdcc6bd59_163","name":"7b09","type":"ULI","href":null,"layout":null,"metadata":null,"text":"If no input exists at time t — thus, x_t does not exist — then we still attempt to pass x_t as a parameter.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_163.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_163.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_163.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_163.markups.0":{"type":"STRONG","start":27,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_163.markups.1":{"type":"STRONG","start":37,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_163.markups.2":{"type":"STRONG","start":88,"end":92,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_164":{"id":"5c5cdcc6bd59_164","name":"2062","type":"P","href":null,"layout":null,"metadata":null,"text":"Our respective solutions follow:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_165":{"id":"5c5cdcc6bd59_165","name":"c5c1","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Define h_0 for any layer as 0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_165.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_165.markups.0":{"type":"STRONG","start":7,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_166":{"id":"5c5cdcc6bd59_166","name":"68f6","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Consider x_t where no input exists at timestep t as 0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_166.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_166.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_166.markups.0":{"type":"STRONG","start":9,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_166.markups.1":{"type":"STRONG","start":47,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_167":{"id":"5c5cdcc6bd59_167","name":"0ed3","type":"P","href":null,"layout":null,"metadata":null,"text":"If these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_168":{"id":"5c5cdcc6bd59_168","name":"cf77","type":"P","href":null,"layout":null,"metadata":null,"text":"We actually have five different types of weight matrices:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_169":{"id":"5c5cdcc6bd59_169","name":"61fa","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*r09EQtFlEA1kIiOJD2aZ6g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*r09EQtFlEA1kIiOJD2aZ6g.png":{"id":"1*r09EQtFlEA1kIiOJD2aZ6g.png","originalHeight":233,"originalWidth":390,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_170":{"id":"5c5cdcc6bd59_170","name":"b66b","type":"P","href":null,"layout":null,"metadata":null,"text":"Pro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(ℓ-1)_t to h^ℓ_t. W_hy maps a hidden state vector h to an output vector y.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_170.markups.15","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_170.markups.0":{"type":"STRONG","start":98,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.1":{"type":"STRONG","start":124,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.2":{"type":"STRONG","start":151,"end":152,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.3":{"type":"STRONG","start":154,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.4":{"type":"STRONG","start":187,"end":188,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.5":{"type":"STRONG","start":220,"end":222,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.6":{"type":"STRONG","start":252,"end":258,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.7":{"type":"STRONG","start":261,"end":264,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.8":{"type":"STRONG","start":285,"end":291,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.9":{"type":"STRONG","start":318,"end":320,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.10":{"type":"STRONG","start":351,"end":353,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.11":{"type":"STRONG","start":384,"end":394,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.12":{"type":"STRONG","start":397,"end":402,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.13":{"type":"STRONG","start":404,"end":409,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.14":{"type":"STRONG","start":436,"end":438,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_170.markups.15":{"type":"STRONG","start":458,"end":459,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_171":{"id":"5c5cdcc6bd59_171","name":"283a","type":"P","href":null,"layout":null,"metadata":null,"text":"Like with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_171.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_171.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_171.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_171.markups.0":{"type":"A","start":204,"end":208,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-5-what-are-bias-units-828d942b4f52","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_171.markups.1":{"type":"STRONG","start":70,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_171.markups.2":{"type":"STRONG","start":177,"end":180,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_172":{"id":"5c5cdcc6bd59_172","name":"246f","type":"P","href":null,"layout":null,"metadata":null,"text":"For both b_h and W_hht\u002FW_hhd, we actually have multiple weight matrices depending on the value of ℓ, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn’t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values — 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn’t have anything to use. Since this would also mean that the number of parameters in the neural network would grow linearly relative to the input, we would have way too many parameters very potentially causing overfitting.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_172.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_172.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_172.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_172.markups.0":{"type":"STRONG","start":9,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_172.markups.1":{"type":"STRONG","start":17,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_172.markups.2":{"type":"STRONG","start":98,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_173":{"id":"5c5cdcc6bd59_173","name":"6da9","type":"P","href":null,"layout":null,"metadata":null,"text":"W_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer ℓ, we could suggest that W_hhd will not exist because W_hy will be in its place.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_173.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_173.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_173.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_173.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_173.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_173.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_173.markups.1":{"type":"STRONG","start":87,"end":88,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_173.markups.2":{"type":"STRONG","start":116,"end":117,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_173.markups.3":{"type":"STRONG","start":141,"end":147,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_173.markups.4":{"type":"STRONG","start":170,"end":175,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_174":{"id":"5c5cdcc6bd59_174","name":"e3ed","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we’ll define the function ƒw:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_174.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_174.markups.0":{"type":"STRONG","start":30,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_175":{"id":"5c5cdcc6bd59_175","name":"d692","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*9YGuqXdNiknmZR2HScMDKw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*9YGuqXdNiknmZR2HScMDKw.png":{"id":"1*9YGuqXdNiknmZR2HScMDKw.png","originalHeight":226,"originalWidth":404,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_176":{"id":"5c5cdcc6bd59_176","name":"e06a","type":"P","href":null,"layout":null,"metadata":null,"text":"The function is very similar to the ANN hidden function you’ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or “squashing” function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W ⋅ h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_176.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_176.markups.0":{"type":"STRONG","start":361,"end":366,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_176.markups.1":{"type":"STRONG","start":388,"end":389,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_176.markups.2":{"type":"STRONG","start":431,"end":436,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_176.markups.3":{"type":"STRONG","start":506,"end":507,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_176.markups.4":{"type":"EM","start":322,"end":323,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_176.markups.5":{"type":"EM","start":349,"end":355,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_177":{"id":"5c5cdcc6bd59_177","name":"d904","type":"P","href":null,"layout":null,"metadata":null,"text":"If you’ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs — more on that later), the fact that they produce gradients with a greater range, and that their second derivative don’t die off as quickly.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_178":{"id":"5c5cdcc6bd59_178","name":"25ba","type":"P","href":null,"layout":null,"metadata":null,"text":"Similar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_179":{"id":"5c5cdcc6bd59_179","name":"0f69","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*NPI9iLLVlYLQ2gu9A9xp0A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NPI9iLLVlYLQ2gu9A9xp0A.png":{"id":"1*NPI9iLLVlYLQ2gu9A9xp0A.png","originalHeight":166,"originalWidth":324,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_180":{"id":"5c5cdcc6bd59_180","name":"9d21","type":"P","href":null,"layout":null,"metadata":null,"text":"If interested, the tanh equation follows (though I won’t walk you through it):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_181":{"id":"5c5cdcc6bd59_181","name":"a172","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*w7LV9vY1hCAXcLk2K_peEg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*w7LV9vY1hCAXcLk2K_peEg.png":{"id":"1*w7LV9vY1hCAXcLk2K_peEg.png","originalHeight":84,"originalWidth":241,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_182":{"id":"5c5cdcc6bd59_182","name":"85eb","type":"P","href":null,"layout":null,"metadata":null,"text":"The final equation is mapping a hidden state to an output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_183":{"id":"5c5cdcc6bd59_183","name":"f531","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*n7simJp73WxCRx_Bz4dXwg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*n7simJp73WxCRx_Bz4dXwg.png":{"id":"1*n7simJp73WxCRx_Bz4dXwg.png","originalHeight":44,"originalWidth":177,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_184":{"id":"5c5cdcc6bd59_184","name":"93b1","type":"P","href":null,"layout":null,"metadata":null,"text":"This is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid\u002Fsoftmax (for example if the output needs to be a probability distribution), etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_184.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_184.markups.0":{"type":"EM","start":8,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_185":{"id":"5c5cdcc6bd59_185","name":"3584","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s how we express recurrent nets, mathematically!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_186":{"id":"5c5cdcc6bd59_186","name":"2650","type":"P","href":null,"layout":null,"metadata":null,"text":"Quick note: Notation may and will differ between various lectures, research paper, articles, etc. For example — some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn’t cover edge cases like I did or obfuscates certain indices like ℓ with hidden to hidden weight matrices. So, just keep note that specifics don’t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_186.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_186.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_186.markups.0":{"type":"STRONG","start":336,"end":338,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_186.markups.1":{"type":"EM","start":210,"end":215,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_187":{"id":"5c5cdcc6bd59_187","name":"ed09","type":"H3","href":null,"layout":null,"metadata":null,"text":"An example? Okay!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_188":{"id":"5c5cdcc6bd59_188","name":"e53b","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s take a look at a quick example of an RNN in action. I’m going to adapt a super dumbed down one from Andrej Karpathy’s Stanford CS231n RNN lecture, where a one to many “character level language model” single layer recurrent neural network needs to output “hello”. We’ll kick it of by giving the RNN the letter “h” , such that it needs to complete the word by outputting the other four letters.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_188.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_188.markups.0":{"type":"A","start":140,"end":151,"href":"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_189":{"id":"5c5cdcc6bd59_189","name":"1901","type":"P","href":null,"layout":null,"metadata":null,"text":"Sidenote: this model nicknamed “char-rnn” — remember it for later, where we get to code our own!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_190":{"id":"5c5cdcc6bd59_190","name":"fc9c","type":"P","href":null,"layout":null,"metadata":null,"text":"The neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word “hello”. We will input the first character, “h”, and from there expect the output at the following timesteps to be: “e”, “l”, “l”, and “o” respectively, to form:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_191":{"id":"5c5cdcc6bd59_191","name":"56af","type":"PQ","href":null,"layout":null,"metadata":null,"text":"hello","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_192":{"id":"5c5cdcc6bd59_192","name":"9f5f","type":"P","href":null,"layout":null,"metadata":null,"text":"We can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent “h”, “e”, “l”, and “o” respectively.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_193":{"id":"5c5cdcc6bd59_193","name":"28cd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pgWSPyximAFHqZtUkiLeKg.png","typename":"ImageMetadata"},"text":"This is called “one-hot encoding”, because only one of the values in the vector is equal to 1 and thus on (or “hot”).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pgWSPyximAFHqZtUkiLeKg.png":{"id":"1*pgWSPyximAFHqZtUkiLeKg.png","originalHeight":192,"originalWidth":440,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_194":{"id":"5c5cdcc6bd59_194","name":"c682","type":"P","href":null,"layout":null,"metadata":null,"text":"This is what we’d expect with a trained RNN:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_195":{"id":"5c5cdcc6bd59_195","name":"0f80","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mmuQb8msqqQLtz580_lpvw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mmuQb8msqqQLtz580_lpvw.png":{"id":"1*mmuQb8msqqQLtz580_lpvw.png","originalHeight":227,"originalWidth":250,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_196":{"id":"5c5cdcc6bd59_196","name":"17a3","type":"P","href":null,"layout":null,"metadata":null,"text":"As you can see, we input the first letter and the word is completed. We don’t know exactly what the hidden states will be — that’s why they’re hidden!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_197":{"id":"5c5cdcc6bd59_197","name":"04ad","type":"P","href":null,"layout":null,"metadata":null,"text":"One interesting technique would be to sample the output at each timestep and feed it into the next as input:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_198":{"id":"5c5cdcc6bd59_198","name":"9eb9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*KyVSttLGcexQWDLvSWD0Lg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*KyVSttLGcexQWDLvSWD0Lg.png":{"id":"1*KyVSttLGcexQWDLvSWD0Lg.png","originalHeight":226,"originalWidth":250,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_199":{"id":"5c5cdcc6bd59_199","name":"08da","type":"P","href":null,"layout":null,"metadata":null,"text":"When we “sample” from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is “e” at the first timestep’s output. Let’s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep’s input, there’s a 90% chance we select “e”; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don’t end up in a loop where you keep sampling the same letter or sequence of letters over and over again.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_199.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_199.markups.0":{"type":"EM","start":364,"end":368,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_200":{"id":"5c5cdcc6bd59_200","name":"0880","type":"P","href":null,"layout":null,"metadata":null,"text":"As mentioned earlier, this is used pretty heavily with LCRNs. It’s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_201":{"id":"5c5cdcc6bd59_201","name":"9cac","type":"P","href":null,"layout":null,"metadata":null,"text":"However, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is “l” and the expected output is also “l”. However, at timestep 4, the input is again “l” but the output is now “o”, to complete the word. Memory is still needed to make a distinction like this.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_201.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_201.markups.0":{"type":"EM","start":58,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_202":{"id":"5c5cdcc6bd59_202","name":"db16","type":"P","href":null,"layout":null,"metadata":null,"text":"In numerical form, it would look something like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_203":{"id":"5c5cdcc6bd59_203","name":"5acf","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*AguGRuRZg6e6RZ7Ctvn2Ww.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*AguGRuRZg6e6RZ7Ctvn2Ww.png":{"id":"1*AguGRuRZg6e6RZ7Ctvn2Ww.png","originalHeight":371,"originalWidth":280,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_204":{"id":"5c5cdcc6bd59_204","name":"c815","type":"P","href":null,"layout":null,"metadata":null,"text":"Of course, we won’t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we’d apply softmax to the output), and will sample from this distribution to get a single character output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_205":{"id":"5c5cdcc6bd59_205","name":"afba","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*6067M6Oqz2zNoyyyQC1Suw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*6067M6Oqz2zNoyyyQC1Suw.png":{"id":"1*6067M6Oqz2zNoyyyQC1Suw.png","originalHeight":574,"originalWidth":300,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_206":{"id":"5c5cdcc6bd59_206","name":"9419","type":"P","href":null,"layout":null,"metadata":null,"text":"Each hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_207":{"id":"5c5cdcc6bd59_207","name":"58bf","type":"P","href":null,"layout":null,"metadata":null,"text":"The RNN is saying: given “h”, “e” is most likely to be the next character. Given “he”, “l” is the next likely character. With “hel”, “l” should be next, and with “hell”, the final character should be “o”.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_208":{"id":"5c5cdcc6bd59_208","name":"b7ec","type":"P","href":null,"layout":null,"metadata":null,"text":"But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_209":{"id":"5c5cdcc6bd59_209","name":"399a","type":"P","href":null,"layout":null,"metadata":null,"text":"One more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (“o”), we can sample this back as input and expect that the “\u003CEND\u003E” token (however we choose to represent it — could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn’t as obvious in this fabricated example, because we know when “hello” has been completed, but consider a real-life scenario where we don’t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using while or stop after the upper limit\u002Fmax possible preset constant value of n is reached).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_209.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_209.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_209.markups.0":{"type":"CODE","start":870,"end":875,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_209.markups.1":{"type":"STRONG","start":34,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_210":{"id":"5c5cdcc6bd59_210","name":"a127","type":"P","href":null,"layout":null,"metadata":null,"text":"Start tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we’ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a “\u003CSTART\u003E” token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_211":{"id":"5c5cdcc6bd59_211","name":"85be","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn’t “fit” what we’ll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as “\u003CSTART\u003E” instead.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_211.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_211.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_211.markups.0":{"type":"STRONG","start":240,"end":244,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_211.markups.1":{"type":"EM","start":101,"end":109,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_212":{"id":"5c5cdcc6bd59_212","name":"2aca","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, just to be clear, the RNN doesn’t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_213":{"id":"5c5cdcc6bd59_213","name":"1c00","type":"P","href":null,"layout":null,"metadata":null,"text":"This is how we can get RNNs to “write”! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_214":{"id":"5c5cdcc6bd59_214","name":"6b00","type":"H3","href":null,"layout":null,"metadata":null,"text":"Training (or, why vanilla RNNs suck.)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_215":{"id":"5c5cdcc6bd59_215","name":"58b5","type":"P","href":null,"layout":null,"metadata":null,"text":"For a recurrent net to be useful, it needs to learn proper weights via training. That’s no surprise.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_216":{"id":"5c5cdcc6bd59_216","name":"8a33","type":"P","href":null,"layout":null,"metadata":null,"text":"Recall this snippet from earlier:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_217":{"id":"5c5cdcc6bd59_217","name":"f72f","type":"BQ","href":null,"layout":null,"metadata":null,"text":"But, if the neural network wasn’t trained on the word “hello”, and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like “hleol” coming out.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_218":{"id":"5c5cdcc6bd59_218","name":"f9d1","type":"P","href":null,"layout":null,"metadata":null,"text":"This is, of course, because we initialize the W weights randomly at first, so random stuff will come out.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_218.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_218.markups.0":{"type":"STRONG","start":46,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_219":{"id":"5c5cdcc6bd59_219","name":"c7a6","type":"P","href":null,"layout":null,"metadata":null,"text":"But, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be “hello” in one-hot encoding form, and we’d compute the discrepancy between this output and what the recurrent net predicts (we’d get the error at each timestep and then add this up) as the total error to then calculate the gradient\u002Fupdate value.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_220":{"id":"5c5cdcc6bd59_220","name":"2ded","type":"P","href":null,"layout":null,"metadata":null,"text":"So, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we’d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we’re differentiating a sum:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_220.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_220.markups.0":{"type":"STRONG","start":125,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_221":{"id":"5c5cdcc6bd59_221","name":"6145","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*d5mzuu-EmcZz0IFukW6XsQ.png","typename":"ImageMetadata"},"text":"For any arbitrary weight W.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_221.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*d5mzuu-EmcZz0IFukW6XsQ.png":{"id":"1*d5mzuu-EmcZz0IFukW6XsQ.png","originalHeight":79,"originalWidth":651,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_221.markups.0":{"type":"STRONG","start":25,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_222":{"id":"5c5cdcc6bd59_222","name":"bb6e","type":"P","href":null,"layout":null,"metadata":null,"text":"But, you should know that, with artificial neural networks, calculating these gradients isn’t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_223":{"id":"5c5cdcc6bd59_223","name":"c4f4","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we’re long overdue for our own mega-post on optimization!)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_223.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_223.markups.0":{"type":"A","start":274,"end":286,"href":"http:\u002F\u002Fsebastianruder.com\u002Foptimizing-gradient-descent\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_224":{"id":"5c5cdcc6bd59_224","name":"0be4","type":"P","href":null,"layout":null,"metadata":null,"text":"Backpropagation with RNNs is called “Backpropagation Through Time” (short for BPTT), since it operates on sequences in time. But don’t be fooled — there’s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you “unroll” an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it’s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There’s more work to do to compute the gradients, but it’s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I’m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_224.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_224.markups.0":{"type":"EM","start":752,"end":763,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_225":{"id":"5c5cdcc6bd59_225","name":"224b","type":"P","href":null,"layout":null,"metadata":null,"text":"One thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_225.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_225.markups.0":{"type":"STRONG","start":407,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_226":{"id":"5c5cdcc6bd59_226","name":"1fee","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, if you haven’t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_227":{"id":"5c5cdcc6bd59_227","name":"c672","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"text":"Rohan #4: The vanishing gradient problem\nOh no — an obstacle to deep learning!ayearofai.com","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_227.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_227.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_227.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_227.mixtapeMetadata","typename":"MixtapeMetadata"}},"Paragraph:5c5cdcc6bd59_227.markups.0":{"type":"A","start":0,"end":91,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_227.markups.1":{"type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_227.markups.2":{"type":"EM","start":41,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Paragraph:5c5cdcc6bd59_227.mixtapeMetadata":{"href":"https:\u002F\u002Fayearofai.com\u002Frohan-4-the-vanishing-gradient-problem-ec68f76ffb9b","thumbnailImageId":"1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg","__typename":"MixtapeMetadata"},"Paragraph:5c5cdcc6bd59_228":{"id":"5c5cdcc6bd59_228","name":"be11","type":"P","href":null,"layout":null,"metadata":null,"text":"You may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That’s like an ANN with hundreds of entire hidden layers! That’s deep. (Well, it’s more long because we’re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_228.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_228.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_228.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_228.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_228.markups.0":{"type":"EM","start":175,"end":183,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_228.markups.1":{"type":"EM","start":263,"end":267,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_228.markups.2":{"type":"EM","start":268,"end":269,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_228.markups.3":{"type":"EM","start":286,"end":291,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_229":{"id":"5c5cdcc6bd59_229","name":"4878","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_229.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_229.markups.0":{"type":"STRONG","start":73,"end":74,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_230":{"id":"5c5cdcc6bd59_230","name":"44b3","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*gKbRtQfPwGK2d7jnKZdv5w.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*gKbRtQfPwGK2d7jnKZdv5w.png":{"id":"1*gKbRtQfPwGK2d7jnKZdv5w.png","originalHeight":79,"originalWidth":346,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_231":{"id":"5c5cdcc6bd59_231","name":"0e72","type":"P","href":null,"layout":null,"metadata":null,"text":"With a tanh activation function, that’s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we’d add — or, as mentioned before, we could average as well — each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_231.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_231.markups.0":{"type":"STRONG","start":134,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_232":{"id":"5c5cdcc6bd59_232","name":"0639","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jf52uXcsAX6Nn8ghLYoJWQ.png","typename":"ImageMetadata"},"text":"Assuming our sequence is of length k.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_232.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jf52uXcsAX6Nn8ghLYoJWQ.png":{"id":"1*jf52uXcsAX6Nn8ghLYoJWQ.png","originalHeight":80,"originalWidth":773,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_232.markups.0":{"type":"STRONG","start":35,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_233":{"id":"5c5cdcc6bd59_233","name":"9baa","type":"P","href":null,"layout":null,"metadata":null,"text":"So we’d be effectively adding together a bunch of terms that have vanished — the exception being very late gradients with a small number of terms — and so dJ\u002FdWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_233.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_233.markups.0":{"type":"STRONG","start":155,"end":163,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_234":{"id":"5c5cdcc6bd59_234","name":"fd7e","type":"P","href":null,"layout":null,"metadata":null,"text":"But, you might be asking, instead of tanh — which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 — why don’t we just use ReLUs? Don’t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_235":{"id":"5c5cdcc6bd59_235","name":"6a76","type":"P","href":null,"layout":null,"metadata":null,"text":"Well, not entirely; it’s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish — or vice-versa, explode — we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_236":{"id":"5c5cdcc6bd59_236","name":"4faf","type":"P","href":null,"layout":null,"metadata":null,"text":"This is more my suspicion though — I’m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_237":{"id":"5c5cdcc6bd59_237","name":"6eaa","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_237.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:aa3fb6f891aba6d55742cf7dabf3f7f7":{"id":"aa3fb6f891aba6d55742cf7dabf3f7f7","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.quora.com%2Fwidgets%2Fembed_iframe%3Fpath%3D%2FIf-we-primarily-use-LSTMs-over-RNNs-to-solve-the-vanishing-gradient-problem-why-cant-we-just-use-ReLUs-leaky-ReLUs-with-RNNs-instead%2Fanswer%2FOscar-Chen-5&url=https%3A%2F%2Fwww.quora.com%2FIf-we-primarily-use-LSTMs-over-RNNs-to-solve-the-vanishing-gradient-problem-why-cant-we-just-use-ReLUs-leaky-ReLUs-with-RNNs-instead%2Fanswers%2F40786198%3Fsrid%3DO7MN&image=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=quora","iframeHeight":560,"iframeWidth":560,"title":"If we primarily use LSTMs over RNNs to solve the vanishing gradient problem, why can't we just use ReLUs\u002Fleaky ReLUs with RNNs instead?","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_237.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:aa3fb6f891aba6d55742cf7dabf3f7f7","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_238":{"id":"5c5cdcc6bd59_238","name":"e537","type":"P","href":null,"layout":null,"metadata":null,"text":"From this, something interesting I learned is that: since ReLUs are unbounded (it’s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid\u002Ftanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn’t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_238.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_238.markups.0":{"type":"EM","start":68,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_239":{"id":"5c5cdcc6bd59_239","name":"b4fc","type":"P","href":null,"layout":null,"metadata":null,"text":"It ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations → large gradients → large change in weights → even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_239.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_239.markups.0":{"type":"EM","start":135,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_240":{"id":"5c5cdcc6bd59_240","name":"2c04","type":"BQ","href":null,"layout":null,"metadata":null,"text":"This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. So that’s why you see most people using sigmoid\u002Ftanh units, despite the vanishing gradient descent problem.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_241":{"id":"5c5cdcc6bd59_241","name":"4f2a","type":"P","href":null,"layout":null,"metadata":null,"text":"Also well said:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_242":{"id":"5c5cdcc6bd59_242","name":"e86a","type":"BQ","href":null,"layout":null,"metadata":null,"text":"With RNN’s, the problem is that you are repeatedly applying your RNN to itself, which tends to [mostly] cause exponential blowup or [rarely, but sometimes] shrinkage.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_243":{"id":"5c5cdcc6bd59_243","name":"4cb6","type":"P","href":null,"layout":null,"metadata":null,"text":"Other issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don’t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_244":{"id":"5c5cdcc6bd59_244","name":"4a29","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn’t fully grasp this section on how the vanishing and exploding gradient\u002Factivation problem is applicable to them, it doesn’t matter anyways. Because, everything you’ve read up to this point so far… throw it all away. Forget about it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_244.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_244.markups.0":{"type":"STRONG","start":15,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_245":{"id":"5c5cdcc6bd59_245","name":"044a","type":"P","href":null,"layout":null,"metadata":null,"text":"Just kidding. Don’t do that.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_246":{"id":"5c5cdcc6bd59_246","name":"1d2b","type":"H3","href":null,"layout":null,"metadata":null,"text":"Fixing the problem with LSTMs (Part I)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_247":{"id":"5c5cdcc6bd59_247","name":"daad","type":"P","href":null,"layout":null,"metadata":null,"text":"You shouldn’t do that because RNNs actually aren’t a lost cause. They’re far from it. We just need to make a few… modifications.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_247.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_247.markups.0":{"type":"EM","start":44,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_248":{"id":"5c5cdcc6bd59_248","name":"9491","type":"P","href":null,"layout":null,"metadata":null,"text":"Enter the LSTM.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_249":{"id":"5c5cdcc6bd59_249","name":"32fa","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*JuC5afKk7QIntsvyEn-IFA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JuC5afKk7QIntsvyEn-IFA.png":{"id":"1*JuC5afKk7QIntsvyEn-IFA.png","originalHeight":170,"originalWidth":298,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_250":{"id":"5c5cdcc6bd59_250","name":"731a","type":"P","href":null,"layout":null,"metadata":null,"text":"Makes sense, no?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_251":{"id":"5c5cdcc6bd59_251","name":"ea5b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*tB6QdzunJV08wyep0ZhVMA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*tB6QdzunJV08wyep0ZhVMA.png":{"id":"1*tB6QdzunJV08wyep0ZhVMA.png","originalHeight":357,"originalWidth":350,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_252":{"id":"5c5cdcc6bd59_252","name":"03ff","type":"P","href":null,"layout":null,"metadata":null,"text":"How about this?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_253":{"id":"5c5cdcc6bd59_253","name":"0c36","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Oin8uuuQzp_wqtHAX1yyjQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Oin8uuuQzp_wqtHAX1yyjQ.png":{"id":"1*Oin8uuuQzp_wqtHAX1yyjQ.png","originalHeight":314,"originalWidth":400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_254":{"id":"5c5cdcc6bd59_254","name":"0d8c","type":"P","href":null,"layout":null,"metadata":null,"text":"OK. Clearly something’s not registering here. But that’s fine; LSTM diagrams are frikin’ difficult for beginners to grasp. I too remember when I first searched up “LSTM” on Google to encounter something similar to the works of art above. I reacted like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_255":{"id":"5c5cdcc6bd59_255","name":"16e7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*S7ABwK33X7no_MP3epry6A.gif","typename":"ImageMetadata"},"text":"MRW first Google Image-ing LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*S7ABwK33X7no_MP3epry6A.gif":{"id":"1*S7ABwK33X7no_MP3epry6A.gif","originalHeight":229,"originalWidth":195,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_256":{"id":"5c5cdcc6bd59_256","name":"7cd4","type":"P","href":null,"layout":null,"metadata":null,"text":"In this section, I’m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I’ll probably fail.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_257":{"id":"5c5cdcc6bd59_257","name":"b288","type":"P","href":null,"layout":null,"metadata":null,"text":"With that being said, let’s dive into Long Short-Term Memory networks. (Yes, that’s what LSTM stands for.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_257.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_257.markups.0":{"type":"STRONG","start":38,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_258":{"id":"5c5cdcc6bd59_258","name":"bb27","type":"P","href":null,"layout":null,"metadata":null,"text":"With RNNs, the real “substance” of the model were the hidden neurons; these were the units that did processing on the input, through time, to produce the outputs. Specifically, at each timestep, a hidden neuron embodies a hidden state that is computed by feeding the weighted sum vector of the input and\u002For previous hidden states with an added bias vector through a tanh squashing function. We can have multiple hidden neurons, each of which when unrolled forms an entire hidden “layer”.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_259":{"id":"5c5cdcc6bd59_259","name":"66c2","type":"P","href":null,"layout":null,"metadata":null,"text":"If you need a refresher on this, look through the “Formalism” section once again.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_260":{"id":"5c5cdcc6bd59_260","name":"cd18","type":"P","href":null,"layout":null,"metadata":null,"text":"With LSTMs, we still have hidden states, but they’re computed through a much more complex mechanism: LSTM cells. Instead of computing each hidden state as a direct function of inputs and other hidden states, we compute it as a function of the LSTM cell’s value (the “cell state”) at that timestep. Each cell state is in turn functionally dependent on the previous cell state and any available input or previous hidden states. That’s right — hidden states are computed from cell states, and cell states are (in part) computed from older and\u002For shallower hidden states.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_260.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_260.markups.0":{"type":"EM","start":363,"end":364,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_261":{"id":"5c5cdcc6bd59_261","name":"095f","type":"P","href":null,"layout":null,"metadata":null,"text":"The cell state at a specific timestep t is denoted c_t. Like a hidden state, a cell state is just a vector.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_261.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_261.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_261.markups.0":{"type":"STRONG","start":38,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_261.markups.1":{"type":"STRONG","start":51,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_262":{"id":"5c5cdcc6bd59_262","name":"e92a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*sr8XQzvr-WTNSbgWwI9qbQ.png","typename":"ImageMetadata"},"text":"For simplicity’s sake, I’ve obfuscated layer index ℓ.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_262.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*sr8XQzvr-WTNSbgWwI9qbQ.png":{"id":"1*sr8XQzvr-WTNSbgWwI9qbQ.png","originalHeight":245,"originalWidth":500,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_262.markups.0":{"type":"STRONG","start":51,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_263":{"id":"5c5cdcc6bd59_263","name":"68f5","type":"P","href":null,"layout":null,"metadata":null,"text":"If the diagram above seems a bit trippy, let me break it down for you.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_264":{"id":"5c5cdcc6bd59_264","name":"8bd2","type":"P","href":null,"layout":null,"metadata":null,"text":"c_t, as highlighted by the three arrows pointing towards it, has multiple potential functional dependencies. Four to be exact, though only a maximum of three can exist at once. These are:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_264.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_264.markups.0":{"type":"STRONG","start":0,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_265":{"id":"5c5cdcc6bd59_265","name":"4f8f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The previous hidden state in time: h_t-1. Again, if t = 1, then this won’t exist. If it does, this would be the first arrow pointing into the left side of c_t.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_265.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_265.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_265.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_265.markups.0":{"type":"STRONG","start":35,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_265.markups.1":{"type":"STRONG","start":52,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_265.markups.2":{"type":"STRONG","start":155,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_266":{"id":"5c5cdcc6bd59_266","name":"f283","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The previous cell state: c_t-1. If t = 1, the dependency obviously won’t exist. This refers to the second arrow pointing into the left side of c_t.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_266.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_266.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_266.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_266.markups.0":{"type":"STRONG","start":25,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_266.markups.1":{"type":"STRONG","start":35,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_266.markups.2":{"type":"STRONG","start":143,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_267":{"id":"5c5cdcc6bd59_267","name":"7237","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Input at the current timestep: x_t. There may very well be no input available, for example if we are at a hidden layer ℓ \u003E 1. So this dependency doesn’t always exist. When it does, it’s the arrow pointing into the bottom of c_t.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_267.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_267.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_267.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_267.markups.0":{"type":"STRONG","start":31,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_267.markups.1":{"type":"STRONG","start":119,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_267.markups.2":{"type":"STRONG","start":224,"end":227,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_268":{"id":"5c5cdcc6bd59_268","name":"83f3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The previous hidden state in depth: h^(ℓ-1)_t. This applies for any hidden layer ℓ \u003E 1. In such case, it would — like the input x_t — be the arrow pointing into the bottom.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_268.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_268.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_268.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_268.markups.0":{"type":"STRONG","start":36,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_268.markups.1":{"type":"STRONG","start":81,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_268.markups.2":{"type":"STRONG","start":128,"end":131,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_269":{"id":"5c5cdcc6bd59_269","name":"2a71","type":"P","href":null,"layout":null,"metadata":null,"text":"Only three can exist at once because the last two are mutually exclusive.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_270":{"id":"5c5cdcc6bd59_270","name":"5160","type":"P","href":null,"layout":null,"metadata":null,"text":"From there, we pass information to the next cell state c_t+1 and compute h_t. As you can hopefully see, h_t then goes on to also influence c_t+1 (as indicated by the horizontal arrow), along with higher level cell states or final outputs (the vertical arrow).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_270.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_270.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_270.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_270.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_270.markups.0":{"type":"STRONG","start":55,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_270.markups.1":{"type":"STRONG","start":73,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_270.markups.2":{"type":"STRONG","start":104,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_270.markups.3":{"type":"STRONG","start":139,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_271":{"id":"5c5cdcc6bd59_271","name":"0f0f","type":"P","href":null,"layout":null,"metadata":null,"text":"Right now the cells are a black box… literally; we know what is inputted to them and what they output, but we don’t know their internal process. So… what’s inside these cells? What do they do? What are the exact computations involved? How have the equations changed?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_272":{"id":"5c5cdcc6bd59_272","name":"ee47","type":"P","href":null,"layout":null,"metadata":null,"text":"To help answer the question, I want you to imagine something: a factory. Inside this factory we have workers who perform their own tasks. Those tasks are, specifically, operating on some sort of product that runs down a conveyer belt. Think of, hell, I don’t know — chicken nuggets! The first worker cuts an appropriately sized piece, the second worker applies egg wash, the third worker adds breadcrumbs, the fourth worker chucks it in the fryer, etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_272.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_272.markups.0":{"type":"STRONG","start":220,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_273":{"id":"5c5cdcc6bd59_273","name":"5a97","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4avrG18SFOMJGI4CpDIsoA.png","typename":"ImageMetadata"},"text":"I’m not sure what product this conveyer belt carries, but it certainly doesn’t look appetizing (or like chicken nuggets).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4avrG18SFOMJGI4CpDIsoA.png":{"id":"1*4avrG18SFOMJGI4CpDIsoA.png","originalHeight":600,"originalWidth":490,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_274":{"id":"5c5cdcc6bd59_274","name":"02ea","type":"P","href":null,"layout":null,"metadata":null,"text":"You’re thinking: “OK Rohan, but how does this relate to LSTMs?”. Good question.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_275":{"id":"5c5cdcc6bd59_275","name":"9004","type":"P","href":null,"layout":null,"metadata":null,"text":"Basically, think of the conveyer belt as the cell state, the chicken-nugget-in-progress as information flowing through the cell, and the workers as operations we apply to this information. The final product is the finished chicken nugget — or, the cell state value.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_276":{"id":"5c5cdcc6bd59_276","name":"b54e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*qNUGFMhlnl0-mNLIVvyGAg.png","typename":"ImageMetadata"},"text":"Chicken. Nugget.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qNUGFMhlnl0-mNLIVvyGAg.png":{"id":"1*qNUGFMhlnl0-mNLIVvyGAg.png","originalHeight":95,"originalWidth":459,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_277":{"id":"5c5cdcc6bd59_277","name":"bca7","type":"P","href":null,"layout":null,"metadata":null,"text":"The reason we use the analogy of a conveyer belt is because information can flow through a cell super super easily. It’s theoretically possible for information to speed past a single cell state without being modified at all. In fact, I think the term ‘modified’ is a really strong one here. With vanilla RNNs, each hidden state takes all the information it has from before and fully transforms it by applying a function over it. LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_277.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_277.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_277.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_277.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_277.markups.0":{"type":"STRONG","start":383,"end":393,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_277.markups.1":{"type":"STRONG","start":480,"end":493,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_277.markups.2":{"type":"EM","start":383,"end":393,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_277.markups.3":{"type":"EM","start":480,"end":493,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_278":{"id":"5c5cdcc6bd59_278","name":"eca6","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*I_nQdhxdoDa7KrZBTFeHSQ.png","typename":"ImageMetadata"},"text":"Ew. Vanilla RNNs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*I_nQdhxdoDa7KrZBTFeHSQ.png":{"id":"1*I_nQdhxdoDa7KrZBTFeHSQ.png","originalHeight":241,"originalWidth":330,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_279":{"id":"5c5cdcc6bd59_279","name":"72ea","type":"P","href":null,"layout":null,"metadata":null,"text":"Vanilla RNNs look something like this. And it’s why the vanishing gradient problem exists; during backprop, gradients cannot flow back past these transformations easily, because the tanh derivatives and weights chain up and multiply together and tend to zero. We then add up or average all these gradients that are basically zero, and we get zero.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_280":{"id":"5c5cdcc6bd59_280","name":"9b8e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*360GYNV8kyF5ATWefrSasA.png","typename":"ImageMetadata"},"text":"LSTMs 💦 💦 💦","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*360GYNV8kyF5ATWefrSasA.png":{"id":"1*360GYNV8kyF5ATWefrSasA.png","originalHeight":276,"originalWidth":450,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_281":{"id":"5c5cdcc6bd59_281","name":"0a23","type":"P","href":null,"layout":null,"metadata":null,"text":"This is an extreme a simplification — and I’ll go on to fill in the blanks later — but it’s sort of what an LSTM looks like. The previous timestep’s cell state value flows through and instead of transforming the information, we tweak it by adding (another vector) to it. The added term is some function ƒw of previous information, but this is not the same function as with vanilla RNNs — it’s heavily changed to make sense in this context (more on that soon), do more interesting things, and also reduce the vanishing gradient problem.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_281.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_281.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_281.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_281.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_281.markups.0":{"type":"STRONG","start":303,"end":306,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_281.markups.1":{"type":"STRONG","start":343,"end":346,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_281.markups.2":{"type":"EM","start":240,"end":247,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_281.markups.3":{"type":"EM","start":343,"end":346,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_282":{"id":"5c5cdcc6bd59_282","name":"ba34","type":"P","href":null,"layout":null,"metadata":null,"text":"Another neat way to think of it is like a live circuit: there are two paths where information, like current, can flow through. After the information passes through ƒw, it’s added to the information flowing towards c_t. Thus, in equation form it could look something like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_282.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_282.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_282.markups.0":{"type":"STRONG","start":163,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_282.markups.1":{"type":"STRONG","start":214,"end":217,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_283":{"id":"5c5cdcc6bd59_283","name":"ea17","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*qGqSrpJmO5h6ZGeIT7RK3w.png","typename":"ImageMetadata"},"text":"Again… sort of. We’ll get into the actual equations soon. This is a good proxy to convey my point.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_283.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_283.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qGqSrpJmO5h6ZGeIT7RK3w.png":{"id":"1*qGqSrpJmO5h6ZGeIT7RK3w.png","originalHeight":39,"originalWidth":208,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_283.markups.0":{"type":"STRONG","start":35,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_283.markups.1":{"type":"EM","start":41,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_284":{"id":"5c5cdcc6bd59_284","name":"c99b","type":"P","href":null,"layout":null,"metadata":null,"text":"With a bit of substitution, we can expand this to:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_285":{"id":"5c5cdcc6bd59_285","name":"2268","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OZs7rDSty0VhDhzTLH4Dgg.png","typename":"ImageMetadata"},"text":"Technically, this could expand even more, if you did some sort of recursive substitution to fully simplify the unrolled recurrence formula. You could express c_t for some large value of t as a really really really really long function of, ultimately, c_1.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_285.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_285.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_285.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OZs7rDSty0VhDhzTLH4Dgg.png":{"id":"1*OZs7rDSty0VhDhzTLH4Dgg.png","originalHeight":39,"originalWidth":444,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_285.markups.0":{"type":"STRONG","start":158,"end":162,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_285.markups.1":{"type":"STRONG","start":186,"end":188,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_285.markups.2":{"type":"STRONG","start":251,"end":254,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_286":{"id":"5c5cdcc6bd59_286","name":"48be","type":"P","href":null,"layout":null,"metadata":null,"text":"Why is this better? Well, if you have basic differentiation knowledge, you’ll know that addition distributes gradients equally. When we take the derivative of this whole expression, it’ll become a long addition of the derivatives of individual terms. As Andrej Karpathy puts it, this additive interaction creates “gradient super-highways”, where gradients can flow back super easily.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_287":{"id":"5c5cdcc6bd59_287","name":"740d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*n26drGfEkc-Xnmqc2Lw7cw.png","typename":"ImageMetadata"},"text":"Look — it’s a long conveyer belt! (In a sense, we can use this conveyor belt analogy for the whole unrolled LSTM as well. Each cell state is a subsection of the conveyer belt.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_287.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_287.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*n26drGfEkc-Xnmqc2Lw7cw.png":{"id":"1*n26drGfEkc-Xnmqc2Lw7cw.png","originalHeight":690,"originalWidth":2161,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_287.markups.0":{"type":"STRONG","start":93,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_287.markups.1":{"type":"EM","start":93,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_288":{"id":"5c5cdcc6bd59_288","name":"2d87","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*szBIWNdr0O0doBI8rfGjzw.png","typename":"ImageMetadata"},"text":"Look — it’s an outdated machine learning algorithm!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*szBIWNdr0O0doBI8rfGjzw.png":{"id":"1*szBIWNdr0O0doBI8rfGjzw.png","originalHeight":193,"originalWidth":610,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_289":{"id":"5c5cdcc6bd59_289","name":"68da","type":"P","href":null,"layout":null,"metadata":null,"text":"In the former, gradients are always added together, never multiplied. In the latter, gradients are always multiplied. Thus, in the former, when we inject a gradient at the end, it’ll easily flow back all the way to the beginning. Contributions by the ƒw function will be made to this gradient flowing on the bottom conveyer belt as well.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_289.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_289.markups.0":{"type":"STRONG","start":251,"end":254,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_290":{"id":"5c5cdcc6bd59_290","name":"779d","type":"P","href":null,"layout":null,"metadata":null,"text":"This is what gradient flow would look like:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_291":{"id":"5c5cdcc6bd59_291","name":"8074","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*dqOCXyepO590ORWV3VgBKw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*dqOCXyepO590ORWV3VgBKw.png":{"id":"1*dqOCXyepO590ORWV3VgBKw.png","originalHeight":262,"originalWidth":740,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_292":{"id":"5c5cdcc6bd59_292","name":"7ea0","type":"P","href":null,"layout":null,"metadata":null,"text":"Before, we discussed that when multiplicative interaction exists between gradients, the gradients either vanish (if they are mostly \u003C 1, as is usually the case for us) or explode (if they are mostly \u003E 1). Here’s some real calculus to demonstrate this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_293":{"id":"5c5cdcc6bd59_293","name":"6f86","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*09oGK1btsVezIoMyBAwqbw.png","typename":"ImageMetadata"},"text":"Former is akin to RNNs. Latter is akin to LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*09oGK1btsVezIoMyBAwqbw.png":{"id":"1*09oGK1btsVezIoMyBAwqbw.png","originalHeight":288,"originalWidth":287,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_294":{"id":"5c5cdcc6bd59_294","name":"0175","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine f being any sort of function, like our ƒw. When we apply a function to itself repeatedly, the chain rule shows that the overall derivative is the multiplication of multiple different derivative terms. But, when we add functions together, the derivative is simply the addition of the individual derivatives. This won’t vanish or explode quickly, so our LSTMs won’t vanish or explode quickly. Yay!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_294.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_294.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_294.markups.0":{"type":"STRONG","start":8,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_294.markups.1":{"type":"STRONG","start":47,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_295":{"id":"5c5cdcc6bd59_295","name":"4e50","type":"P","href":null,"layout":null,"metadata":null,"text":"Furthermore, if some of our gradients vanish — for whatever reason — then it should still be OK. It won’t be optimal, but since our gradient terms add together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). Look: 2 + 0 = 2 but 2 × 0 = 0.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_296":{"id":"5c5cdcc6bd59_296","name":"bbb5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*K7rYONPTfcpCb0xTvO3ydw.jpeg","typename":"ImageMetadata"},"text":"A gradient super highway? Sounds good to me! http:\u002F\u002Fwww.dyoung.com\u002Fassets\u002Fimages\u002FArticles%20images\u002Farticle4_PPH.jpg","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_296.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*K7rYONPTfcpCb0xTvO3ydw.jpeg":{"id":"1*K7rYONPTfcpCb0xTvO3ydw.jpeg","originalHeight":240,"originalWidth":466,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_296.markups.0":{"type":"A","start":45,"end":115,"href":"http:\u002F\u002Fwww.dyoung.com\u002Fassets\u002Fimages\u002FArticles%20images\u002Farticle4_PPH.jpg","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_297":{"id":"5c5cdcc6bd59_297","name":"a58a","type":"P","href":null,"layout":null,"metadata":null,"text":"So far, we haven’t really explored LSTMs. We’ve more setup a foundation for them. And there’s one glaring issue with our foundation: if we just keep adding information to cell state, it could just grow and grow and grow, and essentially act as a counter that only increments. This is not very useful, and could regularly lead to explosion. We want more fine and rich control over memory. Well, worry not, because this is exactly what LSTMs are capable of doing.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_297.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_297.markups.0":{"type":"EM","start":19,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_298":{"id":"5c5cdcc6bd59_298","name":"7ba6","type":"P","href":null,"layout":null,"metadata":null,"text":"LSTM cells handle memory in a very intelligent way, enabling them to learn long-term dependencies and perform well. How, exactly? Well, the cell is sort of like an internal memory state that allows for context; it “forgets”, a.k.a. resets, information it doesn’t find useful from the previous cell state, “writes” in new information it does find useful from the current input and\u002For previous hidden state, and similarly only “reads” out part of its information — the good stuff — in the computation of h_t. This respectively corresponds to the concepts of: resetting memory, writing to memory, and reading from memory. Very similar to how a modern computer system works, and we often describe an LSTM cell as a “memory cell”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_298.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_298.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_298.markups.0":{"type":"STRONG","start":502,"end":505,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_298.markups.1":{"type":"EM","start":336,"end":340,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_299":{"id":"5c5cdcc6bd59_299","name":"6706","type":"P","href":null,"layout":null,"metadata":null,"text":"The “writing to memory” part is additive — it’s what I showed you in the initial diagrams. Information flows through and we add stuff we think is relevant to it. The “resetting memory” part is multiplicative, and occurs before writing to memory; when information from the previous cell state initially flows in, we multiply it by a vector with values between 0 and 1 to reset or retain parts of it we find useless and useful respectively. The “reading from memory” part is also multiplicative with a similar 0–1 range vector, but it doesn’t modify the information flowing through the cell states. Rather, it modifies the information flowing into the hidden states and thus decides what the hidden state is influenced by.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_300":{"id":"5c5cdcc6bd59_300","name":"6813","type":"P","href":null,"layout":null,"metadata":null,"text":"Both of these multiplications are element wise, like so:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_300.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_300.markups.0":{"type":"EM","start":34,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_301":{"id":"5c5cdcc6bd59_301","name":"2255","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*YuIuYxt0oYEvGMoTz_J59g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*YuIuYxt0oYEvGMoTz_J59g.png":{"id":"1*YuIuYxt0oYEvGMoTz_J59g.png","originalHeight":82,"originalWidth":167,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_302":{"id":"5c5cdcc6bd59_302","name":"c869","type":"P","href":null,"layout":null,"metadata":null,"text":"In this equation, when a = 0 the information of c is lost. This is what resetting does, and retaining is the vice versa. I also imagine that values such as 0.5 could be used to diminish the importance of certain information, but not completely wipe it out.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_302.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_302.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_302.markups.0":{"type":"STRONG","start":23,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_302.markups.1":{"type":"STRONG","start":48,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_303":{"id":"5c5cdcc6bd59_303","name":"c920","type":"P","href":null,"layout":null,"metadata":null,"text":"Our (unfinished) cell state computational graph now looks like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_304":{"id":"5c5cdcc6bd59_304","name":"f4a0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_mbUA8vdaTbYreXpdPJccA.png","typename":"ImageMetadata"},"text":"This is closer to what an LSTM looks like, though we’re not exactly there yet.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_mbUA8vdaTbYreXpdPJccA.png":{"id":"1*_mbUA8vdaTbYreXpdPJccA.png","originalHeight":158,"originalWidth":419,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_305":{"id":"5c5cdcc6bd59_305","name":"ba90","type":"P","href":null,"layout":null,"metadata":null,"text":"Sidenote: don’t be scared whenever you see the word “multiplicative” and don’t immediately think of “vanishing” or “exploding”. It depends on the context. Here, as I’ll show mathematically in a bit, it’s fine.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_306":{"id":"5c5cdcc6bd59_306","name":"9705","type":"P","href":null,"layout":null,"metadata":null,"text":"This concept in general is known as gating, because we “gate” what can flow in and out of the LSTM cell. What we actually multiply and add by to reset, write, and read are known as the “gates”. There are four such gates:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_306.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_306.markups.0":{"type":"STRONG","start":36,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_307":{"id":"5c5cdcc6bd59_307","name":"dafe","type":"ULI","href":null,"layout":null,"metadata":null,"text":"f: forget gate. This is the “reset” tool that wipes out, diminishes, or retains information from the previous cell state. It’s the first interaction we make, and it’s multiplicative. That is, we multiply it with the cell state. The sigmoid function is used to compute the forget gate such that its values can be in the range 0 to 1. When a value is 1, we “remember” something, and when it is 0 we “forget”. We might choose to forget, for example, when see a period or some sort of end of sentence marker. This is counterintuitive… I guess it should really be called the “remember gate”!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_307.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_307.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_307.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_307.markups.0":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_307.markups.1":{"type":"EM","start":3,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_307.markups.2":{"type":"EM","start":571,"end":584,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_308":{"id":"5c5cdcc6bd59_308","name":"aeb2","type":"ULI","href":null,"layout":null,"metadata":null,"text":"g: ?. This gate doesn’t really have a name, but it’s partly responsible for the “write” process. It stores a value between -1 and 1 that represents how much we want to add to the cell state by, and represents the input to the cell state. It’s computed with the tanh function. We apply a bounded function to it such that the cell state acts as a stable counter, and it also introduces more complexity. (And it works well.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_308.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_308.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_308.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_308.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_308.markups.0":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_308.markups.1":{"type":"STRONG","start":2,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_308.markups.2":{"type":"STRONG","start":5,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_308.markups.3":{"type":"EM","start":2,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309":{"id":"5c5cdcc6bd59_309","name":"0612","type":"ULI","href":null,"layout":null,"metadata":null,"text":"i: input gate. This is the other gate responsible for the “write” process. It controls how much of g we “let in”, and is thus between 0 and 1, computed with sigmoid. It’s similar to the forget gate in this sense, in that it blocks input like the forget gate blocks the incoming cell state. We multiply i by g and add this to the cell state. Since i is in the range 0 to 1, and g is in the range -1 to 1, we add a value between -1 and 1 to the cell state. Intuitively, this sort of acts as decrementing or incrementing the counter.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_309.markups.13","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_309.markups.0":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.1":{"type":"STRONG","start":2,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.2":{"type":"STRONG","start":99,"end":100,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.3":{"type":"STRONG","start":302,"end":304,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.4":{"type":"STRONG","start":307,"end":309,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.5":{"type":"STRONG","start":340,"end":341,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.6":{"type":"STRONG","start":347,"end":349,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.7":{"type":"STRONG","start":377,"end":378,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.8":{"type":"EM","start":2,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.9":{"type":"EM","start":99,"end":100,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.10":{"type":"EM","start":302,"end":303,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.11":{"type":"EM","start":307,"end":309,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.12":{"type":"EM","start":347,"end":349,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_309.markups.13":{"type":"EM","start":377,"end":378,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310":{"id":"5c5cdcc6bd59_310","name":"57eb","type":"ULI","href":null,"layout":null,"metadata":null,"text":"o: output gate. This is also passed through sigmoid, and is a number between 0 and 1 that modulates which aspects the hidden state can draw from the cell state. It enables the “read from memory” operation. It multiplies with the tanh of the cell state to compute the hidden state. So, I didn’t bring this up before, but the cell state leaks into a tanh before h_t is computed.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_310.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_310.markups.0":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.1":{"type":"STRONG","start":15,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.2":{"type":"STRONG","start":233,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.3":{"type":"STRONG","start":360,"end":363,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.4":{"type":"EM","start":2,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.5":{"type":"EM","start":15,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_310.markups.6":{"type":"EM","start":233,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_311":{"id":"5c5cdcc6bd59_311","name":"5cb0","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s our updated computational graph for the cell state:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_312":{"id":"5c5cdcc6bd59_312","name":"d768","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*3xq3p-nVgxQXXPSXueVWdw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*3xq3p-nVgxQXXPSXueVWdw.png":{"id":"1*3xq3p-nVgxQXXPSXueVWdw.png","originalHeight":251,"originalWidth":533,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_313":{"id":"5c5cdcc6bd59_313","name":"01f6","type":"P","href":null,"layout":null,"metadata":null,"text":"Looks like I’m starting to create a complex diagram of my own. Damn. 😞 I guess LSTMs and immediately interpretable diagrams just weren’t meant to be!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_314":{"id":"5c5cdcc6bd59_314","name":"7eb5","type":"P","href":null,"layout":null,"metadata":null,"text":"Basically, f interacts with the cell state through a multiplication. i interacts with g through a multiplication as well, the result of which interacts with the cell state through an addition. Finally, the cell state leaks into a tanh (that’s the shape of the tanh function in the circle), the result of which then interacts with o through multiplication to compute h_t. This does not disrupt the cell state, which flows to the next timestep. h_t then flows forward (and it could flow upward as well).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_314.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_314.markups.0":{"type":"STRONG","start":11,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_314.markups.1":{"type":"STRONG","start":69,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_314.markups.2":{"type":"STRONG","start":86,"end":88,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_314.markups.3":{"type":"STRONG","start":330,"end":332,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_314.markups.4":{"type":"STRONG","start":366,"end":369,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_314.markups.5":{"type":"STRONG","start":443,"end":447,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_315":{"id":"5c5cdcc6bd59_315","name":"e614","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s the equation form:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_316":{"id":"5c5cdcc6bd59_316","name":"94fe","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*B9Qd1pW1kYM_zcg0IhPfUA.png","typename":"ImageMetadata"},"text":"Each gate should actually be indexed by timestep t — we’ll see why soon.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_316.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*B9Qd1pW1kYM_zcg0IhPfUA.png":{"id":"1*B9Qd1pW1kYM_zcg0IhPfUA.png","originalHeight":84,"originalWidth":222,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_316.markups.0":{"type":"STRONG","start":49,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_317":{"id":"5c5cdcc6bd59_317","name":"2518","type":"P","href":null,"layout":null,"metadata":null,"text":"As you can see, our cell state has no activation function; the activation function is simply the identity function! Yet, the cell state usually doesn’t explode — it stays stable by “forgetting” and “writing”, and does interesting things with this gating to promote context, fine control over memory, and long-term dependency learning.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_318":{"id":"5c5cdcc6bd59_318","name":"b30a","type":"P","href":null,"layout":null,"metadata":null,"text":"So, how are the gates calculated? Well, all of these gates have their own learnable weights and are functions of the last timestep’s hidden state flowing in and any current timestep inputs, not the cell state (contrary to what I may have implied earlier with the gradient flow diagrams). This should make sense when you think about it; I mean, firstly, the g and i gates literally represent input, so they better be functionally dependent on hidden states and input data! On an intuitive level, the gates help us modify the cell state, and we modify the cell state based on our current context. External stimulus that provide context should be used to compute these gates, and since context = input + hidden states our gates are functionally dependent on input and hidden states.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_318.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_318.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_318.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_318.markups.0":{"type":"STRONG","start":357,"end":359,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_318.markups.1":{"type":"STRONG","start":363,"end":365,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_318.markups.2":{"type":"EM","start":133,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_319":{"id":"5c5cdcc6bd59_319","name":"27fb","type":"P","href":null,"layout":null,"metadata":null,"text":"Since every gate has a different value at each timestep, we index by timestep t just like for hidden states, cell states, or something similar.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_319.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_319.markups.0":{"type":"STRONG","start":78,"end":80,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_320":{"id":"5c5cdcc6bd59_320","name":"6fb5","type":"P","href":null,"layout":null,"metadata":null,"text":"We could generalize for multiple hidden layers as well:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_321":{"id":"5c5cdcc6bd59_321","name":"53d0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*BWz6E_IFi6UTLkSNSoq1Yg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BWz6E_IFi6UTLkSNSoq1Yg.png":{"id":"1*BWz6E_IFi6UTLkSNSoq1Yg.png","originalHeight":41,"originalWidth":290,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_322":{"id":"5c5cdcc6bd59_322","name":"1ae0","type":"P","href":null,"layout":null,"metadata":null,"text":"But, for simplicity’s sake, let’s assume we are at the first hidden layer, or that there is only one hidden layer in the LSTM. This way, we can obfuscate the ℓ term and ignore influence from hidden states in the previous depth. We’ll also forget about edge cases and assume input exists at the current timestep. In practice, we obviously can’t make said assumptions, but for the sake of demonstrating the equations it becomes too tedious otherwise.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_322.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_322.markups.0":{"type":"STRONG","start":158,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_323":{"id":"5c5cdcc6bd59_323","name":"c042","type":"P","href":null,"layout":null,"metadata":null,"text":"Sidenote: we make this assumption for the rest of the discussion on LSTMs in this article.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_323.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_323.markups.0":{"type":"STRONG","start":0,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_324":{"id":"5c5cdcc6bd59_324","name":"5a86","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*hP6I692iv7oc6AkcWINDaw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*hP6I692iv7oc6AkcWINDaw.png":{"id":"1*hP6I692iv7oc6AkcWINDaw.png","originalHeight":180,"originalWidth":384,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_325":{"id":"5c5cdcc6bd59_325","name":"033c","type":"P","href":null,"layout":null,"metadata":null,"text":"Like with the RNN hidden state, the index of each weight matrix is descriptive; for example, W_xf are the weights that map input x to the forget gate f. Each gate has weight matrices that map input and hidden states to itself, including biases.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_325.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_325.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_325.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_325.markups.0":{"type":"STRONG","start":93,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_325.markups.1":{"type":"STRONG","start":129,"end":131,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_325.markups.2":{"type":"STRONG","start":150,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_326":{"id":"5c5cdcc6bd59_326","name":"6989","type":"P","href":null,"layout":null,"metadata":null,"text":"And this is the beauty of LSTMs; the whole thing is end-to-end differentiable. These gates can learn when to allow data to flow and what data should flow depending on the context it sees (the input and the hidden states). It learns this based on patterns it sees while training. In this sense, it’s sort of like how a CNN learns feature detectors for images, but the patterns are way more complex and less human interpretable with LSTMs. This is why they perform so well.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_326.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_326.markups.0":{"type":"EM","start":95,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_327":{"id":"5c5cdcc6bd59_327","name":"1be4","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*VJL6ONtLK77GpO2XmFCH7g.png","typename":"ImageMetadata"},"text":"😨 😱 😰 : perhaps your immediate reaction.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_327.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*VJL6ONtLK77GpO2XmFCH7g.png":{"id":"1*VJL6ONtLK77GpO2XmFCH7g.png","originalHeight":321,"originalWidth":551,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_327.markups.0":{"type":"STRONG","start":0,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_328":{"id":"5c5cdcc6bd59_328","name":"b263","type":"P","href":null,"layout":null,"metadata":null,"text":"Okay, this looks scarier, but it’s actually not much different to what we had before, especially once you look past the intimidating web of arrows. One notable change is that we’re showing the previous hidden state in time and the current input flowing in. This diagram makes the assumption that we’re in the first layer and at some timestep \u003E 1 where input exists. We then show how the f, i, g, and o gates are computed from this information — the hidden state and inputs are fed into an activation function like sigmoid (or, for g, a tanh; you can tell because it’s double the height of the others) — and it’s expressed through the web of arrows. It’s implied that we weight the two terms entering our activation functions, adding them up with a bias vector, but it’s not necessarily explicit in the diagram.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_328.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_328.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_328.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_328.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_328.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_328.markups.0":{"type":"STRONG","start":387,"end":388,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_328.markups.1":{"type":"STRONG","start":390,"end":391,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_328.markups.2":{"type":"STRONG","start":393,"end":396,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_328.markups.3":{"type":"STRONG","start":400,"end":401,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_328.markups.4":{"type":"STRONG","start":531,"end":532,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_329":{"id":"5c5cdcc6bd59_329","name":"b56c","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s embed this into our overall LSTM diagram for a single timestep:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_330":{"id":"5c5cdcc6bd59_330","name":"689a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*0h88NXeFxkb-xD1rBq4lgA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0h88NXeFxkb-xD1rBq4lgA.png":{"id":"1*0h88NXeFxkb-xD1rBq4lgA.png","originalHeight":349,"originalWidth":770,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_331":{"id":"5c5cdcc6bd59_331","name":"147c","type":"P","href":null,"layout":null,"metadata":null,"text":"Now let’s zoom out and view our entire unrolled single layer, three timestep LSTM:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_332":{"id":"5c5cdcc6bd59_332","name":"4127","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-lhIk-yAsXk88gcPvEeIRQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-lhIk-yAsXk88gcPvEeIRQ.png":{"id":"1*-lhIk-yAsXk88gcPvEeIRQ.png","originalHeight":1204,"originalWidth":4225,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_333":{"id":"5c5cdcc6bd59_333","name":"c9d0","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s beautiful, isn’t it? The full screen width size just adds to the effect! Here’s a link to the full res version.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_333.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_333.markups.0":{"type":"A","start":78,"end":84,"href":"https:\u002F\u002Fdrive.google.com\u002Ffile\u002Fd\u002F0BwbWRPtraa2zQUsydXRKSkd3YUU\u002Fview?usp=sharing","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_334":{"id":"5c5cdcc6bd59_334","name":"c678","type":"P","href":null,"layout":null,"metadata":null,"text":"The only thing that would look more beautiful would be multiple LSTM cells that stack on top of each other (multiple hidden layers)! 😍","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_335":{"id":"5c5cdcc6bd59_335","name":"e117","type":"H3","href":null,"layout":null,"metadata":null,"text":"Fixing the problem with LSTMs (Part II)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_336":{"id":"5c5cdcc6bd59_336","name":"f89a","type":"P","href":null,"layout":null,"metadata":null,"text":"You’ve come a long way, young padawan. But there’s still a bit left to go. Part I focused on the motivation for LSTMs, how they work, and a bit on why they reduce the vanishing gradient problem. Now, having a full understanding of LSTMs, Part II will hone in on the latter part—analyzing on a more close, technical level why our gradients stop vanishing as quickly. You won’t find a lot of this information online easily; I had to search and ask left and right to find an explanation better and more comprehensive than what you’ll find in other current tutorials.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_337":{"id":"5c5cdcc6bd59_337","name":"a182","type":"P","href":null,"layout":null,"metadata":null,"text":"Firstly, truncated BPTT is often used with LSTMs; it’s a method to speed up training. In particular, note that if we input a sequence of length 1000 into an LSTM, and want to train it, it’s equivalent to training a 1000 layer neural network. Doing forward and backwards passes into this is very memory and time consuming, especially while backpropagating the error when we need to compute a derivative like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_338":{"id":"5c5cdcc6bd59_338","name":"7285","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*UqC4IRIfcDfoiwD8zvqW2A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UqC4IRIfcDfoiwD8zvqW2A.png":{"id":"1*UqC4IRIfcDfoiwD8zvqW2A.png","originalHeight":79,"originalWidth":87,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_339":{"id":"5c5cdcc6bd59_339","name":"06fe","type":"P","href":null,"layout":null,"metadata":null,"text":"…which would include a lot of terms.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_339.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_339.markups.0":{"type":"EM","start":23,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_340":{"id":"5c5cdcc6bd59_340","name":"792f","type":"P","href":null,"layout":null,"metadata":null,"text":"When we backprop the error, and add all the gradients up, this is what we get:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_341":{"id":"5c5cdcc6bd59_341","name":"b325","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ucOvP6wOs9MHzH8WKiykbQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ucOvP6wOs9MHzH8WKiykbQ.png":{"id":"1*ucOvP6wOs9MHzH8WKiykbQ.png","originalHeight":79,"originalWidth":675,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_342":{"id":"5c5cdcc6bd59_342","name":"9c5f","type":"P","href":null,"layout":null,"metadata":null,"text":"Truncated BPTT does two things:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_343":{"id":"5c5cdcc6bd59_343","name":"5bfb","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Instead of doing a forward pass on the whole sequence and then doing a backwards pass, we process the sequence timestep by timestep and do a backwards pass “every so often”. That is — we compute h_1 and c_1, then h_2 and c_2, then h_3 and c_3, and then at some point in time, quantified by k1, we do a backwards pass. Every k1 timesteps, we perform BPTT; if k1 = 10, for example, then once we compute h_10 and c_10 we perform BPTT. Same for h_20 and c_20, and so on so forth. When we perform the backwards pass, our error J won’t be the same as if we did a full forwards pass and full backwards pass, since we haven’t observed all the outputs yet—we wouldn’t have even computed all the potential outputs yet! Instead, the error describes what we’ve observed and computed so far, because we process the sequence timestep by timestep. Intuitively, it’s like we train on a small subset of the training sequence, and this subset increases in length each time, which enables us to continue learning long-term dependencies. We could denote the error at timestep t —where t is a multiple of k1 — with truncated backprop as J^t. So:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.16","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_343.markups.17","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_343.markups.0":{"type":"STRONG","start":195,"end":199,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.1":{"type":"STRONG","start":203,"end":206,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.2":{"type":"STRONG","start":213,"end":217,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.3":{"type":"STRONG","start":221,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.4":{"type":"STRONG","start":231,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.5":{"type":"STRONG","start":239,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.6":{"type":"STRONG","start":290,"end":294,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.7":{"type":"STRONG","start":324,"end":327,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.8":{"type":"STRONG","start":358,"end":365,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.9":{"type":"STRONG","start":401,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.10":{"type":"STRONG","start":410,"end":415,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.11":{"type":"STRONG","start":441,"end":446,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.12":{"type":"STRONG","start":450,"end":454,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.13":{"type":"STRONG","start":522,"end":524,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.14":{"type":"STRONG","start":1056,"end":1057,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.15":{"type":"STRONG","start":1065,"end":1067,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.16":{"type":"STRONG","start":1084,"end":1086,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_343.markups.17":{"type":"STRONG","start":1115,"end":1119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_344":{"id":"5c5cdcc6bd59_344","name":"dd7e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Br6EoWvUmGTNoX3NqkZVpA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Br6EoWvUmGTNoX3NqkZVpA.png":{"id":"1*Br6EoWvUmGTNoX3NqkZVpA.png","originalHeight":80,"originalWidth":445,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_345":{"id":"5c5cdcc6bd59_345","name":"eb5a","type":"P","href":null,"layout":null,"metadata":null,"text":"For example, if t = 20 and k1 = 10, our second (because 20 ÷ 10 = 2) round of BPTT would be:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_345.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_345.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_345.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_345.markups.0":{"type":"STRONG","start":16,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_345.markups.1":{"type":"STRONG","start":27,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_345.markups.2":{"type":"EM","start":40,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_346":{"id":"5c5cdcc6bd59_346","name":"6f7d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pg91-TmNosH9B7Py0wjCdQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pg91-TmNosH9B7Py0wjCdQ.png":{"id":"1*pg91-TmNosH9B7Py0wjCdQ.png","originalHeight":80,"originalWidth":667,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_347":{"id":"5c5cdcc6bd59_347","name":"f441","type":"ULI","href":null,"layout":null,"metadata":null,"text":"On top of this, instead of backpropagating from J^t all the way to the first timestep c_1, we set a cut-off point. This cut-off point, quantified by k2, is the timestep at which our cell states stop contributing to the overall gradient. For example, if k2 = 10, and we’re backpropagating at t = 20, then c_10 is the final cell state to contribute to the overall gradient. Everything before c_10 will have no say. This is designed such that we avoid computing derivatives between cell states far apart in time, which would include a huge number of terms (as mentioned earlier). The equation is now:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_347.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_347.markups.0":{"type":"STRONG","start":48,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.1":{"type":"STRONG","start":86,"end":89,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.2":{"type":"STRONG","start":149,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.3":{"type":"STRONG","start":253,"end":260,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.4":{"type":"STRONG","start":291,"end":297,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.5":{"type":"STRONG","start":304,"end":309,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_347.markups.6":{"type":"STRONG","start":390,"end":395,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_348":{"id":"5c5cdcc6bd59_348","name":"73ca","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GmP4nvsdBTyyRwo7ffRW2g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GmP4nvsdBTyyRwo7ffRW2g.png":{"id":"1*GmP4nvsdBTyyRwo7ffRW2g.png","originalHeight":80,"originalWidth":515,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_349":{"id":"5c5cdcc6bd59_349","name":"956c","type":"P","href":null,"layout":null,"metadata":null,"text":"So, with t = 20, k2 = 10, and k1 = 10, our second round of BPTT would follow:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_349.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_349.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_349.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_349.markups.0":{"type":"STRONG","start":9,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_349.markups.1":{"type":"STRONG","start":17,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_349.markups.2":{"type":"STRONG","start":30,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_350":{"id":"5c5cdcc6bd59_350","name":"12a1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*WyHlRZljjmHEaFKsGg0JQg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WyHlRZljjmHEaFKsGg0JQg.png":{"id":"1*WyHlRZljjmHEaFKsGg0JQg.png","originalHeight":80,"originalWidth":674,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_351":{"id":"5c5cdcc6bd59_351","name":"a297","type":"P","href":null,"layout":null,"metadata":null,"text":"Both k1 and k2 are hyperparameters. k1 does not have to equal k2.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_351.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_351.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_351.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_351.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_351.markups.0":{"type":"STRONG","start":5,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_351.markups.1":{"type":"STRONG","start":12,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_351.markups.2":{"type":"STRONG","start":36,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_351.markups.3":{"type":"STRONG","start":62,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_352":{"id":"5c5cdcc6bd59_352","name":"93ed","type":"P","href":null,"layout":null,"metadata":null,"text":"These two techniques combined enables truncated BPTT to not lose the ability to learn long term dependencies. Here’s a formal definition:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_353":{"id":"5c5cdcc6bd59_353","name":"72ee","type":"BQ","href":null,"layout":null,"metadata":null,"text":"[Truncated BPTT] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps, so a parameter update can be cheap if k2 is small. Consequently, its hidden states have been exposed to many timesteps and so may contain useful information about the far past, which would be opportunistically exploited.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_353.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_353.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_353.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_353.markups.0":{"type":"STRONG","start":74,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_353.markups.1":{"type":"STRONG","start":105,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_353.markups.2":{"type":"STRONG","start":157,"end":159,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_354":{"id":"5c5cdcc6bd59_354","name":"ae91","type":"BQ","href":null,"layout":null,"metadata":null,"text":"— “Training Recurrent Neural Networks”, 2.8.6, Page 23","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_354.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_354.markups.0":{"type":"A","start":2,"end":38,"href":"http:\u002F\u002Fwww.cs.utoronto.ca\u002F~ilya\u002Fpubs\u002Filya_sutskever_phd_thesis.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_355":{"id":"5c5cdcc6bd59_355","name":"28a4","type":"P","href":null,"layout":null,"metadata":null,"text":"The same paper gives nice pseudocode for truncated BPTT:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_356":{"id":"5c5cdcc6bd59_356","name":"f5d9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*0SnUb2iYt1RNa7JsGG-7gQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0SnUb2iYt1RNa7JsGG-7gQ.png":{"id":"1*0SnUb2iYt1RNa7JsGG-7gQ.png","originalHeight":113,"originalWidth":413,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_357":{"id":"5c5cdcc6bd59_357","name":"399f","type":"P","href":null,"layout":null,"metadata":null,"text":"The rest of the math in this section will not be in the context of using truncated backprop, because it’s a technique vs. something rooted in the mathematical foundation of LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_358":{"id":"5c5cdcc6bd59_358","name":"d546","type":"P","href":null,"layout":null,"metadata":null,"text":"Moving on — before, we saw this diagram:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_359":{"id":"5c5cdcc6bd59_359","name":"dc69","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*n26drGfEkc-Xnmqc2Lw7cw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_360":{"id":"5c5cdcc6bd59_360","name":"48f4","type":"P","href":null,"layout":null,"metadata":null,"text":"In this context, ƒw = i ⊙ g, because it’s the value we’re adding to the cell state.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_360.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_360.markups.0":{"type":"STRONG","start":17,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_361":{"id":"5c5cdcc6bd59_361","name":"db0e","type":"P","href":null,"layout":null,"metadata":null,"text":"But this diagram is a bit of a lie. Why? It ignores forget gates. So, does the presence of forget gates affect the vanishing gradient problem? Quite significantly, actually. How? Let’s bring up our cell state equation to see:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_362":{"id":"5c5cdcc6bd59_362","name":"4484","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*i6rbrX0k9mKLXewD4korCw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*i6rbrX0k9mKLXewD4korCw.png":{"id":"1*i6rbrX0k9mKLXewD4korCw.png","originalHeight":39,"originalWidth":236,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_363":{"id":"5c5cdcc6bd59_363","name":"09ae","type":"P","href":null,"layout":null,"metadata":null,"text":"With the forget gate, we now include a multiplicative interaction. Our new diagram will look like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_364":{"id":"5c5cdcc6bd59_364","name":"931c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*UVx1vL6ADQGTBeKSaWX7bw.png","typename":"ImageMetadata"},"text":"Do not confuse forget gate ƒ with function ƒw in this diagram. I know, it’s confusing… 😢","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_364.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_364.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UVx1vL6ADQGTBeKSaWX7bw.png":{"id":"1*UVx1vL6ADQGTBeKSaWX7bw.png","originalHeight":986,"originalWidth":2436,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_364.markups.0":{"type":"STRONG","start":27,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_364.markups.1":{"type":"STRONG","start":43,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_365":{"id":"5c5cdcc6bd59_365","name":"bf2c","type":"P","href":null,"layout":null,"metadata":null,"text":"When our gradients flow back, they will be affected by this multiplicative interaction. So, let’s compute the new derivative:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_366":{"id":"5c5cdcc6bd59_366","name":"0504","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*60XFfJvc0t9a0ekdMTAp_Q.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*60XFfJvc0t9a0ekdMTAp_Q.png":{"id":"1*60XFfJvc0t9a0ekdMTAp_Q.png","originalHeight":79,"originalWidth":113,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_367":{"id":"5c5cdcc6bd59_367","name":"f174","type":"P","href":null,"layout":null,"metadata":null,"text":"This seems super neat, actually. Obviously the gradient will be f, because f acts as a blocker and controls how much c_t-1 influences c_t; it’s the gate that you can fully or partially open and close that lets information from c_t-1 flow through! It’s just intuitive that it would propagate back perfectly.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_367.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_367.markups.0":{"type":"STRONG","start":64,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_367.markups.1":{"type":"STRONG","start":75,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_367.markups.2":{"type":"STRONG","start":117,"end":123,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_367.markups.3":{"type":"STRONG","start":134,"end":137,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_367.markups.4":{"type":"STRONG","start":227,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_367.markups.5":{"type":"EM","start":33,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368":{"id":"5c5cdcc6bd59_368","name":"d859","type":"P","href":null,"layout":null,"metadata":null,"text":"But, if you’ve payed close attention so far, you might be asking: “wait, what happened to ƒw’s contribution to the gradient?” If you’re a hardcore mathematician, you might also be worried that we’re content with leaving the gradient as just f. This is because the gates f, i, and g are all functions of c_t-1; they are functions of h_t-1, which is, in turn, a function of c_t-1! The diagram shows this visually, as well. It seems we’re failing to apply calculus properly. We’d need to backprop through f and through i ⊙ g to complete the derivative.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_368.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_368.markups.0":{"type":"STRONG","start":90,"end":92,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.1":{"type":"STRONG","start":125,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.2":{"type":"STRONG","start":241,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.3":{"type":"STRONG","start":270,"end":271,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.4":{"type":"STRONG","start":272,"end":274,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.5":{"type":"STRONG","start":280,"end":282,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.6":{"type":"STRONG","start":303,"end":308,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.7":{"type":"STRONG","start":332,"end":337,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.8":{"type":"STRONG","start":372,"end":377,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.9":{"type":"STRONG","start":502,"end":503,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.10":{"type":"STRONG","start":516,"end":522,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.11":{"type":"EM","start":67,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_368.markups.12":{"type":"EM","start":92,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_369":{"id":"5c5cdcc6bd59_369","name":"3f0c","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s walk through the differentiation to show why you’re actually not wrong, but neither am I:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_369.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_369.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_369.markups.0":{"type":"STRONG","start":76,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_369.markups.1":{"type":"EM","start":77,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_370":{"id":"5c5cdcc6bd59_370","name":"f0c6","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*x1mvDnbZOmZ1CnHJo23tZg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*x1mvDnbZOmZ1CnHJo23tZg.png":{"id":"1*x1mvDnbZOmZ1CnHJo23tZg.png","originalHeight":162,"originalWidth":380,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_371":{"id":"5c5cdcc6bd59_371","name":"80e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, with the first derivative, we need to apply product rule. Why? Because we’re differentiating the product of two functions of c_t-1. The former being the forget gate, and the latter being just c_t-1. Let’s do it:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_371.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_371.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_371.markups.0":{"type":"STRONG","start":130,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_371.markups.1":{"type":"STRONG","start":197,"end":202,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_372":{"id":"5c5cdcc6bd59_372","name":"b39b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TwjnkG6vtuIke1pkAzTzzA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TwjnkG6vtuIke1pkAzTzzA.png":{"id":"1*TwjnkG6vtuIke1pkAzTzzA.png","originalHeight":124,"originalWidth":271,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_373":{"id":"5c5cdcc6bd59_373","name":"00fd","type":"P","href":null,"layout":null,"metadata":null,"text":"Then, from product rule:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_374":{"id":"5c5cdcc6bd59_374","name":"416d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*cgq4UnWxun6rQ6H00gDzWg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*cgq4UnWxun6rQ6H00gDzWg.png":{"id":"1*cgq4UnWxun6rQ6H00gDzWg.png","originalHeight":164,"originalWidth":381,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_375":{"id":"5c5cdcc6bd59_375","name":"7dd1","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s the first derivative done. We purposely choose not to compute the derivative of the forget gate with respect to the previous cell state on previous. You’ll see why in a bit.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_376":{"id":"5c5cdcc6bd59_376","name":"e8f2","type":"P","href":null,"layout":null,"metadata":null,"text":"Now let’s tackle the second one:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_377":{"id":"5c5cdcc6bd59_377","name":"be20","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*1T0iaNg6vY4pEOz-ybkjuw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1T0iaNg6vY4pEOz-ybkjuw.png":{"id":"1*1T0iaNg6vY4pEOz-ybkjuw.png","originalHeight":77,"originalWidth":150,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_378":{"id":"5c5cdcc6bd59_378","name":"10b0","type":"P","href":null,"layout":null,"metadata":null,"text":"You’ll notice that it’s also two functions of c_t-1 multiplied together, so we use the product rule again:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_378.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_378.markups.0":{"type":"STRONG","start":46,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_379":{"id":"5c5cdcc6bd59_379","name":"f85e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*tVfrKCc1T7eRgypliDe19A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*tVfrKCc1T7eRgypliDe19A.png":{"id":"1*tVfrKCc1T7eRgypliDe19A.png","originalHeight":124,"originalWidth":320,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_380":{"id":"5c5cdcc6bd59_380","name":"618f","type":"P","href":null,"layout":null,"metadata":null,"text":"So:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_381":{"id":"5c5cdcc6bd59_381","name":"c8bf","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*s2nnva6Yhb2AuZDYYALbEA.png","typename":"ImageMetadata"},"text":"Once again, we purposely do not simplify the gate derivative terms.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*s2nnva6Yhb2AuZDYYALbEA.png":{"id":"1*s2nnva6Yhb2AuZDYYALbEA.png","originalHeight":164,"originalWidth":361,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_382":{"id":"5c5cdcc6bd59_382","name":"41e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Thus, our overall derivative becomes:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_383":{"id":"5c5cdcc6bd59_383","name":"7b20","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*o0dzU_s9WxoTYfOkQo1a0A.png","typename":"ImageMetadata"},"text":"Notice that the first term in this derivative is our forget gate.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*o0dzU_s9WxoTYfOkQo1a0A.png":{"id":"1*o0dzU_s9WxoTYfOkQo1a0A.png","originalHeight":79,"originalWidth":461,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_384":{"id":"5c5cdcc6bd59_384","name":"9307","type":"P","href":null,"layout":null,"metadata":null,"text":"Pay attention to the caption of the diagram.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_385":{"id":"5c5cdcc6bd59_385","name":"4712","type":"P","href":null,"layout":null,"metadata":null,"text":"This is actually our real derivative. Modern LSTM implementations just use an auto differentiation library to compute derivatives, so they’ll probably come up with this. However, effectively (or, rather, approximately), our gradient is just the forget gate, because the other three terms tend towards zero. Yup — they vanish. Why?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_385.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_385.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_385.markups.0":{"type":"EM","start":21,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_385.markups.1":{"type":"EM","start":179,"end":191,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_386":{"id":"5c5cdcc6bd59_386","name":"b5a4","type":"P","href":null,"layout":null,"metadata":null,"text":"When we backprop error in LSTMs, we backprop through cell states to propagate the error from the outputs to the cell state we want. For example, if we want to backprop the error from the output at time t down k timesteps, then we need to compute the derivative of the cell state at time t to the cell state at time t-k. Look what happens when we do that:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_386.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_386.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_386.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_386.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_386.markups.0":{"type":"STRONG","start":202,"end":203,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_386.markups.1":{"type":"STRONG","start":209,"end":211,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_386.markups.2":{"type":"STRONG","start":287,"end":289,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_386.markups.3":{"type":"STRONG","start":315,"end":318,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_387":{"id":"5c5cdcc6bd59_387","name":"db33","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*dBFbl6NCqp94Lnyb0taFWg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*dBFbl6NCqp94Lnyb0taFWg.png":{"id":"1*dBFbl6NCqp94Lnyb0taFWg.png","originalHeight":79,"originalWidth":642,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_388":{"id":"5c5cdcc6bd59_388","name":"57e3","type":"P","href":null,"layout":null,"metadata":null,"text":"We didn’t simplify the gate w.r.t. cell state derivatives for a reason; as we backpropagate through time, they begin to vanish. Thus, whatever they multiplied with is killed off from making contributions to the gradient, too. So, effectively:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_389":{"id":"5c5cdcc6bd59_389","name":"08ea","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Q9BJ7JxQ08YvfBW_OsJabw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Q9BJ7JxQ08YvfBW_OsJabw.png":{"id":"1*Q9BJ7JxQ08YvfBW_OsJabw.png","originalHeight":71,"originalWidth":458,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_390":{"id":"5c5cdcc6bd59_390","name":"e607","type":"P","href":null,"layout":null,"metadata":null,"text":"The rationale behind this is pretty simple, and we don’t need math for it; these gates are the outputs of non-linearities eg. sigmoid and tanh. If we were to get the derivative of them in getting our cell state derivative, then this derivative would contain the derivatives of sigmoid\u002Ftanh in them. But, just because we don’t need to use math to show this, doesn’t mean we don’t want to 😏 :","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_390.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_390.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_390.markups.0":{"type":"EM","start":326,"end":330,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_390.markups.1":{"type":"EM","start":379,"end":384,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_391":{"id":"5c5cdcc6bd59_391","name":"0dbb","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-mUDovQ8ovejmWPNoFSI1g.png","typename":"ImageMetadata"},"text":"I obfuscated the input to the sigmoid function for the input gate, just for simplicity.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-mUDovQ8ovejmWPNoFSI1g.png":{"id":"1*-mUDovQ8ovejmWPNoFSI1g.png","originalHeight":79,"originalWidth":541,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_392":{"id":"5c5cdcc6bd59_392","name":"f6be","type":"P","href":null,"layout":null,"metadata":null,"text":"Recall from our vanishing gradient article that the max output of sigmoid’s first order derivative is 0.25, and it’s something similar for tanh. This becomes textbook vanishing gradient problem. As we backprop through more and more cell states, the gradient terms become longer and longer, and this will definitely vanish. When they don’t vanish, they’ll be super minor contributions, so we can just leave them out for brevity.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_393":{"id":"5c5cdcc6bd59_393","name":"95ec","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Sidenote: one person reached out to me unsure of why gradients with long terms — aka, that are equal to the product of a lot of terms — usually vanishes\u002Fexplodes. Here’s what I said in response:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_394":{"id":"5c5cdcc6bd59_394","name":"eabd","type":"BQ","href":null,"layout":null,"metadata":null,"text":"“If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee those terms are around 1 each. If they’re not, it’ll explode or vanish. And, given the nature of the problems where this is an issue, it’s very unlikely they’ll be around 1 each. Especially if they are the output some non-linear function like sigmoid\u002Ftanh or their derivatives.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_395":{"id":"5c5cdcc6bd59_395","name":"944e","type":"BQ","href":null,"layout":null,"metadata":null,"text":"For example, let’s say the gradient term = k_1 × k_2 × k_3 × … × k_100. 100 terms in this product.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_396":{"id":"5c5cdcc6bd59_396","name":"1583","type":"BQ","href":null,"layout":null,"metadata":null,"text":"If each of these terms is, let’s say, around 0.5, then you have 0.5¹⁰⁰ = some absurdly low number. If you have each term be arond 1.5, then you have 1.5¹⁰⁰ which is some absurdly high number.\n\nWhen we introduce tanh\u002Fsigmoid and\u002For their derivatives in these huge products, you can guarantee that they’ll saturate and die off. As mentioned, the max for sigmoid’s first order derivative is 0.25, so just imagine something like 0.25¹⁰⁰.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_397":{"id":"5c5cdcc6bd59_397","name":"59c9","type":"P","href":null,"layout":null,"metadata":null,"text":"Ultimately, the reason I obfuscate these terms that vanish in the derivative is because I would like to show the effect of the forget gate on gradient flow now. If I included the other terms, the same implications would be present, but the math would just take longer to type out and render.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_398":{"id":"5c5cdcc6bd59_398","name":"e361","type":"P","href":null,"layout":null,"metadata":null,"text":"Because ƒw = i ⊙ g, we can redraw our diagram showing that ƒw won’t make any contributions to the gradient flow back. Again — ƒw does, but it’s effectively negligible, so we can just exclude it from our updated gradient flow diagram, which follows:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_398.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_398.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_398.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_398.markups.0":{"type":"STRONG","start":8,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_398.markups.1":{"type":"STRONG","start":59,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_398.markups.2":{"type":"STRONG","start":126,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_399":{"id":"5c5cdcc6bd59_399","name":"3217","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*RsHULZCgY6p-5bKkE99Q-Q.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*RsHULZCgY6p-5bKkE99Q-Q.png":{"id":"1*RsHULZCgY6p-5bKkE99Q-Q.png","originalHeight":453,"originalWidth":1095,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_400":{"id":"5c5cdcc6bd59_400","name":"8811","type":"P","href":null,"layout":null,"metadata":null,"text":"But wait! This doesn’t look good; the gradients have to multiply by this f_t gate at each timestep. Before, they didn’t have to multiply by anything (or, in other words, they multiplied by 1) and flowed past super easily.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_400.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_400.markups.0":{"type":"STRONG","start":73,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_401":{"id":"5c5cdcc6bd59_401","name":"9feb","type":"P","href":null,"layout":null,"metadata":null,"text":"Machine learning researchers coined a name for the type of function we had before we introduced the forget gate where the derivative of one cell state w.r.t. the previous is 1.0: “Constant Error Carousel” (CEC). With our new function, the derivative is equal to f. You’ll see this referred to as a “linear carousel” in papers.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_401.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_401.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_401.markups.0":{"type":"STRONG","start":174,"end":177,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_401.markups.1":{"type":"STRONG","start":262,"end":263,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_402":{"id":"5c5cdcc6bd59_402","name":"fdbb","type":"P","href":null,"layout":null,"metadata":null,"text":"Before we introduced a forget gate — where all we had was the additive interaction from ƒw — our cell state function was a CEC:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_402.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_402.markups.0":{"type":"STRONG","start":88,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_403":{"id":"5c5cdcc6bd59_403","name":"2bf9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*9O__qOVOK1wxJDFy6m4YAQ.png","typename":"ImageMetadata"},"text":"A CEC — same as before, but no forget gate.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*9O__qOVOK1wxJDFy6m4YAQ.png":{"id":"1*9O__qOVOK1wxJDFy6m4YAQ.png","originalHeight":84,"originalWidth":208,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_404":{"id":"5c5cdcc6bd59_404","name":"61b2","type":"P","href":null,"layout":null,"metadata":null,"text":"The derivative of this cell state w.r.t. the previous one, again as long as we don’t backprop through the i and g gates, is just 1. That’s why gradients flow back super comfortably, without vanishing at all. Basically, for a CEC to exist in this context, the coefficient of c_t-1 needs to be 1.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_404.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_404.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_404.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_404.markups.0":{"type":"STRONG","start":106,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_404.markups.1":{"type":"STRONG","start":112,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_404.markups.2":{"type":"STRONG","start":274,"end":280,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_405":{"id":"5c5cdcc6bd59_405","name":"7ab2","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we introduced this multiplicative interaction (for good reason), we got a linear carousel; the coefficient of c_t-1 is f. So, in our case, when f = 1 (when we’re not going to forget) our function becomes a CEC, and our gradients will pretty much never vanish. If it’s close to 0, though, the gradient term will immediately die. Gradients will stay on the carousel for a while until the forget gate is triggered; the effect on the gradient is like a step function, in that it’s constant with a value of 1 and then drops off to zero\u002Fdies when we have f ≈ 0.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_405.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_405.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_405.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_405.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_405.markups.0":{"type":"STRONG","start":115,"end":121,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_405.markups.1":{"type":"STRONG","start":124,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_405.markups.2":{"type":"STRONG","start":149,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_405.markups.3":{"type":"STRONG","start":554,"end":559,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_406":{"id":"5c5cdcc6bd59_406","name":"3337","type":"P","href":null,"layout":null,"metadata":null,"text":"Intuitively, this seems problematic. Let’s do some math to investigate:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_407":{"id":"5c5cdcc6bd59_407","name":"0955","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*UbqEhAyW7bMv_tDf-cvuWg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UbqEhAyW7bMv_tDf-cvuWg.png":{"id":"1*UbqEhAyW7bMv_tDf-cvuWg.png","originalHeight":164,"originalWidth":372,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_408":{"id":"5c5cdcc6bd59_408","name":"2abc","type":"P","href":null,"layout":null,"metadata":null,"text":"The derivative of a cell state to the previous is f_t. The derivative of a cell state to two prior cell states is f_t ⊙ f_t-1. Thus:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_408.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_408.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_408.markups.0":{"type":"STRONG","start":50,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_408.markups.1":{"type":"STRONG","start":114,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_409":{"id":"5c5cdcc6bd59_409","name":"1596","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*p9OndETS7tR-zUU-1TuaTw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*p9OndETS7tR-zUU-1TuaTw.png":{"id":"1*p9OndETS7tR-zUU-1TuaTw.png","originalHeight":83,"originalWidth":603,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_410":{"id":"5c5cdcc6bd59_410","name":"8238","type":"P","href":null,"layout":null,"metadata":null,"text":"As we backpropagate through time, these forget gates keep chaining up and multiplying together to form the overall gradient term.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_411":{"id":"5c5cdcc6bd59_411","name":"6f12","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, imagine an LSTM with 100 timesteps. If we wanted to get the derivative of the error w.r.t. a weight like W_xi, to optimize it, remember that with BPTT we add up or average all the gradients from the different timesteps:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_411.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_411.markups.0":{"type":"STRONG","start":110,"end":114,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_412":{"id":"5c5cdcc6bd59_412","name":"d867","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*rhb_2DO5MulJvzV9QxasMg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*rhb_2DO5MulJvzV9QxasMg.png":{"id":"1*rhb_2DO5MulJvzV9QxasMg.png","originalHeight":101,"originalWidth":692,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_413":{"id":"5c5cdcc6bd59_413","name":"a73a","type":"P","href":null,"layout":null,"metadata":null,"text":"OK. Now let’s look at an early (in time) term, like the gradient propagated from the error to the third cell:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_414":{"id":"5c5cdcc6bd59_414","name":"9f34","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*fAaYlJPgsPjRGJGoyc8XCw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fAaYlJPgsPjRGJGoyc8XCw.png":{"id":"1*fAaYlJPgsPjRGJGoyc8XCw.png","originalHeight":79,"originalWidth":106,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_415":{"id":"5c5cdcc6bd59_415","name":"9972","type":"P","href":null,"layout":null,"metadata":null,"text":"Remember that J is an addition of errors from Y individual outputs, so we backpropagate through each of the outputs first:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_415.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_415.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_415.markups.0":{"type":"STRONG","start":14,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_415.markups.1":{"type":"STRONG","start":45,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_416":{"id":"5c5cdcc6bd59_416","name":"7c21","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*vsybuvtlGl-cQqUI1Pqbag.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*vsybuvtlGl-cQqUI1Pqbag.png":{"id":"1*vsybuvtlGl-cQqUI1Pqbag.png","originalHeight":164,"originalWidth":609,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_417":{"id":"5c5cdcc6bd59_417","name":"4355","type":"P","href":null,"layout":null,"metadata":null,"text":"The first few terms, where we backprop y_k to c_3 where k \u003C 3, would just be equal to zero because c_3 only exists after these outputs have been computed.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_417.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_417.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_417.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_417.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_417.markups.0":{"type":"STRONG","start":39,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_417.markups.1":{"type":"STRONG","start":46,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_417.markups.2":{"type":"STRONG","start":56,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_417.markups.3":{"type":"STRONG","start":99,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_418":{"id":"5c5cdcc6bd59_418","name":"e686","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s assume that Y = 100 and continue with our assumption that t = 100 (so each timestep gives rise to an output), for simplicity. With this, let’s now look at the last term in this sum.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_418.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_418.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_418.markups.0":{"type":"STRONG","start":18,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_418.markups.1":{"type":"STRONG","start":64,"end":72,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_419":{"id":"5c5cdcc6bd59_419","name":"53d1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*JJIQxpb1mHjDn5KaoOKQkA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JJIQxpb1mHjDn5KaoOKQkA.png":{"id":"1*JJIQxpb1mHjDn5KaoOKQkA.png","originalHeight":79,"originalWidth":501,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_420":{"id":"5c5cdcc6bd59_420","name":"6208","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s a lot of forget gates chained together. If one of these forget gates is [approximately] zero, the whole gradient dies. If these also tend to be a small number between 0 and 1, the whole thing will vanish, and c_3 won’t make any contributions to the gradient here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_420.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_420.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_420.markups.0":{"type":"STRONG","start":47,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_420.markups.1":{"type":"STRONG","start":216,"end":220,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421":{"id":"5c5cdcc6bd59_421","name":"fb13","type":"P","href":null,"layout":null,"metadata":null,"text":"This isn’t intrinsically an issue though! Because, when a forget gate is zero, it means that cell is no longer making any contributions past that point. If f_4 is zero, then any y outputs at\u002Fpast timestep 4 won’t be influenced by c_3 (as well as c_2 and c_1) because we “erased” it from memory. Therefore that particular gradient should be zero. If y_80 is zero, then any outputs at\u002Fpast timestep 80 won’t be influenced by c_1, c_2, … , c_79. Same story here. If these forget gates are between 0 and 1, then the influence of our cell decays over time anyways, and our gradients will be very small, so they’ll reflect that. Gers 1999 calls this “releasing resources”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_421.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_421.markups.0":{"type":"A","start":623,"end":632,"href":"https:\u002F\u002Fpdfs.semanticscholar.org\u002F1154\u002F0131eae85b2e11d53df7f1360eeb6476e7f4.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.1":{"type":"STRONG","start":156,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.2":{"type":"STRONG","start":178,"end":179,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.3":{"type":"STRONG","start":230,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.4":{"type":"STRONG","start":246,"end":250,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.5":{"type":"STRONG","start":254,"end":257,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.6":{"type":"STRONG","start":349,"end":353,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.7":{"type":"STRONG","start":423,"end":441,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_421.markups.8":{"type":"EM","start":11,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_422":{"id":"5c5cdcc6bd59_422","name":"b6f8","type":"P","href":null,"layout":null,"metadata":null,"text":"Cell c_3 will still contribute to the overall gradient, though. For example, take this term:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_422.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_422.markups.0":{"type":"STRONG","start":5,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_423":{"id":"5c5cdcc6bd59_423","name":"0851","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*KZjK3wcZpYG_qnjkMB1HkA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*KZjK3wcZpYG_qnjkMB1HkA.png":{"id":"1*KZjK3wcZpYG_qnjkMB1HkA.png","originalHeight":79,"originalWidth":474,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_424":{"id":"5c5cdcc6bd59_424","name":"8736","type":"P","href":null,"layout":null,"metadata":null,"text":"Here, we’re looking at y_12 instead of y_100. Chances are that, if you have a sequence of length 100, your 100th cell state isn’t drawing from your 3rd; the forget gate would have been triggered at some point by then. However, the 12th cell state probably will still be drawing from the ones before it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_424.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_424.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_424.markups.0":{"type":"STRONG","start":23,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_424.markups.1":{"type":"STRONG","start":39,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425":{"id":"5c5cdcc6bd59_425","name":"5cd8","type":"P","href":null,"layout":null,"metadata":null,"text":"If we decide not to forget in the first 12 timesteps, ie. f_1 … f_12 are each not far from 1, then c_3 would have more influence over y_12 and the error that stems from y_12. Thus, the gradient would not vanish and c_3 still contributes to update W_xi, it just doesn’t contribute a gradient where it’s not warranted to (that is, where it doesn’t actually contribute to any activation, because it’s been forgotten). To summarize: one activated forget gate will indeed kill off gradient flow to cell(s), but that is a good thing because the network is learning that that gradient from the future has no benefit and is completely irrelevant to those particular cell(s), since those cells have been forgotten by then. In practice, different cells learn different ranges of context, some short, some long. This is a benefit for LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_425.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_425.markups.0":{"type":"STRONG","start":58,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.1":{"type":"STRONG","start":99,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.2":{"type":"STRONG","start":134,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.3":{"type":"STRONG","start":169,"end":173,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.4":{"type":"STRONG","start":215,"end":218,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.5":{"type":"STRONG","start":247,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_425.markups.6":{"type":"EM","start":811,"end":819,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_426":{"id":"5c5cdcc6bd59_426","name":"7b24","type":"P","href":null,"layout":null,"metadata":null,"text":"So, given a gradient between two cell states in time, when all of these forget gates are [approximately] equal to 1, the gradient signal will remain stable, because we’re multiplying by 1 at each timestep — effectively, not multiplying by anything at all. In such a case, our gradient flow diagram would look like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_427":{"id":"5c5cdcc6bd59_427","name":"9b4b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*rBJm9F6zz8drQnDlWd7wvQ.png","typename":"ImageMetadata"},"text":"It’s… it’s beautiful!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*rBJm9F6zz8drQnDlWd7wvQ.png":{"id":"1*rBJm9F6zz8drQnDlWd7wvQ.png","originalHeight":694,"originalWidth":2190,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_428":{"id":"5c5cdcc6bd59_428","name":"f12e","type":"P","href":null,"layout":null,"metadata":null,"text":"The gradient will have literally zero interactions or disturbances, and will just flow through like it’s driving 150 mph on an empty countryside America highway. The beauty of CECs is that they’re always like this.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_428.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_428.markups.0":{"type":"EM","start":197,"end":204,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_429":{"id":"5c5cdcc6bd59_429","name":"87cf","type":"P","href":null,"layout":null,"metadata":null,"text":"But, let’s get back to reality. LSTMs aren’t CECs. One disadvantage of these forget gates chaining together is that it could block learning. That is, when we set out to train our LSTM, the forget gates have not been learned; we have to learn them while we learn everything else. So, if they all start around 0, no gradients will flow through our cell states when we perform BPTT, and learning won’t happen at all.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_430":{"id":"5c5cdcc6bd59_430","name":"3db3","type":"P","href":null,"layout":null,"metadata":null,"text":"The obvious solution is to set the forget gate bias to a very large value when training, so it starts at 1 instead of 0 (because y = 1 is to the far right of the sigmoid function, so adding to the input will ensure ~1 will be the output). In early stages of training, the forget gates equalling\u002Fapproximating 1 will result in learning not being blocked. So many papers do this and mention it explicitly such that this forget gate bias could even be considered a hyperparameter.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_430.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_430.markups.0":{"type":"STRONG","start":129,"end":134,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_431":{"id":"5c5cdcc6bd59_431","name":"d763","type":"P","href":null,"layout":null,"metadata":null,"text":"By introducing forget gates, we stray from CECs and thus the guarantee that our gradients will never ever vanish. But, again, we do it for good reason. And when gradients vanish it’s because we chose to forget that cell — so it’s not necessarily a bad thing. We just need to make sure the forget gates don’t block learning in initial stages of training; in such a case, we shouldn’t need to bother about vanishing gradients too much.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_432":{"id":"5c5cdcc6bd59_432","name":"60bc","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s a more technical explanation:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_433":{"id":"5c5cdcc6bd59_433","name":"7ad7","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_433.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:f077b0ddd653f1c9f755d681a53be4a5":{"id":"f077b0ddd653f1c9f755d681a53be4a5","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.quora.com%2Fwidgets%2Fembed_iframe%3Fpath%3D%2FWhy-doesnt-the-use-of-a-forget-gate-in-LSTMs-cause-vanishing-dying-gradients%2Fanswer%2FCorentin-Tallec&url=https%3A%2F%2Fwww.quora.com%2FWhy-doesnt-the-use-of-a-forget-gate-in-LSTMs-cause-vanishing-dying-gradients%2Fanswer%2FCorentin-Tallec%3Fsrid%3DO7MN&image=https%3A%2F%2Fwww.quora.com%2Fstatic%2Fimages%2Flogo%2Fwordmark_default.png&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=quora","iframeHeight":560,"iframeWidth":560,"title":"Why doesn't the use of a forget gate in LSTMs cause vanishing\u002Fdying gradients?","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_433.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:f077b0ddd653f1c9f755d681a53be4a5","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_434":{"id":"5c5cdcc6bd59_434","name":"e1c7","type":"P","href":null,"layout":null,"metadata":null,"text":"We can try computing some more derivatives, just for fun! Let’s sub in real values for the timesteps, backprop across more than one timestep, and do it for a gate this time.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_435":{"id":"5c5cdcc6bd59_435","name":"62f7","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll expand c_4 and express it in terms of our gates only. In the process, each c_t, except c_1, will collapse into a few interactions between the f, i, and g gate:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_435.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_435.markups.0":{"type":"STRONG","start":13,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.1":{"type":"STRONG","start":81,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.2":{"type":"STRONG","start":93,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.3":{"type":"STRONG","start":148,"end":149,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.4":{"type":"STRONG","start":151,"end":152,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.5":{"type":"STRONG","start":153,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_435.markups.6":{"type":"STRONG","start":157,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_436":{"id":"5c5cdcc6bd59_436","name":"26eb","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*2LZxa4YAMGCJqOrirI1-_w.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*2LZxa4YAMGCJqOrirI1-_w.png":{"id":"1*2LZxa4YAMGCJqOrirI1-_w.png","originalHeight":166,"originalWidth":669,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_437":{"id":"5c5cdcc6bd59_437","name":"5092","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, let’s get the derivative of c_4 with respect to one of the earliest possible gates, like g_2. In the expression above, this turns out to just be the coefficient of g_2:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_437.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_437.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_437.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_437.markups.0":{"type":"STRONG","start":33,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_437.markups.1":{"type":"STRONG","start":94,"end":97,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_437.markups.2":{"type":"STRONG","start":169,"end":172,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_438":{"id":"5c5cdcc6bd59_438","name":"c936","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*rHDurdaN9SKnChWfyVk38w.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*rHDurdaN9SKnChWfyVk38w.png":{"id":"1*rHDurdaN9SKnChWfyVk38w.png","originalHeight":79,"originalWidth":205,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_439":{"id":"5c5cdcc6bd59_439","name":"708a","type":"P","href":null,"layout":null,"metadata":null,"text":"We experience the same neatness here as with the cell state backprop! It makes complete sense that the gradient would be i_2 ⊙ f_3 ⊙ f_4, since i_2 controls what influence g_2 has over c_2, f_3 controls what influence c_2 has on c_3, and f_4 controls what influence c_3 has over c_4. Notice the chaining up of the forget gates 👻; everything about the carousels I just talked about — and what they imply about vanishing gradients — applies here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_439.markups.10","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_439.markups.0":{"type":"STRONG","start":121,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.1":{"type":"STRONG","start":127,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.2":{"type":"STRONG","start":144,"end":148,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.3":{"type":"STRONG","start":172,"end":176,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.4":{"type":"STRONG","start":185,"end":188,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.5":{"type":"STRONG","start":190,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.6":{"type":"STRONG","start":218,"end":222,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.7":{"type":"STRONG","start":229,"end":232,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.8":{"type":"STRONG","start":238,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.9":{"type":"STRONG","start":266,"end":270,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_439.markups.10":{"type":"STRONG","start":279,"end":282,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_440":{"id":"5c5cdcc6bd59_440","name":"6229","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ll leave it up to you to derive something similar for the other gates.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_441":{"id":"5c5cdcc6bd59_441","name":"f15d","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s it! That’s why LSTMs rock their socks off when it comes to keeping their gradients in check.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_442":{"id":"5c5cdcc6bd59_442","name":"627a","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s a neat GIF showing a visual representation of the gradients that exist at each timestep, starting from timestep 128 and going all the way to the first, during backprop. More noise represents greater values:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_443":{"id":"5c5cdcc6bd59_443","name":"2414","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"Super highway indeed. imgur.com\u002Fgallery\u002FvaNahKE.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_443.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_443.iframe","typename":"Iframe"},"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_443.markups.0":{"type":"A","start":22,"end":47,"href":"http:\u002F\u002Fimgur.com\u002Fgallery\u002FvaNahKE","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"MediaResource:c239248e2e0b9a4aadc7b43d8c08ca12":{"id":"c239248e2e0b9a4aadc7b43d8c08ca12","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fi.imgur.com%2FvaNahKE.mp4&src_secure=1&url=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&image=http%3A%2F%2Fi.imgur.com%2FvaNahKEh.jpg&key=d04bfffea46d4aeda930ec88cc64b87c&type=video%2Fmp4&schema=imgur","iframeHeight":480,"iframeWidth":854,"title":"RNN vs LSTM: Vanishing Gradients","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_443.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:c239248e2e0b9a4aadc7b43d8c08ca12","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_444":{"id":"5c5cdcc6bd59_444","name":"868a","type":"P","href":null,"layout":null,"metadata":null,"text":"As you can see, the vanilla RNN’s gradients die off way quicker than the LSTM’s. The RNN is almost immediate in comparison. LSTMs seem like a super highway indeed, although it does seem that they do vanish. In this diagram, it can be suggested that the gradients perhaps die for the LSTM eventually because we chose to forget early cell states; again, this depends on the application at hand, and is learnable. (I’m not sure if this GIF uses truncated backprop, so that could be another thing. In general, I don’t know the context of this GIF.) Also, part of the gradient signal definitely vanishes—it’s the signals that pass through the f\u002Fi\u002Fg gates that we looked at earlier and obfuscated from the cell state→cell state derivative. We showed they would vanish because of tanh\u002Fsigmoid derivatives; initially, these signals will make a fairly significant contribution, but over time they’ll get smaller and smaller. That’s the explanation for this GIF.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_444.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_444.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_444.markups.0":{"type":"STRONG","start":638,"end":644,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_444.markups.1":{"type":"EM","start":400,"end":409,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_445":{"id":"5c5cdcc6bd59_445","name":"2b66","type":"P","href":null,"layout":null,"metadata":null,"text":"Exploding gradients is still an issue, though. Recall that when we have a bunch of gradient terms added together, if some of them vanish it doesn’t mean the whole thing will vanish (versus if they were multiplied together). However, if some of the gradients explode, the whole thing explodes; x + 0 = x, but x + ∞ = ∞. If cell states become unstable and grow too much in some rare scenario, then our gradients could explode. In such a case we’d need to implement gradient clipping, which is where we choose some arbitrary threshold that gradients cannot be larger than; so, grad = min(grad, clip_threshold). This would enable the LSTM to deal with such cases without essentially collapsing. Many successful LSTM applications use gradient clipping.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_445.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_445.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_445.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_445.markups.0":{"type":"CODE","start":574,"end":606,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_445.markups.1":{"type":"STRONG","start":312,"end":313,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_445.markups.2":{"type":"STRONG","start":316,"end":317,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_446":{"id":"5c5cdcc6bd59_446","name":"69b6","type":"P","href":null,"layout":null,"metadata":null,"text":"Usually, though, exploding gradients are avoided because sooner or later the forget gate in the carousel is triggered and we reset the memory.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_447":{"id":"5c5cdcc6bd59_447","name":"3679","type":"P","href":null,"layout":null,"metadata":null,"text":"There are variants of LSTMs. People have tried modifying the model, like computing the hidden state without using tanh activation (so h_t = o ⊙ c_t) or ditching the i input gate and only using g, since that would still satisfy the -1 to 1 range. The results didn’t change by much.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_447.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_447.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_447.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_447.markups.0":{"type":"STRONG","start":134,"end":147,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_447.markups.1":{"type":"STRONG","start":165,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_447.markups.2":{"type":"STRONG","start":193,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_448":{"id":"5c5cdcc6bd59_448","name":"dc3e","type":"P","href":null,"layout":null,"metadata":null,"text":"In fact, some researchers even applied evolutionary algorithms to spawn and test a bunch of variants on the standard LSTM equations. Most of the good ones just worked roughly the same.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_449":{"id":"5c5cdcc6bd59_449","name":"c7f8","type":"P","href":null,"layout":null,"metadata":null,"text":"This highlights an issue with LSTMs — they are definitely fairly handwavy. We use them because their architecture allows us to prevent gradients from vanishing such that we can learn long-term dependencies, but there’s not much theoretical or empirical backing for them. ANNs and RNNs make sense in that they’re biologically inspired and that they’re essentially just deep composite functions that have parameters we can optimize. LSTMs stray so far from statistical methods and introduce complex concepts\u002Farchitectures that work but aren’t necessarily justified from the get-go. Fully understanding why LSTMs work so well and coming up with better\u002Fsimpler architectures is a hot topic of research right now.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_450":{"id":"5c5cdcc6bd59_450","name":"4bca","type":"P","href":null,"layout":null,"metadata":null,"text":"There are also other variants of RNNs, similar to LSTMs, like GRUs (Gated Recurrent Units). LSTM is still the king, but GRUs in particular have grown in popularity and are seen in many recent, well-respected research papers. It’s a must learn next to LSTMs, but this article would get too bloated with it. TL;DR: GRUs have a less complex architecture than LSTMs but achieves similar results, and they can control the flow of information without requiring a memory cell. And, they’re fairly new. (See, told you “coming up with better\u002Fsimpler architectures is a hot topic of research right now” is true!)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_450.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_450.markups.0":{"type":"A","start":62,"end":66,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGated_recurrent_unit","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_451":{"id":"5c5cdcc6bd59_451","name":"c57a","type":"H3","href":null,"layout":null,"metadata":null,"text":"Yay RNNs!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_452":{"id":"5c5cdcc6bd59_452","name":"8a09","type":"P","href":null,"layout":null,"metadata":null,"text":"Wanna see a couple cool things? Practical applications were talked about in the first section, and the next section will walk through more technical applications of RNNs ie. in recent research papers. This section, instead, contains some fun things RNNs have done or produced that’ll ease yourself from the hell that was actually understanding them. As simply as we try to convey things on this blog, LSTMs be LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_453":{"id":"5c5cdcc6bd59_453","name":"7337","type":"P","href":null,"layout":null,"metadata":null,"text":"Sidenote: now, don’t be frightened by “RNNs”. Do be frightened by “vanilla RNNs”, in most contexts, but we use RNN as an umbrella term, and it most often refers to an LSTM or a similar variant like GRU.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_453.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_453.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_453.markups.0":{"type":"STRONG","start":46,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_453.markups.1":{"type":"EM","start":46,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_454":{"id":"5c5cdcc6bd59_454","name":"f30f","type":"P","href":null,"layout":null,"metadata":null,"text":"Many if not all of these are taken from Andrej Karpathy’s CS231n lecture, or his blog post on the same subject:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_454.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_454.markups.0":{"type":"A","start":58,"end":72,"href":"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=cO0a0QYmFm8&index=10&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_455":{"id":"5c5cdcc6bd59_455","name":"d19b","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"text":"The Unreasonable Effectiveness of Recurrent Neural Networks\nMusings of a Computer Scientist.karpathy.github.io","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_455.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_455.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_455.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_455.mixtapeMetadata","typename":"MixtapeMetadata"}},"Paragraph:5c5cdcc6bd59_455.markups.0":{"type":"A","start":0,"end":110,"href":"http:\u002F\u002Fkarpathy.github.io\u002F2015\u002F05\u002F21\u002Frnn-effectiveness\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_455.markups.1":{"type":"STRONG","start":0,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_455.markups.2":{"type":"EM","start":60,"end":92,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Paragraph:5c5cdcc6bd59_455.mixtapeMetadata":{"href":"http:\u002F\u002Fkarpathy.github.io\u002F2015\u002F05\u002F21\u002Frnn-effectiveness\u002F","thumbnailImageId":"0*7sIxt7RqO7deGldw.","__typename":"MixtapeMetadata"},"Paragraph:5c5cdcc6bd59_456":{"id":"5c5cdcc6bd59_456","name":"83d4","type":"P","href":null,"layout":null,"metadata":null,"text":"You should most certainly visit either his blog post or lecture for info on exactly how these experiments were conducted, and for more interpretation on their results. Taking a look at the ‘Visualizing the predictions and the “neuron” firings in the RNN’ section would also be helpful to gain more insight and intuition on how RNNs work and learn over time.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_456.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_456.markups.0":{"type":"STRONG","start":11,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_457":{"id":"5c5cdcc6bd59_457","name":"c051","type":"P","href":null,"layout":null,"metadata":null,"text":"A recurrent neural network generated this body of text, after it “read” a bunch of Shakespeare:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_458":{"id":"5c5cdcc6bd59_458","name":"802f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*BkvFHx8nYL1-NHmzZ_BbCQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BkvFHx8nYL1-NHmzZ_BbCQ.png":{"id":"1*BkvFHx8nYL1-NHmzZ_BbCQ.png","originalHeight":619,"originalWidth":574,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_459":{"id":"5c5cdcc6bd59_459","name":"62d6","type":"P","href":null,"layout":null,"metadata":null,"text":"Similarly, Karpathy gave an LSTM a lot of Paul Graham’s startup advice and life wisdom to read, and it produced this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_460":{"id":"5c5cdcc6bd59_460","name":"3654","type":"PQ","href":null,"layout":null,"metadata":null,"text":"“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_461":{"id":"5c5cdcc6bd59_461","name":"ee9f","type":"P","href":null,"layout":null,"metadata":null,"text":"A lot of relevant terminology, but it doesn’t really… come together 😖.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_462":{"id":"5c5cdcc6bd59_462","name":"1fc3","type":"P","href":null,"layout":null,"metadata":null,"text":"An LSTM can even generate valid XML, after reading Wikipedia!:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_463":{"id":"5c5cdcc6bd59_463","name":"89df","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003Cpage\u003E\n  \u003Ctitle\u003EAntichrist\u003C\u002Ftitle\u003E\n  \u003Cid\u003E865\u003C\u002Fid\u003E\n  \u003Crevision\u003E\n    \u003Cid\u003E15900676\u003C\u002Fid\u003E\n    \u003Ctimestamp\u003E2002-08-03T18:14:12Z\u003C\u002Ftimestamp\u003E\n    \u003Ccontributor\u003E\n      \u003Cusername\u003EParis\u003C\u002Fusername\u003E\n      \u003Cid\u003E23\u003C\u002Fid\u003E\n    \u003C\u002Fcontributor\u003E\n    \u003Cminor \u002F\u003E\n    \u003Ccomment\u003EAutomated conversion\u003C\u002Fcomment\u003E\n    \u003Ctext xml:space=\"preserve\"\u003E#REDIRECT [[Christianity]]\u003C\u002Ftext\u003E\n  \u003C\u002Frevision\u003E\n\u003C\u002Fpage\u003E","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_463.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_463.markups.0":{"type":"CODE","start":0,"end":365,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_464":{"id":"5c5cdcc6bd59_464","name":"7056","type":"P","href":null,"layout":null,"metadata":null,"text":"After a similar LSTM read the raw Latex version of an advanced algebra textbook, it came up with this — put frankly — fancy looking bogus. Let’s be real, you could definitely believe this was actual math 😜:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_465":{"id":"5c5cdcc6bd59_465","name":"221d","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_465.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:32e75fb2f7f388775689a155c5c27d86":{"id":"32e75fb2f7f388775689a155c5c27d86","iframeSrc":"https:\u002F\u002Fdrive.google.com\u002Fviewerng\u002Fviewer?url=https%3A\u002F\u002Fcs.stanford.edu\u002Fpeople\u002Fjcjohns\u002Ffake-math\u002F4.pdf&embedded=true","iframeHeight":780,"iframeWidth":600,"title":"","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_465.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:32e75fb2f7f388775689a155c5c27d86","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_466":{"id":"5c5cdcc6bd59_466","name":"e129","type":"P","href":null,"layout":null,"metadata":null,"text":"An LSTM also read the Linux source code, and tried to write some code of its own:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_467":{"id":"5c5cdcc6bd59_467","name":"ce46","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u002F*\n * Increment the size file of the new incorrect UI_FILTER group information\n * of the size generatively.\n *\u002F\nstatic int indicate_policy(void)\n{\n  int error;\n  if (fd == MARN_EPT) {\n    \u002F*\n     * The kernel blank will coeld it to userspace.\n     *\u002F\n    if (ss-\u003Esegment \u003C mem_total)\n      unblock_graph_and_set_blocked();\n    else\n      ret = 1;\n    goto bail;\n  }\n  segaddr = in_SB(in.addr);\n  selector = seg \u002F 16;\n  setup_works = true;\n  for (i = 0; i \u003C blocks; i++) {\n    seq = buf[i++];\n    bpf = bd-\u003Ebd.next + i * search;\n    if (fd) {\n      current = blocked;\n    }\n  }\n  rw-\u003Ename = \"Getjbbregs\";\n  bprm_self_clearl(&iv-\u003Eversion);\n  regs-\u003Enew = blocks[(BPF_STATS \u003C\u003C info-\u003Ehistoridac)] | PFMR_CLOBATHINC_SECONDS \u003C\u003C 12;\n  return segtable;\n}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.16","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.17","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.18","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.19","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.20","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.21","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.22","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.23","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.24","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.25","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.26","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.27","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.28","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.29","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.30","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.31","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.32","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.33","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.34","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.35","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.36","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.37","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.38","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.39","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.40","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.41","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_467.markups.42","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_467.markups.0":{"type":"CODE","start":0,"end":745,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.1":{"type":"STRONG","start":112,"end":118,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.2":{"type":"STRONG","start":119,"end":122,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.3":{"type":"STRONG","start":123,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.4":{"type":"STRONG","start":139,"end":143,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.5":{"type":"STRONG","start":149,"end":152,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.6":{"type":"STRONG","start":162,"end":164,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.7":{"type":"STRONG","start":169,"end":171,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.8":{"type":"STRONG","start":255,"end":257,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.9":{"type":"STRONG","start":261,"end":263,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.10":{"type":"STRONG","start":271,"end":272,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.11":{"type":"STRONG","start":327,"end":331,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.12":{"type":"STRONG","start":342,"end":343,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.13":{"type":"STRONG","start":351,"end":355,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.14":{"type":"STRONG","start":376,"end":377,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.15":{"type":"STRONG","start":405,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.16":{"type":"STRONG","start":411,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.17":{"type":"STRONG","start":431,"end":432,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.18":{"type":"STRONG","start":441,"end":444,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.19":{"type":"STRONG","start":448,"end":449,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.20":{"type":"STRONG","start":455,"end":456,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.21":{"type":"STRONG","start":466,"end":468,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.22":{"type":"STRONG","start":480,"end":481,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.23":{"type":"STRONG","start":487,"end":489,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.24":{"type":"STRONG","start":500,"end":501,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.25":{"type":"STRONG","start":504,"end":506,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.26":{"type":"STRONG","start":514,"end":515,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.27":{"type":"STRONG","start":518,"end":519,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.28":{"type":"STRONG","start":532,"end":534,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.29":{"type":"STRONG","start":556,"end":557,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.30":{"type":"STRONG","start":581,"end":583,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.31":{"type":"STRONG","start":588,"end":589,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.32":{"type":"STRONG","start":623,"end":624,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.33":{"type":"STRONG","start":626,"end":628,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.34":{"type":"STRONG","start":644,"end":646,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.35":{"type":"STRONG","start":650,"end":651,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.36":{"type":"STRONG","start":670,"end":672,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.37":{"type":"STRONG","start":677,"end":679,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.38":{"type":"STRONG","start":692,"end":693,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.39":{"type":"STRONG","start":718,"end":720,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.40":{"type":"STRONG","start":727,"end":733,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.41":{"type":"EM","start":0,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_467.markups.42":{"type":"EM","start":188,"end":250,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_468":{"id":"5c5cdcc6bd59_468","name":"da41","type":"P","href":null,"layout":null,"metadata":null,"text":"SUPERINTELLIGENCE MUCH‽ SELF-RECURSIVE IMPROVEMENT MUCH‽ THE END OF THE UNIVERSE MUCH‽","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_468.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_468.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_468.markups.0":{"type":"STRONG","start":0,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_468.markups.1":{"type":"EM","start":0,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_469":{"id":"5c5cdcc6bd59_469","name":"7094","type":"P","href":null,"layout":null,"metadata":null,"text":"Nope. Just some code doesn’t compile or make any sense. It even has its own bogus comments!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_470":{"id":"5c5cdcc6bd59_470","name":"b3fc","type":"P","href":null,"layout":null,"metadata":null,"text":"Generating music? Easy! A fun watch:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_471":{"id":"5c5cdcc6bd59_471","name":"eae8","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_471.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:8de70b7fa3e5cb979099278112052953":{"id":"8de70b7fa3e5cb979099278112052953","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FA2gyidoFsoI%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DA2gyidoFsoI&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FA2gyidoFsoI%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"Generating Music with RNN","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_471.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:8de70b7fa3e5cb979099278112052953","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_472":{"id":"5c5cdcc6bd59_472","name":"c80a","type":"P","href":null,"layout":null,"metadata":null,"text":"A more informative watch:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_473":{"id":"5c5cdcc6bd59_473","name":"565f","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_473.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:3f992ef4ac506dafa8d2d8badfc31dc2":{"id":"3f992ef4ac506dafa8d2d8badfc31dc2","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FaSr8_QQYpYM%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaSr8_QQYpYM&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaSr8_QQYpYM%2Fhqdefault.jpg&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"Using LSTM Recurrent Neural Networks for Music Generation","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_473.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:3f992ef4ac506dafa8d2d8badfc31dc2","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_474":{"id":"5c5cdcc6bd59_474","name":"6518","type":"P","href":null,"layout":null,"metadata":null,"text":"Something even cooler and… creepier (seriously, the results after the first couple iterations of training are so unsettling):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_475":{"id":"5c5cdcc6bd59_475","name":"7c5c","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_475.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:b5f70a5d514e61ad4646217c70974843":{"id":"b5f70a5d514e61ad4646217c70974843","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FNG-LATBZNBs%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNG-LATBZNBs&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNG-LATBZNBs%2Fhqdefault.jpg&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"Neural Network Tries to Generate English Speech (RNN\u002FLSTM)","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_475.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:b5f70a5d514e61ad4646217c70974843","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_476":{"id":"5c5cdcc6bd59_476","name":"7eb1","type":"H3","href":null,"layout":null,"metadata":null,"text":"In Practice","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_477":{"id":"5c5cdcc6bd59_477","name":"a21c","type":"P","href":null,"layout":null,"metadata":null,"text":"So we’ve seen how RNNs work in theory; now where do they fit in in practice?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_478":{"id":"5c5cdcc6bd59_478","name":"e928","type":"P","href":null,"layout":null,"metadata":null,"text":"As it turns out, recurrent neural networks can do a whole lot. I’ll try to cover a few of the important, significant, and interesting uses that have cropped up over the last few years.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_479":{"id":"5c5cdcc6bd59_479","name":"e30a","type":"H4","href":null,"layout":null,"metadata":null,"text":"Bidirectional Recurrent Neural Networks","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_480":{"id":"5c5cdcc6bd59_480","name":"5985","type":"P","href":null,"layout":null,"metadata":null,"text":"The Problem: Giving the network access to a sequence of vectors is fine and dandy, but what if we want our output at time t to be conditioned on an input vector that comes at a later timestep? Take the example of speech recognition, where our input vectors are some kind of audio features at time t and the output is the predicted phoneme at that time. In our traditional RNN architecture, the output at time t is conditioned only on input vectors 1..t, but as it turns out future information might be useful too. The sounds at time step t+1 (and maybe t+2, t+3, …) are likely part of the same phoneme, and therefore could help us make more accurate predictions. But our network won’t have access to them until we already output a prediction at time t. That’s bad.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_480.markups.11","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_480.markups.0":{"type":"A","start":331,"end":338,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPhoneme","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.1":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.2":{"type":"STRONG","start":122,"end":123,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.3":{"type":"STRONG","start":297,"end":298,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.4":{"type":"STRONG","start":409,"end":410,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.5":{"type":"STRONG","start":448,"end":452,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.6":{"type":"STRONG","start":538,"end":542,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.7":{"type":"STRONG","start":553,"end":556,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.8":{"type":"STRONG","start":558,"end":561,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.9":{"type":"STRONG","start":750,"end":751,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.10":{"type":"EM","start":338,"end":339,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_480.markups.11":{"type":"EM","start":426,"end":430,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_481":{"id":"5c5cdcc6bd59_481","name":"60ef","type":"P","href":null,"layout":null,"metadata":null,"text":"The Solution: We essentially “double up” each RNN neuron into two independent neurons — a “forward” neuron and a “backward” neuron. The forward neuron is the same as a regular RNN neuron, which gets inputs 0..T sequentially, updating its internal state and outputting some value at each time step along the way. The backward neuron follows the same general principle, but it sees the input vectors in reverse order.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_481.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_481.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_481.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_481.markups.1":{"type":"STRONG","start":206,"end":210,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_482":{"id":"5c5cdcc6bd59_482","name":"e2ef","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll look at an example to make sense of all this.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_483":{"id":"5c5cdcc6bd59_483","name":"2749","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Vsvw39SW0xEwRLijLRb3qg.png","typename":"ImageMetadata"},"text":"This is a typical recurrent neural network: at each timestep, the hidden state is updated based on the latest input.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Vsvw39SW0xEwRLijLRb3qg.png":{"id":"1*Vsvw39SW0xEwRLijLRb3qg.png","originalHeight":504,"originalWidth":404,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_484":{"id":"5c5cdcc6bd59_484","name":"89e5","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*JZNjmHjYFVcHrPKTDmvoXQ.png","typename":"ImageMetadata"},"text":"This is a bidirectional recurrent neural network. There are two neurons: one that takes inputs like normal, and one that takes them in reverse. Their output is combined to produce one output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JZNjmHjYFVcHrPKTDmvoXQ.png":{"id":"1*JZNjmHjYFVcHrPKTDmvoXQ.png","originalHeight":504,"originalWidth":606,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_485":{"id":"5c5cdcc6bd59_485","name":"ea03","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s walk through this timestep-by-timestep. At t=0, our vanilla RNN cell takes the input, updates its hidden state, and outputs a value. Now let’s look at the BiRNN: the “forward” half of our BiRNN neuron does exactly the same thing, but the “backward” half looks through all of our inputs — in reverse order, t=T..0 — and updates its hidden state with each one. Then when we get to the t=0 input vector, it updates its hidden state one last time and outputs a final value. We then take this final output value and combine it with the “forward” half (“combine” is pretty loosely-defined, usually just by concatenation or addition). Moving on to t=1, our “forward” part reads in the next input, updates state, and outputs another value. Combined with the second-to-last output of our “backward” counterpart, and we have the second output of our BiRNN neuron. Rinse and repeat.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_485.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_485.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_485.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_485.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_485.markups.0":{"type":"STRONG","start":49,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_485.markups.1":{"type":"STRONG","start":312,"end":318,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_485.markups.2":{"type":"STRONG","start":389,"end":392,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_485.markups.3":{"type":"STRONG","start":647,"end":650,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_486":{"id":"5c5cdcc6bd59_486","name":"1bf8","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s the general idea. Neat, right? BiRNNs (and their more adept cousin, BiLSTMs) are used all over the place. Maybe we’ll see them popping up in some of the other case studies that we’ll be looking at.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_487":{"id":"5c5cdcc6bd59_487","name":"7549","type":"H4","href":null,"layout":null,"metadata":null,"text":"Autoencoders","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_488":{"id":"5c5cdcc6bd59_488","name":"cdb3","type":"P","href":null,"layout":null,"metadata":null,"text":"Remember when we talked about autoencoders? Turns out we can use RNNs there too!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_488.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_488.markups.0":{"type":"A","start":30,"end":42,"href":"https:\u002F\u002Fayearofai.com\u002Flenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_489":{"id":"5c5cdcc6bd59_489","name":"107c","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s refresh: what is an autoencoder? Put simply, it’s a clever way of tricking a neural network to learn a useful representation of some data. Let’s say we have a dataset of images of faces, and we want to compress the thousands of numbers representing RGB values of pixels down into a 500-dimensional latent vector. We construct a network as such, where the middle layer has 500 neurons:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_490":{"id":"5c5cdcc6bd59_490","name":"08e3","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*M1bVZtZ6UPTyXoiy.","typename":"ImageMetadata"},"text":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FAutoencoder#\u002Fmedia\u002FFile:Autoencoder_structure.png","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_490.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*M1bVZtZ6UPTyXoiy.":{"id":"0*M1bVZtZ6UPTyXoiy.","originalHeight":506,"originalWidth":677,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_490.markups.0":{"type":"A","start":0,"end":79,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FAutoencoder#\u002Fmedia\u002FFile:Autoencoder_structure.png","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_491":{"id":"5c5cdcc6bd59_491","name":"fe44","type":"P","href":null,"layout":null,"metadata":null,"text":"…and train it to reproduce the input in the output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_492":{"id":"5c5cdcc6bd59_492","name":"46d5","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s explore this idea a little further. Imagine that data is flowing through our network, starting with the input layer and through each subsequent layer. We can view each layer as performing a transformation, converting our input to another vector, and then that vector into another, until we get our output. If we train our network to reproduce the input, that means that each intermediate vector must still represent the same information as the input, in some form or another. Essentially, the activations of each layer are a new representation of our input vector. If our network trains well, we can convert a 10,000-dimensional vector of pixel values into a 500-dimensional vector of image features which can be converted back into a 10,000-dimensional vector of pixel values that approximates what the input would have been.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_493":{"id":"5c5cdcc6bd59_493","name":"f1d3","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*RP5VZyqDJ9JI5wBk.","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*RP5VZyqDJ9JI5wBk.":{"id":"0*RP5VZyqDJ9JI5wBk.","originalHeight":34,"originalWidth":201,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_494":{"id":"5c5cdcc6bd59_494","name":"1008","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*Tc8mc_NWMmQZ15ND.","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*Tc8mc_NWMmQZ15ND.":{"id":"0*Tc8mc_NWMmQZ15ND.","originalHeight":34,"originalWidth":200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_495":{"id":"5c5cdcc6bd59_495","name":"c1de","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s make this a tad more concrete. We have two functions, f and g. f is our encoder, mapping from an n-long vector to an m-long vector. (n is the size of our input, m is the size of our latent representation.) g is our decoder, which maps back from an m-long vector to an n-long vector. In the normal autoencoder setting, both f and g are neural networks trained jointly (or different parts of a single network, same thing really) to reconstruct x.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_495.markups.11","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_495.markups.0":{"type":"STRONG","start":60,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.1":{"type":"STRONG","start":66,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.2":{"type":"STRONG","start":69,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.3":{"type":"STRONG","start":103,"end":104,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.4":{"type":"STRONG","start":123,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.5":{"type":"STRONG","start":139,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.6":{"type":"STRONG","start":167,"end":168,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.7":{"type":"STRONG","start":212,"end":213,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.8":{"type":"STRONG","start":254,"end":255,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.9":{"type":"STRONG","start":274,"end":275,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.10":{"type":"STRONG","start":329,"end":330,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_495.markups.11":{"type":"STRONG","start":335,"end":336,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_496":{"id":"5c5cdcc6bd59_496","name":"f029","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*1R_heM-ujpUvlbGM.","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*1R_heM-ujpUvlbGM.":{"id":"0*1R_heM-ujpUvlbGM.","originalHeight":37,"originalWidth":182,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_497":{"id":"5c5cdcc6bd59_497","name":"1680","type":"P","href":null,"layout":null,"metadata":null,"text":"So, where do RNNs fit in? Let’s say our inputs are now sequences of vectors instead of a single vector. We can use a similar concept, with both the encoder and decoder represented using an RNN. Here’s how it works: we feed our input sequence into the encoder RNN. With each input vector of the sequence, this encoder updates its internal state. Eventually, once it has seen the entire input, we have some final network internal state which represents our entire input sequence. Neat! Now, we make the hidden state of our decoder RNN the initial hidden state of our encoder, and ask it to spit out a sequence. Ideally, it spits out something close to what the initial sequence was.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_497.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_497.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_497.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_497.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_497.markups.0":{"type":"EM","start":251,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_497.markups.1":{"type":"EM","start":309,"end":316,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_497.markups.2":{"type":"EM","start":521,"end":532,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_497.markups.3":{"type":"EM","start":565,"end":572,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498":{"id":"5c5cdcc6bd59_498","name":"b8c2","type":"P","href":null,"layout":null,"metadata":null,"text":"Going back to our math-y definitions, we see that it basically fits in to the same framework, except we have q n-long vectors going into f and coming out of g. So q n-long vectors go in to f, and a single m-long vector comes out. We then give this m-long vector back to g, which spits out q n-long vectors.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_498.markups.10","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_498.markups.0":{"type":"STRONG","start":109,"end":110,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.1":{"type":"STRONG","start":137,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.2":{"type":"STRONG","start":157,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.3":{"type":"STRONG","start":163,"end":164,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.4":{"type":"STRONG","start":165,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.5":{"type":"STRONG","start":189,"end":190,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.6":{"type":"STRONG","start":205,"end":206,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.7":{"type":"STRONG","start":248,"end":249,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.8":{"type":"STRONG","start":270,"end":271,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.9":{"type":"STRONG","start":289,"end":290,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_498.markups.10":{"type":"STRONG","start":291,"end":292,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_499":{"id":"5c5cdcc6bd59_499","name":"5a64","type":"P","href":null,"layout":null,"metadata":null,"text":"That was a lot of letters, but you get the idea (I hope).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_500":{"id":"5c5cdcc6bd59_500","name":"b829","type":"P","href":null,"layout":null,"metadata":null,"text":"Like much of deep learning, the concept itself is pretty simple, but the implications are pretty cool. We can take any sequence — a variable-length sequence, mind you — and convert it into a fixed-size vector. And then convert that back to a variable-length sequence.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_501":{"id":"5c5cdcc6bd59_501","name":"5546","type":"P","href":null,"layout":null,"metadata":null,"text":"It turns out this model is actually incredibly powerful, so let’s take a look at one particularly useful (and successful) application: machine translation.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_502":{"id":"5c5cdcc6bd59_502","name":"4be2","type":"H4","href":null,"layout":null,"metadata":null,"text":"Neural Machine Translation","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_503":{"id":"5c5cdcc6bd59_503","name":"9d22","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s take these ideas we just learned about sequence-to-sequence (or seq2seq, for short) RNNs and apply them to machine translation. We throw in a sequence of words in one language, and it outputs a sequence of words in another. Simple enough, right?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_504":{"id":"5c5cdcc6bd59_504","name":"cd0e","type":"P","href":null,"layout":null,"metadata":null,"text":"The model we’re going to look at specifically is Google’s implementation of NMT. You can read all the gory details in their paper, but for now why don’t I give you the watered-down version.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_504.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_504.markups.0":{"type":"A","start":115,"end":129,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1609.08144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_505":{"id":"5c5cdcc6bd59_505","name":"3f1b","type":"P","href":null,"layout":null,"metadata":null,"text":"At it’s core, the GNMT architecture is just another seq2seq model. We have an encoder, consisting of 8 LSTM layers with skip connections (the first layer is bidirectional). We also have a decoder, once again containing 8 LSTM layers with skip connections. (A skip connection in a neural network is a connection which skips a layer and connects to the next available layer.) The decoder network outputs a probability distribution of words (well, sort of — we’ll talk more about that later), which we sample from to get our [translated] sentence. 🎉","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_506":{"id":"5c5cdcc6bd59_506","name":"afc6","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s a scary diagram from the paper:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_507":{"id":"5c5cdcc6bd59_507","name":"6a00","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*mk1BeF8ANMbAVOzD.","typename":"ImageMetadata"},"text":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1609.08144","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_507.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*mk1BeF8ANMbAVOzD.":{"id":"0*mk1BeF8ANMbAVOzD.","originalHeight":363,"originalWidth":656,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_507.markups.0":{"type":"A","start":0,"end":32,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1609.08144","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_508":{"id":"5c5cdcc6bd59_508","name":"a450","type":"P","href":null,"layout":null,"metadata":null,"text":"But there are a few other aspects to the GNMT that are important to note (there’s actually lots of interesting stuff going on in this architecture, so I really recommend you do read the paper).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_508.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_508.markups.0":{"type":"A","start":177,"end":191,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1609.08144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_509":{"id":"5c5cdcc6bd59_509","name":"1fbc","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s turn our attention to the center of the above diagram. This is a critical part of the GNMT architecture (and GNMT is certainly not the first to use attention) which allows the decoder to focus on certain parts of the encoder’s output as it produces output. Specifically, the GNMT architecture differs from the traditional seq2seq model in that our encoder does not produce a single fixed-width vector (the final hidden state) representing the entire output. Instead, we actually look at the output from each time step, and each time step gives us some latent representation. While decoding, we combine all of these hidden vectors into one context vector using something called soft attention.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_509.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_509.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_509.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_509.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_509.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_509.markups.0":{"type":"STRONG","start":370,"end":371,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_509.markups.1":{"type":"EM","start":15,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_509.markups.2":{"type":"EM","start":367,"end":371,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_509.markups.3":{"type":"EM","start":645,"end":652,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_509.markups.4":{"type":"EM","start":683,"end":697,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_510":{"id":"5c5cdcc6bd59_510","name":"369f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*ua03RdgdNWPw1_Jd.","typename":"ImageMetadata"},"text":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1609.08144","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_510.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*ua03RdgdNWPw1_Jd.":{"id":"0*ua03RdgdNWPw1_Jd.","originalHeight":244,"originalWidth":704,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_510.markups.0":{"type":"A","start":0,"end":32,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1609.08144","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511":{"id":"5c5cdcc6bd59_511","name":"ddb5","type":"P","href":null,"layout":null,"metadata":null,"text":"More concretely, that works like this (at every decoder time step). We first look at the output of the first decoder layer from the last time step. Following the notation from the paper, we’ll call that yi-1. We also have a series of encoder outputs, x1…xM, one for each encoder timestep. For each encoder timestep, we give our special attention function yi-1 and xt and get back a single fixed-size vector st, which we then run through a softmax. So, we’ve converted our encoder information from that timestep (and some decoder information) into a single attention vector — this attention vector tells us which parts of the encoder output we should look at more closely. We multiply this attention vector by our encoder output xt, which has the effect of “focusing” more on certain values and less on others. Finally, we take the sum of those “focused” vectors over each encoder timestep to produce our attention context for this timestep ai, which is fed to every decoder layer.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_511.markups.9","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_511.markups.0":{"type":"STRONG","start":203,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.1":{"type":"STRONG","start":251,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.2":{"type":"STRONG","start":355,"end":360,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.3":{"type":"STRONG","start":364,"end":366,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.4":{"type":"STRONG","start":407,"end":409,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.5":{"type":"STRONG","start":728,"end":730,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.6":{"type":"STRONG","start":940,"end":942,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.7":{"type":"EM","start":132,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.8":{"type":"EM","start":298,"end":305,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_511.markups.9":{"type":"EM","start":382,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_512":{"id":"5c5cdcc6bd59_512","name":"8681","type":"P","href":null,"layout":null,"metadata":null,"text":"Oh yeah, that attention function? That’s just yet another neural network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_512.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_512.markups.0":{"type":"EM","start":50,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_513":{"id":"5c5cdcc6bd59_513","name":"145a","type":"P","href":null,"layout":null,"metadata":null,"text":"Attention mechanisms like this one are pretty common in many deep learning architectures. This is an example of soft attention: we learn a distribution over our inputs and compute a weighted sum. This process is fully-differentiated, so we can use standard backpropogation to figure out how to train our attention model. Another possible mechanism is called hard attention, in which we select just one of the possible inputs and “focus” solely on that input. This process is not differentiable, so we need to use some other algorithm (usually some kind of reinforcement learning) to train a hard attention algorithm.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_513.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_513.markups.0":{"type":"EM","start":358,"end":372,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_514":{"id":"5c5cdcc6bd59_514","name":"10b2","type":"P","href":null,"layout":null,"metadata":null,"text":"GNMT combines all kinds of other cool ideas to achieve state-of-the-art results, including a wordpiece model which segments words into smaller “wordpieces” to help translate rarer words and neat parallelization techniques that let them train this monstrosity of an architecture in reasonable time.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_515":{"id":"5c5cdcc6bd59_515","name":"818e","type":"P","href":null,"layout":null,"metadata":null,"text":"A few months ago, Google put their GNMT model into production. Cutting-edge research is being implemented in the real world at an incredible rapid pace within the field of machine learning, and this is just one of countless examples.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_515.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_515.markups.0":{"type":"A","start":18,"end":61,"href":"https:\u002F\u002Fblog.google\u002Fproducts\u002Ftranslate\u002Ffound-translation-more-accurate-fluent-sentences-google-translate\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_516":{"id":"5c5cdcc6bd59_516","name":"0c13","type":"H4","href":null,"layout":null,"metadata":null,"text":"Long-Term Recurrent Convolutional Networks","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_517":{"id":"5c5cdcc6bd59_517","name":"c3b8","type":"P","href":null,"layout":null,"metadata":null,"text":"(Not to be confused with LCRNs.)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_518":{"id":"5c5cdcc6bd59_518","name":"e750","type":"P","href":null,"layout":null,"metadata":null,"text":"The Problem: We have a sequence of images that we need to make predictions for. CNNs are good at processing images, RNNs are good at processing sequences…how do we put the two together?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_518.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_518.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_519":{"id":"5c5cdcc6bd59_519","name":"606b","type":"P","href":null,"layout":null,"metadata":null,"text":"The Solution: The solution proposed in this paper is as straightforward as you would expect: take your image, extract features using a CNN, and feed this feature vector to your LSTM.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_519.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_519.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_519.markups.0":{"type":"A","start":39,"end":49,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1411.4389.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_519.markups.1":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_520":{"id":"5c5cdcc6bd59_520","name":"fdf8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*qiQ7DvCkHydXAFZ1.","typename":"ImageMetadata"},"text":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1411.4389","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_520.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*qiQ7DvCkHydXAFZ1.":{"id":"0*qiQ7DvCkHydXAFZ1.","originalHeight":319,"originalWidth":355,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_520.markups.0":{"type":"A","start":0,"end":31,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1411.4389","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_521":{"id":"5c5cdcc6bd59_521","name":"1c9f","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s really all there is to it, and the reason it works is because (as we’ve seen before) CNNs are incredibly adept at converting raw pixel data to a more meaningful representation. This saves the LSTMs the problem of parsing through the pixels to figure out what’s going on in the image and allows the LSTM weights to focus on converting a vector of image features into some meaningful sequence (say, a caption). It’s the same reason that using a word embedding is often preferred to a one-hot vector when feeding in words to an NLP model: the more meaningful your representation is, the easier it is to make further predictions with it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_521.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_521.markups.0":{"type":"A","start":70,"end":90,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_522":{"id":"5c5cdcc6bd59_522","name":"b189","type":"H4","href":null,"layout":null,"metadata":null,"text":"Image Captioning","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_523":{"id":"5c5cdcc6bd59_523","name":"3e15","type":"P","href":null,"layout":null,"metadata":null,"text":"(To be confused with LCRNs!)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_524":{"id":"5c5cdcc6bd59_524","name":"7c86","type":"P","href":null,"layout":null,"metadata":null,"text":"So there has been a lot of really impressive work on image captioning lately, but I wanna give a special shout-out to this 2015 paper from Karpathy et al. It was one of the first ML papers I had ever read, and really got me excited about the field. And, it uses RNNs, so that’s cool too.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_524.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_524.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_524.markups.0":{"type":"A","start":118,"end":133,"href":"http:\u002F\u002Fcs.stanford.edu\u002Fpeople\u002Fkarpathy\u002Fcvpr2015.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_524.markups.1":{"type":"EM","start":148,"end":153,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_525":{"id":"5c5cdcc6bd59_525","name":"ca5d","type":"P","href":null,"layout":null,"metadata":null,"text":"The idea behind image captioning is kind of self-explanatory, but I’ll explain it anyway. You give the model an image, it gives you a caption. Which is kind of insane if you think about it — a computer can go from pixels to interpreting what it’s seeing, and from that generate real and grammatical sentences to explain what it sees. I still can’t really believe stuff like this actually works, but somehow it does.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_526":{"id":"5c5cdcc6bd59_526","name":"3370","type":"P","href":null,"layout":null,"metadata":null,"text":"The model described in this specific paper combines our old pal convolutional neural networks with our newly-discovered RNNs. Step 1 is to pass our image through a convolutional neural network and extract some features from the last fully-connected layer. This lets us convert our pixel representation of the image into something that’s hopefully a bit more meaningful. We take this image feature vector and use it to initialize the hidden state of our RNN.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_527":{"id":"5c5cdcc6bd59_527","name":"de23","type":"P","href":null,"layout":null,"metadata":null,"text":"This is where it gets cool. We feed our network a start token, and it gives us a word (more accurately, a distribution of words, which we sample to get the first word of our caption). We feed this word back as the next input, and sample another word from the output. And again, and again, and again, until we finally sample an end token and have a complete caption.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_528":{"id":"5c5cdcc6bd59_528","name":"0330","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s not strictly necessary to feed the word that we sampled back to the network, but that’s pretty common practice to help the network condition its output on the previous word (the hidden state is critical for this too, of course). The results from this particular paper were pretty cool, you can see some of the results here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_528.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_528.markups.0":{"type":"A","start":323,"end":327,"href":"http:\u002F\u002Fcs.stanford.edu\u002Fpeople\u002Fkarpathy\u002Fdeepimagesent\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_529":{"id":"5c5cdcc6bd59_529","name":"6ec4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Neural Machine Translation, Again","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_530":{"id":"5c5cdcc6bd59_530","name":"aba2","type":"P","href":null,"layout":null,"metadata":null,"text":"Yes, NMTs are just that cool that I need to talk about them again.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_531":{"id":"5c5cdcc6bd59_531","name":"7a0e","type":"P","href":null,"layout":null,"metadata":null,"text":"The Problem: With our good ol’ GNMT architecture, we can train a massive model to convert from language A to language B. That’s great — except, if we support more than a hundred languages, we need to train more than 10,000 different language-pair models, each of which can take months to converge. That’s no good, and it’s the reason that when Google put GNMT in production, they only did so for eight language-pairs (still a monumental achievement). But…what if we didn’t need to train a separate model for each language pair? What if we could train one model for all the language pairs — impossible, right?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_531.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_531.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_531.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_531.markups.1":{"type":"STRONG","start":216,"end":222,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_532":{"id":"5c5cdcc6bd59_532","name":"1717","type":"P","href":null,"layout":null,"metadata":null,"text":"The Solution: Apparently it’s not impossible, and to make things even crazier, we can use the original GNMT architecture without modification. The only real change is that we prepend a special language token to the beginning of each sequence telling it what language to translate to. (We also use one shared wordpiece model for all language, instead of one per language pair.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_532.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_532.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_532.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_532.markups.1":{"type":"STRONG","start":79,"end":141,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_533":{"id":"5c5cdcc6bd59_533","name":"90b7","type":"P","href":null,"layout":null,"metadata":null,"text":"So we’ve condensed tens of thousands of NMT models into a single model that is able to translate to and from any language it was trained on. The paper elaborates on the implications and benefits of this more than I will, but to summarize:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_533.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_533.markups.0":{"type":"A","start":141,"end":150,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.04558.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_534":{"id":"5c5cdcc6bd59_534","name":"2db1","type":"ULI","href":null,"layout":null,"metadata":null,"text":"One model instead of tens of thousands. Months of training time saved, simpler production deployment, fewer parameters — simplicity wins out over complexity.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_535":{"id":"5c5cdcc6bd59_535","name":"66a7","type":"ULI","href":null,"layout":null,"metadata":null,"text":"We might have more training data for some language pairs than others. When we have separate models for each language pair, this means that the pairs with less data will have significantly poorer performance. If we put them all into one model, the language pairs with less data can still benefit from all of the data in the other language pairs, because all of the language pairs share weights (since they all use the same model).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_536":{"id":"5c5cdcc6bd59_536","name":"2a4f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"This one is absolutely nuts. If we train our network to translate English → Spanish and Spanish → French, our network automatically knows how to translate English → French (reasonably well).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_536.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_536.markups.0":{"type":"STRONG","start":106,"end":171,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_537":{"id":"5c5cdcc6bd59_537","name":"0314","type":"P","href":null,"layout":null,"metadata":null,"text":"Expanding on that last point some more: the authors of the paper even found evidence of an interlingua, or an intermediate representation that is shared by multiple languages. Being able to learn an interlingua is the ideal end goal to create a fully generalized multilingual NMT: we learn an encoder\u002Fdecoder to convert to\u002Ffrom the interlingua for each language, and we immediately know how to translate to and from that language. We aren’t quite there yet, but this is a major step in that direction. Creating a larger multilingual NMT model and giving it even more data could be all it takes to achieve new state-of-the-art translation results.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_537.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_537.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_537.markups.0":{"type":"EM","start":91,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_537.markups.1":{"type":"EM","start":441,"end":446,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_538":{"id":"5c5cdcc6bd59_538","name":"e74d","type":"H4","href":null,"layout":null,"metadata":null,"text":"So, yeah","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_539":{"id":"5c5cdcc6bd59_539","name":"1de9","type":"P","href":null,"layout":null,"metadata":null,"text":"RNNs are pretty awesome. There are new RNN papers published literally every day and it’s impossible to cover everything — if you think I missed something important, definitely let me know. (From Rohan: Except Neural Turing Machines and Learning to Learn. Those are dope, we know it, and we’re going to be covering them soon!)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_539.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_539.markups.0":{"type":"A","start":176,"end":187,"href":"https:\u002F\u002Ftwitter.com\u002FLennyKhazan","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_540":{"id":"5c5cdcc6bd59_540","name":"449c","type":"H3","href":null,"layout":null,"metadata":null,"text":"Building a Vanilla Recurrent Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_541":{"id":"5c5cdcc6bd59_541","name":"55e8","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s get practical for a minute and see how we can build one of these things in practice. We’ll stick with char-rnn (the single layer character level language model we talked about much earlier) with raw numpy so we can see the nitty-gritty details, but if you’re using one of these in practice there are much better solutions! For out-of-the-box functional deep learning models Keras is the de facto framework that people seem to use. For more creative models and all kinds of other fancy stuff I’m a fan of the newly-released PyTorch, or the “older” TensorFlow.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_541.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_541.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_541.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_541.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_541.markups.0":{"type":"A","start":380,"end":385,"href":"https:\u002F\u002Fkeras.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_541.markups.1":{"type":"A","start":529,"end":536,"href":"http:\u002F\u002Fpytorch.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_541.markups.2":{"type":"A","start":553,"end":563,"href":"https:\u002F\u002Fwww.tensorflow.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_541.markups.3":{"type":"EM","start":296,"end":328,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_542":{"id":"5c5cdcc6bd59_542","name":"5997","type":"P","href":null,"layout":null,"metadata":null,"text":"I’m going to walk us through this implementation line by line so we can see exactly what’s going on. It’s really well-commented, so feel free to peruse it on your own too.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_542.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_542.markups.0":{"type":"A","start":29,"end":48,"href":"https:\u002F\u002Fgist.github.com\u002Fkarpathy\u002Fd4dee566867f8291f086","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_543":{"id":"5c5cdcc6bd59_543","name":"8745","type":"P","href":null,"layout":null,"metadata":null,"text":"Afterwards, I challenge you to code an LSTM!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_544":{"id":"5c5cdcc6bd59_544","name":"4989","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:5c5cdcc6bd59_544.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:515164c3f20643d9534d745371d34b9f":{"id":"515164c3f20643d9534d745371d34b9f","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python\u002Fnumpy","__typename":"MediaResource"},"$Paragraph:5c5cdcc6bd59_544.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:515164c3f20643d9534d745371d34b9f","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:5c5cdcc6bd59_545":{"id":"5c5cdcc6bd59_545","name":"8a37","type":"P","href":null,"layout":null,"metadata":null,"text":"import numpy as np","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_545.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_545.markups.0":{"type":"CODE","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_546":{"id":"5c5cdcc6bd59_546","name":"54ad","type":"P","href":null,"layout":null,"metadata":null,"text":"Well, duh.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_547":{"id":"5c5cdcc6bd59_547","name":"a14a","type":"P","href":null,"layout":null,"metadata":null,"text":"data = open(‘input.txt’, ‘r’).read()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint ‘data has %d characters, %d unique.’ % (data_size, vocab_size)\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_547.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_547.markups.0":{"type":"CODE","start":0,"end":277,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_548":{"id":"5c5cdcc6bd59_548","name":"3f08","type":"P","href":null,"layout":null,"metadata":null,"text":"We load in our data and get a list of all of the characters that appear in it. We set up two dictionaries: one mapping characters to an index, and one for the reverse. We’ll use this when converting characters to\u002Ffrom a one-hot encoding later on.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_549":{"id":"5c5cdcc6bd59_549","name":"f0a4","type":"P","href":null,"layout":null,"metadata":null,"text":"hidden_size = 100\nseq_length = 25\nlearning_rate = 1e-1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_549.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_549.markups.0":{"type":"CODE","start":0,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_550":{"id":"5c5cdcc6bd59_550","name":"b15a","type":"P","href":null,"layout":null,"metadata":null,"text":"Typical hyperparam stuff. Our RNN layer will have a hidden size of 100, and we’ll train our network on batches of 25 characters at a time. Since we’ll be training our network with BPTT, we need to make sure the sequences are sufficiently short that we can unroll the network all the way and keep everything in memory. Finally, set the learning rate to .1.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_551":{"id":"5c5cdcc6bd59_551","name":"a836","type":"P","href":null,"layout":null,"metadata":null,"text":"Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\nWhh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\nWhy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\nbh = np.zeros((hidden_size, 1)) # hidden bias\nby = np.zeros((vocab_size, 1)) # output bias","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_551.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_551.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_551.markups.0":{"type":"CODE","start":0,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_551.markups.1":{"type":"CODE","start":70,"end":303,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_552":{"id":"5c5cdcc6bd59_552","name":"7dce","type":"P","href":null,"layout":null,"metadata":null,"text":"We set up our parameters — note that this is just a typical RNN, no fancy LSTM cells. We have weight matrices for updating our hidden state with each input, updating our hidden state with each timestep, and producing an output (and biases for our hidden state + output). We could be doing some fancy weight initialization here, but some normally-distributed randomness is sufficient for breaking symmetry.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_553":{"id":"5c5cdcc6bd59_553","name":"c19b","type":"P","href":null,"layout":null,"metadata":null,"text":"Now let’s talk loss function. We start by computing the forward pass, then computing the backward pass, just like with any neural network.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_554":{"id":"5c5cdcc6bd59_554","name":"08d8","type":"P","href":null,"layout":null,"metadata":null,"text":"xs, hs, ys, ps = {}, {}, {}, {}\nhs[-1] = np.copy(hprev)\nloss = 0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_554.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_554.markups.0":{"type":"CODE","start":0,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_555":{"id":"5c5cdcc6bd59_555","name":"7017","type":"P","href":null,"layout":null,"metadata":null,"text":"We start off by just setting up some variables to store our one-hot inputs, hidden states, outputs, and softmax probabilities.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_556":{"id":"5c5cdcc6bd59_556","name":"03ad","type":"P","href":null,"layout":null,"metadata":null,"text":"for t in xrange(len(inputs)):","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_556.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_556.markups.0":{"type":"CODE","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_557":{"id":"5c5cdcc6bd59_557","name":"7bca","type":"P","href":null,"layout":null,"metadata":null,"text":"Go through each timestep, and for each timestep…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_558":{"id":"5c5cdcc6bd59_558","name":"d87e","type":"P","href":null,"layout":null,"metadata":null,"text":"xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\nxs[t][inputs[t]] = 1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_558.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_558.markups.0":{"type":"CODE","start":0,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_559":{"id":"5c5cdcc6bd59_559","name":"0962","type":"P","href":null,"layout":null,"metadata":null,"text":"Convert our input character at this timestep to a one-hot vector.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_560":{"id":"5c5cdcc6bd59_560","name":"ea9f","type":"P","href":null,"layout":null,"metadata":null,"text":"hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_560.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_560.markups.0":{"type":"CODE","start":0,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_561":{"id":"5c5cdcc6bd59_561","name":"de7c","type":"P","href":null,"layout":null,"metadata":null,"text":"Update our hidden state. We saw this formula already — use our Wxh and Whh matrices to update our hidden state based on the last state and our input, and add a bias.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_561.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_561.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_561.markups.0":{"type":"STRONG","start":63,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_561.markups.1":{"type":"STRONG","start":71,"end":74,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_562":{"id":"5c5cdcc6bd59_562","name":"15c1","type":"P","href":null,"layout":null,"metadata":null,"text":"ys[t] = np.dot(Why, hs[t]) + by","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_562.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_562.markups.0":{"type":"CODE","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_563":{"id":"5c5cdcc6bd59_563","name":"26df","type":"P","href":null,"layout":null,"metadata":null,"text":"Compute our output…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_564":{"id":"5c5cdcc6bd59_564","name":"4308","type":"P","href":null,"layout":null,"metadata":null,"text":"ps[t] = np.exp(ys[t]) \u002F np.sum(np.exp(ys[t])) # probabilities for next chars","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_564.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_564.markups.0":{"type":"CODE","start":0,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_565":{"id":"5c5cdcc6bd59_565","name":"46cd","type":"P","href":null,"layout":null,"metadata":null,"text":"…and convert it to a probability distribution with a softmax.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_566":{"id":"5c5cdcc6bd59_566","name":"b3bb","type":"P","href":null,"layout":null,"metadata":null,"text":"loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_566.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_566.markups.0":{"type":"CODE","start":0,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_567":{"id":"5c5cdcc6bd59_567","name":"011a","type":"P","href":null,"layout":null,"metadata":null,"text":"Accumulate the loss for this time step as the negative log of the predicted probability. Ideally, we would have a probability of 1 for the actual next character. If it is 1, the loss is 0, log(1) = 0. As the predicted probability approaches 0, the loss approaches inf, because log(0) = -inf.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_567.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_567.markups.0":{"type":"EM","start":139,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_568":{"id":"5c5cdcc6bd59_568","name":"d366","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s it for the forward pass (not bad, right? Boiled down, it’s like six lines of code. Piece of cake).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_569":{"id":"5c5cdcc6bd59_569","name":"b1db","type":"P","href":null,"layout":null,"metadata":null,"text":"dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n dhnext = np.zeros_like(hs[0])","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_569.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_569.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_569.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_569.markups.0":{"type":"CODE","start":0,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_569.markups.1":{"type":"CODE","start":78,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_569.markups.2":{"type":"CODE","start":127,"end":157,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_570":{"id":"5c5cdcc6bd59_570","name":"cdb0","type":"P","href":null,"layout":null,"metadata":null,"text":"Setting up some variables for our backward pass — the gradients of our weight matrices, the gradients for our biases, and the gradients from the next timestep (we’ll see how that works in a bit).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_571":{"id":"5c5cdcc6bd59_571","name":"2fd8","type":"P","href":null,"layout":null,"metadata":null,"text":"for t in reversed(xrange(len(inputs))):","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_571.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_571.markups.0":{"type":"CODE","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_572":{"id":"5c5cdcc6bd59_572","name":"14bf","type":"P","href":null,"layout":null,"metadata":null,"text":"Go through our sequence in reverse as we back up the gradients.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_573":{"id":"5c5cdcc6bd59_573","name":"9148","type":"P","href":null,"layout":null,"metadata":null,"text":"dy = np.copy(ps[t])\n dy[targets[t]] -= 1 # backprop into y. see http:\u002F\u002Fcs231n.github.io\u002Fneural-networks-case-study\u002F#grad if confused here","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_573.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_573.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_573.markups.0":{"type":"CODE","start":0,"end":137,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_573.markups.1":{"type":"A","start":64,"end":120,"href":"http:\u002F\u002Fcs231n.github.io\u002Fneural-networks-case-study\u002F#grad","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_574":{"id":"5c5cdcc6bd59_574","name":"3fb3","type":"P","href":null,"layout":null,"metadata":null,"text":"First, get the gradient of the output, dy. As it turns out, the gradient of the cross-entropy loss is really as copying over the distribution and subtracting 1 from the correct class.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_574.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_574.markups.0":{"type":"A","start":43,"end":58,"href":"http:\u002F\u002Fcs231n.github.io\u002Fneural-networks-case-study\u002F#grad","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_575":{"id":"5c5cdcc6bd59_575","name":"b8b8","type":"P","href":null,"layout":null,"metadata":null,"text":"Remember backpropogation? When we have a weighted sum, the gradient of the weights is just the corresponding value that it is being multiplied by, because the other terms drop out and that one weight is treated as a constant. So, computing the gradient of our Why matrix is super simple: just multiply the gradient of loss w.r.t. the output (dy) by the derivative of the output w.r.t. Why (which is just the hidden state at our given timestep), and we get the derivative of the loss w.r.t. Why.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_575.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_575.markups.0":{"type":"A","start":0,"end":24,"href":"https:\u002F\u002Fayearofai.com\u002Frohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_576":{"id":"5c5cdcc6bd59_576","name":"9790","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*TVvKSJJqaM9CDjlk.","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*TVvKSJJqaM9CDjlk.":{"id":"0*TVvKSJJqaM9CDjlk.","originalHeight":86,"originalWidth":426,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_577":{"id":"5c5cdcc6bd59_577","name":"247c","type":"P","href":null,"layout":null,"metadata":null,"text":"dWhy += np.dot(dy, hs[t].T)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_577.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_577.markups.0":{"type":"CODE","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_578":{"id":"5c5cdcc6bd59_578","name":"2ff0","type":"P","href":null,"layout":null,"metadata":null,"text":"Like the other gradients (except dy, of course) we accumulate these gradients over all timesteps and apply them at the end.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_579":{"id":"5c5cdcc6bd59_579","name":"c397","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*QKDwSXVEJ9fHQ4hT.","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*QKDwSXVEJ9fHQ4hT.":{"id":"0*QKDwSXVEJ9fHQ4hT.","originalHeight":86,"originalWidth":321,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_580":{"id":"5c5cdcc6bd59_580","name":"1c4f","type":"P","href":null,"layout":null,"metadata":null,"text":"dby += dy","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_580.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_580.markups.0":{"type":"CODE","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_581":{"id":"5c5cdcc6bd59_581","name":"a781","type":"P","href":null,"layout":null,"metadata":null,"text":"The derivative of loss w.r.t. output (dy) multiplied by the derivative of our output w.r.t. the bias (which is 1) gives us the derivative of our output w.r.t. the bias. So far so good.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_582":{"id":"5c5cdcc6bd59_582","name":"6890","type":"P","href":null,"layout":null,"metadata":null,"text":"dh = np.dot(Why.T, dy) + dhnext # backprop into h","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_582.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_582.markups.0":{"type":"CODE","start":0,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_583":{"id":"5c5cdcc6bd59_583","name":"9602","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*cVr1t2s7gsC4Sd6vowSkHA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*cVr1t2s7gsC4Sd6vowSkHA.png":{"id":"1*cVr1t2s7gsC4Sd6vowSkHA.png","originalHeight":74,"originalWidth":265,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_584":{"id":"5c5cdcc6bd59_584","name":"6dbe","type":"P","href":null,"layout":null,"metadata":null,"text":"We compute dL\u002Fdh using the chain rule, and accumulate it over all timesteps (hence + dhnext). We’ll need this for the next step.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_584.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_584.markups.0":{"type":"CODE","start":83,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_585":{"id":"5c5cdcc6bd59_585","name":"6d9e","type":"P","href":null,"layout":null,"metadata":null,"text":"dhraw = (1 — hs[t] * hs[t]) * dh # backprop through tanh nonlinearity","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_585.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_585.markups.0":{"type":"CODE","start":0,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_586":{"id":"5c5cdcc6bd59_586","name":"796c","type":"P","href":null,"layout":null,"metadata":null,"text":"This computes the derivative of the np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) line from earlier.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_586.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_586.markups.0":{"type":"CODE","start":36,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_587":{"id":"5c5cdcc6bd59_587","name":"7fa1","type":"P","href":null,"layout":null,"metadata":null,"text":"dbh += dhraw","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_587.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_587.markups.0":{"type":"CODE","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_588":{"id":"5c5cdcc6bd59_588","name":"f74d","type":"P","href":null,"layout":null,"metadata":null,"text":"Which is also our bh derivative, for the same reason that the by derivative was just dy.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_589":{"id":"5c5cdcc6bd59_589","name":"a0f2","type":"P","href":null,"layout":null,"metadata":null,"text":"dWxh += np.dot(dhraw, xs[t].T)\ndWhh += np.dot(dhraw, hs[t-1].T)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_589.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_589.markups.0":{"type":"CODE","start":0,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_590":{"id":"5c5cdcc6bd59_590","name":"cea8","type":"P","href":null,"layout":null,"metadata":null,"text":"We accumulate our weight gradients.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_591":{"id":"5c5cdcc6bd59_591","name":"1bb0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*uf-YEbf0258UhbDc5QZLRw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*uf-YEbf0258UhbDc5QZLRw.png":{"id":"1*uf-YEbf0258UhbDc5QZLRw.png","originalHeight":77,"originalWidth":286,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_592":{"id":"5c5cdcc6bd59_592","name":"d79b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Vl1LVzPJSZKDplA9J5cElg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Vl1LVzPJSZKDplA9J5cElg.png":{"id":"1*Vl1LVzPJSZKDplA9J5cElg.png","originalHeight":77,"originalWidth":287,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:5c5cdcc6bd59_593":{"id":"5c5cdcc6bd59_593","name":"012f","type":"P","href":null,"layout":null,"metadata":null,"text":"dhnext = np.dot(Whh.T, dhraw)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_593.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_593.markups.0":{"type":"CODE","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_594":{"id":"5c5cdcc6bd59_594","name":"bb22","type":"P","href":null,"layout":null,"metadata":null,"text":"And finally, store dh for this timestep so we can use it for the previous one.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_595":{"id":"5c5cdcc6bd59_595","name":"1ea0","type":"P","href":null,"layout":null,"metadata":null,"text":"for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\nnp.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_595.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_595.markups.0":{"type":"CODE","start":0,"end":117,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_596":{"id":"5c5cdcc6bd59_596","name":"a901","type":"P","href":null,"layout":null,"metadata":null,"text":"Last but not least, a little gradient clipping so we don’t get no exploding gradients.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_597":{"id":"5c5cdcc6bd59_597","name":"8392","type":"P","href":null,"layout":null,"metadata":null,"text":"return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_597.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_597.markups.0":{"type":"CODE","start":0,"end":58,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_598":{"id":"5c5cdcc6bd59_598","name":"4dc5","type":"P","href":null,"layout":null,"metadata":null,"text":"And then return all the gradients so we can apply an optimizer step. And that’s it for the backprop code; not too bad, right?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_598.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_598.markups.0":{"type":"EM","start":110,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_599":{"id":"5c5cdcc6bd59_599","name":"4eaf","type":"P","href":null,"layout":null,"metadata":null,"text":"def sample(h, seed_ix, n):","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_599.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_599.markups.0":{"type":"CODE","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_600":{"id":"5c5cdcc6bd59_600","name":"ed15","type":"P","href":null,"layout":null,"metadata":null,"text":"This method is used for sampling a generated sequence from the network, starting with state h, first letter seed_ix, with length n.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_600.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_600.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_600.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_600.markups.0":{"type":"CODE","start":92,"end":93,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_600.markups.1":{"type":"CODE","start":108,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_600.markups.2":{"type":"CODE","start":129,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_601":{"id":"5c5cdcc6bd59_601","name":"d445","type":"P","href":null,"layout":null,"metadata":null,"text":"x = np.zeros((vocab_size, 1)) \nx[seed_ix] = 1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_601.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_601.markups.0":{"type":"CODE","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_602":{"id":"5c5cdcc6bd59_602","name":"a8f6","type":"P","href":null,"layout":null,"metadata":null,"text":"Set up our one-hot encoded input vector based on the seed character.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_603":{"id":"5c5cdcc6bd59_603","name":"8a82","type":"P","href":null,"layout":null,"metadata":null,"text":"ixes = []","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_603.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_603.markups.0":{"type":"CODE","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_604":{"id":"5c5cdcc6bd59_604","name":"7e2b","type":"P","href":null,"layout":null,"metadata":null,"text":"And an array to keep track of our sequence.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_605":{"id":"5c5cdcc6bd59_605","name":"3cae","type":"P","href":null,"layout":null,"metadata":null,"text":"for t in xrange(n):","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_605.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_605.markups.0":{"type":"CODE","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_606":{"id":"5c5cdcc6bd59_606","name":"c8e6","type":"P","href":null,"layout":null,"metadata":null,"text":"To generate each character in our sequence…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_607":{"id":"5c5cdcc6bd59_607","name":"18cf","type":"P","href":null,"layout":null,"metadata":null,"text":"h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_607.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_607.markups.0":{"type":"CODE","start":0,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_608":{"id":"5c5cdcc6bd59_608","name":"6d86","type":"P","href":null,"layout":null,"metadata":null,"text":"Update our hidden state! We saw this formula in the last function, too.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_609":{"id":"5c5cdcc6bd59_609","name":"ce6f","type":"P","href":null,"layout":null,"metadata":null,"text":"y = np.dot(Why, h) + by\np = np.exp(y) \u002F np.sum(np.exp(y))","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_609.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_609.markups.0":{"type":"CODE","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_610":{"id":"5c5cdcc6bd59_610","name":"d108","type":"P","href":null,"layout":null,"metadata":null,"text":"Generate our output and run it through a softmax. Again, straight from the last function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_611":{"id":"5c5cdcc6bd59_611","name":"62ae","type":"P","href":null,"layout":null,"metadata":null,"text":"ix = np.random.choice(range(vocab_size), p=p.ravel())","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_611.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_611.markups.0":{"type":"CODE","start":0,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_612":{"id":"5c5cdcc6bd59_612","name":"fd39","type":"P","href":null,"layout":null,"metadata":null,"text":"Sample from our output distribution using some numpy magic.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_613":{"id":"5c5cdcc6bd59_613","name":"cec1","type":"P","href":null,"layout":null,"metadata":null,"text":"x = np.zeros((vocab_size, 1))\nx[ix] = 1\nixes.append(ix)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_613.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_613.markups.0":{"type":"CODE","start":0,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_614":{"id":"5c5cdcc6bd59_614","name":"e8b3","type":"P","href":null,"layout":null,"metadata":null,"text":"Convert the sampled value into a one-hot encoding and append it to the array.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_615":{"id":"5c5cdcc6bd59_615","name":"4928","type":"P","href":null,"layout":null,"metadata":null,"text":"return ixes","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_615.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_615.markups.0":{"type":"CODE","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_616":{"id":"5c5cdcc6bd59_616","name":"e074","type":"P","href":null,"layout":null,"metadata":null,"text":"…and of course, return the final sequence when we’re done.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_617":{"id":"5c5cdcc6bd59_617","name":"6968","type":"P","href":null,"layout":null,"metadata":null,"text":"n, p = 0, 0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_617.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_617.markups.0":{"type":"CODE","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_618":{"id":"5c5cdcc6bd59_618","name":"2eb1","type":"P","href":null,"layout":null,"metadata":null,"text":"n is the number of training iterations we’ve done. p is the index into our training data for where we are now.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_618.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_618.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_618.markups.0":{"type":"STRONG","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_618.markups.1":{"type":"STRONG","start":51,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_619":{"id":"5c5cdcc6bd59_619","name":"ee12","type":"P","href":null,"layout":null,"metadata":null,"text":"mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_619.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_619.markups.0":{"type":"CODE","start":0,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_620":{"id":"5c5cdcc6bd59_620","name":"9951","type":"P","href":null,"layout":null,"metadata":null,"text":"Set up memory variables for the Adagrad algorithm (out of scope of this post, maybe next time — it’s just a variant on gradient descent).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_621":{"id":"5c5cdcc6bd59_621","name":"ffc4","type":"P","href":null,"layout":null,"metadata":null,"text":"while True:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_621.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_621.markups.0":{"type":"CODE","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_622":{"id":"5c5cdcc6bd59_622","name":"bcde","type":"P","href":null,"layout":null,"metadata":null,"text":"Training loop.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_623":{"id":"5c5cdcc6bd59_623","name":"a04e","type":"P","href":null,"layout":null,"metadata":null,"text":"if p+seq_length+1 \u003E= len(data) or n == 0:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_623.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_623.markups.0":{"type":"CODE","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_624":{"id":"5c5cdcc6bd59_624","name":"3444","type":"P","href":null,"layout":null,"metadata":null,"text":"This is a little check to see if we need to reset our memory because we’re starting back at the beginning of our data.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_625":{"id":"5c5cdcc6bd59_625","name":"a0be","type":"P","href":null,"layout":null,"metadata":null,"text":"hprev = np.zeros((hidden_size,1)) # reset RNN memory","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_625.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_625.markups.0":{"type":"CODE","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_626":{"id":"5c5cdcc6bd59_626","name":"038a","type":"P","href":null,"layout":null,"metadata":null,"text":"…and if we are, reset the memory.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_627":{"id":"5c5cdcc6bd59_627","name":"9c3d","type":"P","href":null,"layout":null,"metadata":null,"text":"p = 0","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_627.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_627.markups.0":{"type":"CODE","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_628":{"id":"5c5cdcc6bd59_628","name":"bd23","type":"P","href":null,"layout":null,"metadata":null,"text":"And reset the data pointer.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_629":{"id":"5c5cdcc6bd59_629","name":"0161","type":"P","href":null,"layout":null,"metadata":null,"text":"inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\ntargets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_629.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_629.markups.0":{"type":"CODE","start":0,"end":118,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_630":{"id":"5c5cdcc6bd59_630","name":"70c2","type":"P","href":null,"layout":null,"metadata":null,"text":"We grab a seq_length-long piece of the data as our input to the network. At each timestep, we want to predict the next character; this means that our “targets” will be the next character for each input. We get a snippet of the input the same length as the input, but offset by 1, for the target.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_630.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_630.markups.0":{"type":"CODE","start":10,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_631":{"id":"5c5cdcc6bd59_631","name":"3f42","type":"P","href":null,"layout":null,"metadata":null,"text":"if n % 100 == 0:\nsample_ix = sample(hprev, inputs[0], 200)\ntxt = ‘’.join(ix_to_char[ix] for ix in sample_ix)\nprint ‘ — — \\n %s \\n — — ‘ % (txt, )","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_631.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_631.markups.0":{"type":"CODE","start":0,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_632":{"id":"5c5cdcc6bd59_632","name":"a7eb","type":"P","href":null,"layout":null,"metadata":null,"text":"Here we just print to the terminal a sample every 100 training steps so we can see how its doing. Ideally, this will print out gibberish the first few times, before gradually printing out more and more reasonable language.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_633":{"id":"5c5cdcc6bd59_633","name":"e412","type":"P","href":null,"layout":null,"metadata":null,"text":"loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_633.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_633.markups.0":{"type":"CODE","start":0,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_634":{"id":"5c5cdcc6bd59_634","name":"538d","type":"P","href":null,"layout":null,"metadata":null,"text":"Do a forward pass, backward pass, and get the gradients.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_635":{"id":"5c5cdcc6bd59_635","name":"9d99","type":"P","href":null,"layout":null,"metadata":null,"text":"smooth_loss = smooth_loss * 0.999 + loss * 0.001","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_635.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_635.markups.0":{"type":"CODE","start":0,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_636":{"id":"5c5cdcc6bd59_636","name":"4ab1","type":"P","href":null,"layout":null,"metadata":null,"text":"Adagrad stuff.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_637":{"id":"5c5cdcc6bd59_637","name":"3823","type":"P","href":null,"layout":null,"metadata":null,"text":"if n % 100 == 0: print ‘iter %d, loss: %f’ % (n, smooth_loss) # print progress","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_637.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_637.markups.0":{"type":"CODE","start":0,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_638":{"id":"5c5cdcc6bd59_638","name":"b38c","type":"P","href":null,"layout":null,"metadata":null,"text":"Keep up with progress.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_639":{"id":"5c5cdcc6bd59_639","name":"995f","type":"P","href":null,"layout":null,"metadata":null,"text":"for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\nmem += dparam * dparam\nparam += -learning_rate * dparam \u002F np.sqrt(mem + 1e-8) # adagrad update","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_639.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_639.markups.0":{"type":"CODE","start":0,"end":210,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_640":{"id":"5c5cdcc6bd59_640","name":"9509","type":"P","href":null,"layout":null,"metadata":null,"text":"More Adagrad. We should really do an article on optimization algorithms.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_641":{"id":"5c5cdcc6bd59_641","name":"a2fc","type":"P","href":null,"layout":null,"metadata":null,"text":"p += seq_length # move data pointer\nn += 1 # iteration counter","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_641.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_641.markups.0":{"type":"CODE","start":0,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_642":{"id":"5c5cdcc6bd59_642","name":"a889","type":"P","href":null,"layout":null,"metadata":null,"text":"Annnddd finally, we update our data pointer and iteration counter.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_643":{"id":"5c5cdcc6bd59_643","name":"7850","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s it. We have an RNN. Neat-o. Reminder: your challenge is to code an LSTM… and TensorFlow doesn’t count!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_644":{"id":"5c5cdcc6bd59_644","name":"5a53","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_645":{"id":"5c5cdcc6bd59_645","name":"8a00","type":"P","href":null,"layout":null,"metadata":null,"text":"Wow. That was a lot. If you came in knowing nothing or very little about recurrent neural nets, you sure as hell know a lot now. And you don’t just know about something cool; you know about something very important — something that can equip you to read and understand some of the most prominent and hottest recent research papers in machine learning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_645.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_645.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_645.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_645.markups.0":{"type":"STRONG","start":16,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_645.markups.1":{"type":"EM","start":16,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_645.markups.2":{"type":"EM","start":200,"end":214,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_646":{"id":"5c5cdcc6bd59_646","name":"eec5","type":"P","href":null,"layout":null,"metadata":null,"text":"Something this article didn’t do so good at was making sure the calculus and derivatives were in the context of operating on vectors (because, remember, RNNs\u002FLSTMs operate over vectors). In many cases the derivatives were in the 1-D context. It’s not something you need to worry about, but you might want to look into.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_647":{"id":"5c5cdcc6bd59_647","name":"f6a4","type":"P","href":null,"layout":null,"metadata":null,"text":"We’re finally at the point where we can focus our energies on this blog towards cooler stuff including hot research papers like Neural Turing Machines or Learning to Learn, case studies eg. AlphaGo, other parts of machine learning and artificial intelligence (I’m, Rohan, personally looking forward to optimization!), or different algorithms like GANs. There’s very little compulsory content or “groundwork” we need to cover anymore. So, now, we’re officially onto the cool stuff.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_648":{"id":"5c5cdcc6bd59_648","name":"4f33","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s right. A Year Of AI is officially… cool.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_648.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:5c5cdcc6bd59_648.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:5c5cdcc6bd59_648.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_648.markups.1":{"type":"EM","start":42,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:5c5cdcc6bd59_649":{"id":"5c5cdcc6bd59_649","name":"f41b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*necEBipfgD-Z3_9R9usFgg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*necEBipfgD-Z3_9R9usFgg.png":{"id":"1*necEBipfgD-Z3_9R9usFgg.png","originalHeight":80,"originalWidth":80,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:data-science":{"id":"data-science","displayTitle":"Data Science","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:algorithms":{"id":"algorithms","displayTitle":"Algorithms","__typename":"Tag"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.0":{"__typename":"SuggestedPost","post":{"type":"id","generated":false,"id":"Post:e514f0b00c7c","typename":"Post"},"postSuggestionReasons":[{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.0.postSuggestionReasons.0","typename":"PostSuggestionReason"}]},"Post:e514f0b00c7c":{"__typename":"Post","id":"e514f0b00c7c","title":"All you need to know about RNNs","isLocked":false,"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*P83tkcIAj1UjJiBE","typename":"ImageMetadata"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fall-you-need-to-know-about-rnns-e514f0b00c7c","firstPublishedAt":1551878300641,"readingTime":6.6427672955974835,"statusForCollection":"APPROVED","visibility":"PUBLIC","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"creator":{"type":"id","generated":false,"id":"User:ddf3befa5aec","typename":"User"},"previewContent":{"type":"id","generated":true,"id":"$Post:e514f0b00c7c.previewContent","typename":"PreviewContent"},"readingList":"READING_LIST_NONE","clapCount":234,"viewerClapCount":0,"isLimitedState":false,"voterCount":50,"recommenders":[]},"ImageMetadata:0*P83tkcIAj1UjJiBE":{"id":"0*P83tkcIAj1UjJiBE","focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata","originalHeight":3146,"originalWidth":3146},"Collection:7f60cf5620c9":{"name":"Towards Data Science","id":"7f60cf5620c9","slug":"towards-data-science","domain":"towardsdatascience.com","__typename":"Collection"},"User:ddf3befa5aec":{"name":"Suleka Helmini","username":"sulekahelmini9628","id":"ddf3befa5aec","bio":"Software Engineering Undergraduate at IIT","isFollowing":false,"imageId":"2*sf38bnoA9V-OYjanWPWiag.jpeg","mediumMemberAt":0,"__typename":"User"},"$Post:e514f0b00c7c.previewContent":{"isFullContent":false,"__typename":"PreviewContent"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.0.postSuggestionReasons.0":{"reason":"CF_POST_SIMILAR_TO_POST","users":[],"topics":[],"collections":[],"tags":[],"__typename":"PostSuggestionReason"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.1":{"__typename":"SuggestedPost","post":{"type":"id","generated":false,"id":"Post:f7e94251cc80","typename":"Post"},"postSuggestionReasons":[{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.1.postSuggestionReasons.0","typename":"PostSuggestionReason"}]},"Post:f7e94251cc80":{"__typename":"Post","id":"f7e94251cc80","title":"Building a Recurrent Neural Network from Scratch","isLocked":true,"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*Zr7A2WmVFMeydgRm","typename":"ImageMetadata"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002Fx8-the-ai-community\u002Fbuilding-a-recurrent-neural-network-from-scratch-f7e94251cc80","firstPublishedAt":1570285862515,"readingTime":5.708490566037736,"statusForCollection":"APPROVED","visibility":"LOCKED","collection":{"type":"id","generated":false,"id":"Collection:283f7138fc2a","typename":"Collection"},"creator":{"type":"id","generated":false,"id":"User:552f4a4a68a2","typename":"User"},"previewContent":{"type":"id","generated":true,"id":"$Post:f7e94251cc80.previewContent","typename":"PreviewContent"},"readingList":"READING_LIST_NONE","clapCount":125,"viewerClapCount":0,"isLimitedState":false,"voterCount":30,"recommenders":[]},"ImageMetadata:0*Zr7A2WmVFMeydgRm":{"id":"0*Zr7A2WmVFMeydgRm","focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata","originalHeight":3505,"originalWidth":5257},"Collection:283f7138fc2a":{"name":"AI Graduate","id":"283f7138fc2a","slug":"x8-the-ai-community","domain":null,"__typename":"Collection"},"User:552f4a4a68a2":{"name":"Prateek Karkare","username":"prateekkarkare","id":"552f4a4a68a2","bio":"Observe.Learn.Repeat | Artificial Intelligence, Electronics, Music, Travel","isFollowing":false,"imageId":"1*wTepRSOi6fIJpeLEHwhFAw.jpeg","mediumMemberAt":0,"__typename":"User"},"$Post:f7e94251cc80.previewContent":{"isFullContent":false,"__typename":"PreviewContent"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.1.postSuggestionReasons.0":{"reason":"CF_POST_SIMILAR_TO_POST","users":[],"topics":[],"collections":[],"tags":[],"__typename":"PostSuggestionReason"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.2":{"__typename":"SuggestedPost","post":{"type":"id","generated":false,"id":"Post:3f06d7653a85","typename":"Post"},"postSuggestionReasons":[{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.2.postSuggestionReasons.0","typename":"PostSuggestionReason"}]},"Post:3f06d7653a85":{"__typename":"Post","id":"3f06d7653a85","title":"Recurrent Neural Networks (RNNs)","isLocked":false,"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*c_I0vMTeN0augGW-.jpg","typename":"ImageMetadata"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Frecurrent-neural-networks-rnns-3f06d7653a85","firstPublishedAt":1562879718332,"readingTime":10.617924528301886,"statusForCollection":"APPROVED","visibility":"PUBLIC","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"creator":{"type":"id","generated":false,"id":"User:70c04bf6660e","typename":"User"},"previewContent":{"type":"id","generated":true,"id":"$Post:3f06d7653a85.previewContent","typename":"PreviewContent"},"readingList":"READING_LIST_NONE","clapCount":180,"viewerClapCount":0,"isLimitedState":false,"voterCount":50,"recommenders":[]},"ImageMetadata:0*c_I0vMTeN0augGW-.jpg":{"id":"0*c_I0vMTeN0augGW-.jpg","focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata","originalHeight":1080,"originalWidth":1920},"User:70c04bf6660e":{"name":"Javaid Nabi","username":"javaid.nabi","id":"70c04bf6660e","bio":"ML Enthusiast","isFollowing":false,"imageId":"1*X5vPFhNx6Ah8dXYYuL4N5Q.png","mediumMemberAt":0,"__typename":"User"},"$Post:3f06d7653a85.previewContent":{"isFullContent":false,"__typename":"PreviewContent"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.2.postSuggestionReasons.0":{"reason":"CF_POST_SIMILAR_TO_POST","users":[],"topics":[],"collections":[],"tags":[],"__typename":"PostSuggestionReason"},"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}})":{"items":[{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.0","typename":"SuggestedPost"},{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.1","typename":"SuggestedPost"},{"type":"id","generated":true,"id":"$Post:10300100899b.recirc({\"paging\":{\"limit\":3}}).items.2","typename":"SuggestedPost"}],"__typename":"RecircItemConnection"},"$Post:10300100899b.postResponses":{"count":23,"__typename":"PostResponses","responsesConnection({\"paging\":{\"limit\":10}})":{"type":"id","generated":true,"id":"$Post:10300100899b.postResponses.responsesConnection({\"paging\":{\"limit\":10}})","typename":"StreamConnection"}},"$Post:10300100899b.previewContent":{"subtitle":"The ultimate guide to machine learning’s favorite child.","__typename":"PreviewContent"},"$Post:10300100899b.postResponses.responsesConnection({\"paging\":{\"limit\":10}})":{"pagingInfo":null,"stream":[],"__typename":"StreamConnection"}}</script><script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/manifest.3c174731.js.descarga"></script><script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/vendors_main.a48ff263.chunk.js.descarga"></script><script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/main.cebc2e24.chunk.js.descarga"></script><script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/vendors_instrumentation.d5b46d66.chunk.js.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/instrumentation.684b230b.chunk.js.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/reporting.b1af84b2.chunk.js.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/vendors_AMPPost_CollectionHomepage_CollectionHom.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/vendors_AMPPost_DebugCachedPost_Post_SequencePos.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/AMPPost_CollectionHomepage_CollectionHomepagePre.descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/AMPPost_CollectionHomepage_CollectionHomep(1).descarga"></script>
<script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/Post.882d7d77.chunk.js.descarga"></script><script>window.main();</script><script src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/p.js.descarga" async="" id="parsely-cf"></script><div><div class="wj t wl agn ai ago agp agq agr ags agt agu agv agw agx agy agz aha ahb ahc ahd ahe ahf ug ahg ahh ahi ahj ahk ahl ahm ahn aho ahp ahq ahr"><div><div class="branch-journeys-top"><div class="n fw"><p class="ih xn aq b ar as at au r">To make Medium work, we log user data. By using Medium, you agree to our <a href="https://medium.com/policy/f03bf92035c9" class="co cp az ba bb bc bd be bf bg bj bk gm gn aga" target="_blank" rel="noopener">Privacy Policy</a>, including cookie policy.</p><div class="ahs r aht"><div class="bf r dw v"><span class="aq b ar as at au r av aw"><button class="co cp az ba bb bc bd be bf bg hq hr bj bk gm gn" data-testid="close-button"><svg width="19" height="19" viewBox="0 0 19 19"><path d="M13.8 4.6L9.5 8.89 5.21 4.6l-.61.61 4.29 4.3-4.29 4.28.61.62 4.3-4.3 4.28 4.3.62-.62-4.3-4.29 4.3-4.29" fill-rule="evenodd"></path></svg></button></span></div></div></div></div></div></div></div><iframe src="./Rohan &amp; Lenny #3_ Recurrent Neural Networks &amp; LSTMs_files/a16180790160.html" hidden="" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe></body></html>